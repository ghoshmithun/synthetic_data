{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.cluster as cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../credit_card_kaggle/creditcard.csv')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = list(data.columns[ data.columns != 'Class' ])\n",
    "label_cols = ['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler=StandardScaler()\n",
    "stdscaler.fit(data[data_cols])\n",
    "data[data_cols] = stdscaler.transform(data[data_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.996583</td>\n",
       "      <td>-0.694242</td>\n",
       "      <td>-0.044075</td>\n",
       "      <td>1.672773</td>\n",
       "      <td>0.973366</td>\n",
       "      <td>-0.245117</td>\n",
       "      <td>0.347068</td>\n",
       "      <td>0.193679</td>\n",
       "      <td>0.082637</td>\n",
       "      <td>0.331128</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024923</td>\n",
       "      <td>0.382854</td>\n",
       "      <td>-0.176911</td>\n",
       "      <td>0.110507</td>\n",
       "      <td>0.246585</td>\n",
       "      <td>-0.392170</td>\n",
       "      <td>0.330892</td>\n",
       "      <td>-0.063781</td>\n",
       "      <td>0.244964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.996583</td>\n",
       "      <td>0.608496</td>\n",
       "      <td>0.161176</td>\n",
       "      <td>0.109797</td>\n",
       "      <td>0.316523</td>\n",
       "      <td>0.043483</td>\n",
       "      <td>-0.061820</td>\n",
       "      <td>-0.063700</td>\n",
       "      <td>0.071253</td>\n",
       "      <td>-0.232494</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.307377</td>\n",
       "      <td>-0.880077</td>\n",
       "      <td>0.162201</td>\n",
       "      <td>-0.561131</td>\n",
       "      <td>0.320694</td>\n",
       "      <td>0.261069</td>\n",
       "      <td>-0.022256</td>\n",
       "      <td>0.044608</td>\n",
       "      <td>-0.342475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.996562</td>\n",
       "      <td>-0.693500</td>\n",
       "      <td>-0.811578</td>\n",
       "      <td>1.169468</td>\n",
       "      <td>0.268231</td>\n",
       "      <td>-0.364572</td>\n",
       "      <td>1.351454</td>\n",
       "      <td>0.639776</td>\n",
       "      <td>0.207373</td>\n",
       "      <td>-1.378675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337632</td>\n",
       "      <td>1.063358</td>\n",
       "      <td>1.456320</td>\n",
       "      <td>-1.138092</td>\n",
       "      <td>-0.628537</td>\n",
       "      <td>-0.288447</td>\n",
       "      <td>-0.137137</td>\n",
       "      <td>-0.181021</td>\n",
       "      <td>1.160686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.996562</td>\n",
       "      <td>-0.493325</td>\n",
       "      <td>-0.112169</td>\n",
       "      <td>1.182516</td>\n",
       "      <td>-0.609727</td>\n",
       "      <td>-0.007469</td>\n",
       "      <td>0.936150</td>\n",
       "      <td>0.192071</td>\n",
       "      <td>0.316018</td>\n",
       "      <td>-1.262503</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147443</td>\n",
       "      <td>0.007267</td>\n",
       "      <td>-0.304777</td>\n",
       "      <td>-1.941027</td>\n",
       "      <td>1.241904</td>\n",
       "      <td>-0.460217</td>\n",
       "      <td>0.155396</td>\n",
       "      <td>0.186189</td>\n",
       "      <td>0.140534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.996541</td>\n",
       "      <td>-0.591330</td>\n",
       "      <td>0.531541</td>\n",
       "      <td>1.021412</td>\n",
       "      <td>0.284655</td>\n",
       "      <td>-0.295015</td>\n",
       "      <td>0.071999</td>\n",
       "      <td>0.479302</td>\n",
       "      <td>-0.226510</td>\n",
       "      <td>0.744326</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012839</td>\n",
       "      <td>1.100011</td>\n",
       "      <td>-0.220123</td>\n",
       "      <td>0.233250</td>\n",
       "      <td>-0.395202</td>\n",
       "      <td>1.041611</td>\n",
       "      <td>0.543620</td>\n",
       "      <td>0.651816</td>\n",
       "      <td>-0.073403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0 -1.996583 -0.694242 -0.044075  1.672773  0.973366 -0.245117  0.347068   \n",
       "1 -1.996583  0.608496  0.161176  0.109797  0.316523  0.043483 -0.061820   \n",
       "2 -1.996562 -0.693500 -0.811578  1.169468  0.268231 -0.364572  1.351454   \n",
       "3 -1.996562 -0.493325 -0.112169  1.182516 -0.609727 -0.007469  0.936150   \n",
       "4 -1.996541 -0.591330  0.531541  1.021412  0.284655 -0.295015  0.071999   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0  0.193679  0.082637  0.331128  ... -0.024923  0.382854 -0.176911  0.110507   \n",
       "1 -0.063700  0.071253 -0.232494  ... -0.307377 -0.880077  0.162201 -0.561131   \n",
       "2  0.639776  0.207373 -1.378675  ...  0.337632  1.063358  1.456320 -1.138092   \n",
       "3  0.192071  0.316018 -1.262503  ... -0.147443  0.007267 -0.304777 -1.941027   \n",
       "4  0.479302 -0.226510  0.744326  ... -0.012839  1.100011 -0.220123  0.233250   \n",
       "\n",
       "        V25       V26       V27       V28    Amount  Class  \n",
       "0  0.246585 -0.392170  0.330892 -0.063781  0.244964      0  \n",
       "1  0.320694  0.261069 -0.022256  0.044608 -0.342475      0  \n",
       "2 -0.628537 -0.288447 -0.137137 -0.181021  1.160686      0  \n",
       "3  1.241904 -0.460217  0.155396  0.186189  0.140534      0  \n",
       "4 -0.395202  1.041611  0.543620  0.651816 -0.073403      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb72bd95ad0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST # Training dataset\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_block(input_dim, output_dim):\n",
    "    '''\n",
    "    Function for returning a block of the generator's neural network\n",
    "    given input and output dimensions.\n",
    "    Parameters:\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        output_dim: the dimension of the output vector, a scalar\n",
    "    Returns:\n",
    "        a generator neural network layer, with a linear transformation \n",
    "          followed by a batch normalization and then a relu activation\n",
    "    '''\n",
    "    return nn.Sequential(\n",
    "        # https://pytorch.org/docs/stable/nn.html.\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Verify the generator block function\n",
    "def test_gen_block(in_features, out_features, num_test=1000):\n",
    "    block = get_generator_block(in_features, out_features)\n",
    "\n",
    "    # Check the three parts\n",
    "    assert len(block) == 3\n",
    "    assert type(block[0]) == nn.Linear\n",
    "    assert type(block[1]) == nn.BatchNorm1d\n",
    "    assert type(block[2]) == nn.ReLU\n",
    "    \n",
    "    # Check the output shape\n",
    "    test_input = torch.randn(num_test, in_features)\n",
    "    test_output = block(test_input)\n",
    "    assert tuple(test_output.shape) == (num_test, out_features)\n",
    "    assert test_output.std() > 0.55\n",
    "    assert test_output.std() < 0.65\n",
    "\n",
    "test_gen_block(25, 12)\n",
    "test_gen_block(15, 28)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Generator\n",
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        im_dim: the dimension of the images, fitted for the dataset used, a scalar\n",
    "          (MNIST images are 28 x 28 = 784 so that is your default)\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, z_dim=10, im_dim=30, hidden_dim=20):\n",
    "        super(Generator, self).__init__()\n",
    "        # Build the neural network\n",
    "        self.gen = nn.Sequential(\n",
    "            get_generator_block(z_dim, hidden_dim),\n",
    "            get_generator_block(hidden_dim, hidden_dim * 2),\n",
    "            get_generator_block(hidden_dim * 2, hidden_dim * 4),\n",
    "            get_generator_block(hidden_dim * 4, hidden_dim * 8),\n",
    "            # There is a dropdown with hints if you need them!\n",
    "            nn.Linear(hidden_dim * 8, im_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
    "        returns generated images.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
    "        '''\n",
    "        return self.gen(noise)\n",
    "    \n",
    "    # Needed for grading\n",
    "    def get_gen(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            the sequential model\n",
    "        '''\n",
    "        return self.gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Verify the generator class\n",
    "def test_generator(z_dim, im_dim, hidden_dim, num_test=10000):\n",
    "    gen = Generator(z_dim, im_dim, hidden_dim).get_gen()\n",
    "    \n",
    "    # Check there are six modules in the sequential part\n",
    "    assert len(gen) == 6\n",
    "    assert str(gen.__getitem__(4)).replace(' ', '') == f'Linear(in_features={hidden_dim * 8},out_features={im_dim},bias=True)'\n",
    "    assert str(gen.__getitem__(5)).replace(' ', '') == 'Sigmoid()'\n",
    "    test_input = torch.randn(num_test, z_dim)\n",
    "    test_output = gen(test_input)\n",
    "\n",
    "    # Check that the output shape is correct\n",
    "    assert tuple(test_output.shape) == (num_test, im_dim)\n",
    "    assert test_output.max() < 1, \"Make sure to use a sigmoid\"\n",
    "    assert test_output.min() > 0, \"Make sure to use a sigmoid\"\n",
    "    assert test_output.std() > 0.05, \"Don't use batchnorm here\"\n",
    "    assert test_output.std() < 0.15, \"Don't use batchnorm here\"\n",
    "\n",
    "test_generator(5, 10, 20)\n",
    "test_generator(20, 8, 24)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(n_samples, z_dim, device='cpu'):\n",
    "    '''\n",
    "    Function for creating noise vectors: Given the dimensions (n_samples, z_dim),\n",
    "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
    "    Parameters:\n",
    "        n_samples: the number of samples to generate, a scalar\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        device: the device type\n",
    "    '''\n",
    "    # NOTE: To use this on GPU with device='cuda', make sure to pass the device \n",
    "    # argument to the function you use to generate the noise.\n",
    "    return torch.randn(n_samples,z_dim,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Verify the noise vector function\n",
    "def test_get_noise(n_samples, z_dim, device='cpu'):\n",
    "    noise = get_noise(n_samples, z_dim, device)\n",
    "    # Make sure a normal distribution was used\n",
    "    assert tuple(noise.shape) == (n_samples, z_dim)\n",
    "    assert torch.abs(noise.std() - torch.tensor(1.0)) < 0.01\n",
    "    assert str(noise.device).startswith(device)\n",
    "\n",
    "test_get_noise(1000, 100, 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    test_get_noise(1000, 32, 'cuda')\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_discriminator_block\n",
    "def get_discriminator_block(input_dim, output_dim):\n",
    "    '''\n",
    "    Discriminator Block\n",
    "    Function for returning a neural network of the discriminator given input and output dimensions.\n",
    "    Parameters:\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        output_dim: the dimension of the output vector, a scalar\n",
    "    Returns:\n",
    "        a discriminator neural network layer, with a linear transformation \n",
    "          followed by an nn.LeakyReLU activation with negative slope of 0.2 \n",
    "          (https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html)\n",
    "    '''\n",
    "    return nn.Sequential(\n",
    "         nn.Linear(input_dim, output_dim), #Layer 1\n",
    "         nn.LeakyReLU(0.2, inplace=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "#  Verify the discriminator block function\n",
    "def test_disc_block(in_features, out_features, num_test=10000):\n",
    "    block = get_discriminator_block(in_features, out_features)\n",
    "\n",
    "    # Check there are two parts\n",
    "    assert len(block) == 2\n",
    "    test_input = torch.randn(num_test, in_features)\n",
    "    test_output = block(test_input)\n",
    "\n",
    "    # Check that the shape is right\n",
    "    assert tuple(test_output.shape) == (num_test, out_features)\n",
    "    \n",
    "    # Check that the LeakyReLU slope is about 0.2\n",
    "    assert -test_output.min() / test_output.max() > 0.1\n",
    "    assert -test_output.min() / test_output.max() < 0.3\n",
    "    assert test_output.std() > 0.3\n",
    "    assert test_output.std() < 0.5\n",
    "    \n",
    "    assert str(block.__getitem__(0)).replace(' ', '') == f'Linear(in_features={in_features},out_features={out_features},bias=True)'        \n",
    "    assert str(block.__getitem__(1)).replace(' ', '').replace(',inplace=True', '') == 'LeakyReLU(negative_slope=0.2)'\n",
    "\n",
    "\n",
    "test_disc_block(25, 12)\n",
    "test_disc_block(15, 28)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Values:\n",
    "        im_dim: the dimension of the images, fitted for the dataset used, a scalar\n",
    "            (MNIST images are 28x28 = 784 so that is your default)\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, im_dim=30, hidden_dim=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            get_discriminator_block(im_dim, hidden_dim * 4),\n",
    "            get_discriminator_block(hidden_dim * 4, hidden_dim * 2),\n",
    "            get_discriminator_block(hidden_dim * 2, hidden_dim),\n",
    "            # Hint: You want to transform the final output into a single value,\n",
    "            #       so add one more linear map.\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of the discriminator: Given an image tensor, \n",
    "        returns a 1-dimension tensor representing fake/real.\n",
    "        Parameters:\n",
    "            image: a flattened image tensor with dimension (im_dim)\n",
    "        '''\n",
    "        return self.disc(image.float())\n",
    "    \n",
    "    # Needed for grading\n",
    "    def get_disc(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            the sequential model\n",
    "        '''\n",
    "        return self.disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Verify the discriminator class\n",
    "def test_discriminator(z_dim, hidden_dim, num_test=100):\n",
    "    \n",
    "    disc = Discriminator(z_dim, hidden_dim).get_disc()\n",
    "\n",
    "    # Check there are three parts\n",
    "    assert len(disc) == 4\n",
    "    assert type(disc.__getitem__(3)) == nn.Linear\n",
    "\n",
    "    # Check the linear layer is correct\n",
    "    test_input = torch.randn(num_test, z_dim)\n",
    "    test_output = disc(test_input)\n",
    "    assert tuple(test_output.shape) == (num_test, 1)\n",
    "\n",
    "test_discriminator(5, 10)\n",
    "test_discriminator(20, 8)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your parameters\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 10\n",
    "z_dim = 10\n",
    "hidden_dim=20\n",
    "display_step = 500\n",
    "batch_size = 16\n",
    "lr = 0.00001\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "disc = Discriminator().to(device) \n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_disc_loss\n",
    "def get_disc_loss(gen, disc, criterion, real, num_images, z_dim, device):\n",
    "    '''\n",
    "    Return the loss of the discriminator given inputs.\n",
    "    Parameters:\n",
    "        gen: the generator model, which returns an image given z-dimensional noise\n",
    "        disc: the discriminator model, which returns a single-dimensional prediction of real/fake\n",
    "        criterion: the loss function, which should be used to compare \n",
    "               the discriminator's predictions to the ground truth reality of the images \n",
    "               (e.g. fake = 0, real = 1)\n",
    "        real: a batch of real images\n",
    "        num_images: the number of images the generator should produce, \n",
    "                which is also the length of the real images\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        device: the device type\n",
    "    Returns:\n",
    "        disc_loss: a torch scalar loss value for the current batch\n",
    "    '''\n",
    "    #     These are the steps you will need to complete:\n",
    "    #       1) Create noise vectors and generate a batch (num_images) of fake images. \n",
    "    #            Make sure to pass the device argument to the noise.\n",
    "    #       2) Get the discriminator's prediction of the fake image \n",
    "    #            and calculate the loss. Don't forget to detach the generator!\n",
    "    #            (Remember the loss function you set earlier -- criterion. You need a \n",
    "    #            'ground truth' tensor in order to calculate the loss. \n",
    "    #            For example, a ground truth tensor for a fake image is all zeros.)\n",
    "    #       3) Get the discriminator's prediction of the real image and calculate the loss.\n",
    "    #       4) Calculate the discriminator's loss by averaging the real and fake loss\n",
    "    #            and set it to disc_loss.\n",
    "\n",
    "    fake_noise = get_noise(num_images, z_dim, device=device)\n",
    "    fake = gen(fake_noise)\n",
    "    disc_fake_pred = disc(fake.detach())\n",
    "    disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "    disc_real_pred = disc(real)\n",
    "    disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "    disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "    return disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features= data[data_cols]\n",
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = data[['Class']]\n",
    "type(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train = TensorDataset(torch.tensor(features.values,dtype=torch.float64), torch.tensor(targets.values,dtype=torch.float64))\n",
    "train_loader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=DataLoader(train, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "def test_disc_reasonable(num_images=10):\n",
    "    z_dim = 64\n",
    "    gen = torch.zeros_like\n",
    "    disc = nn.Identity()\n",
    "    criterion = torch.mul # Multiply\n",
    "    real = torch.ones(num_images, 1)\n",
    "    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu')\n",
    "    assert tuple(disc_loss.shape) == (num_images, z_dim)\n",
    "    assert torch.all(torch.abs(disc_loss - 0.5) < 1e-5)\n",
    "\n",
    "    gen = torch.ones_like\n",
    "    disc = nn.Identity()\n",
    "    criterion = torch.mul # Multiply\n",
    "    real = torch.zeros(num_images, 1)\n",
    "    assert torch.all(torch.abs(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu')) < 1e-5)\n",
    "\n",
    "def test_disc_loss(max_tests = 10):\n",
    "    z_dim = 64\n",
    "    gen = Generator(z_dim).to(device)\n",
    "    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "    disc = Discriminator().to(device) \n",
    "    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "    num_steps = 0\n",
    "    for real in dataloader:\n",
    "        real=real[0]\n",
    "        cur_batch_size = len(real)\n",
    "        real = real.view(cur_batch_size, -1).to(device)\n",
    "#         real=torch.tensor(real).view(cur_batch_size, -1).to(device)\n",
    "\n",
    "        ### Update discriminator ###\n",
    "        # Zero out the gradient before backpropagation\n",
    "        disc_opt.zero_grad()\n",
    "\n",
    "        # Calculate discriminator loss\n",
    "        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)\n",
    "        assert (disc_loss - 0.68).abs() < 0.05\n",
    "\n",
    "        # Update gradients\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "\n",
    "        # Check that they detached correctly\n",
    "        assert gen.gen[0][0].weight.grad is None\n",
    "\n",
    "        # Update optimizer\n",
    "        old_weight = disc.disc[0][0].weight.data.clone()\n",
    "        disc_opt.step()\n",
    "        new_weight = disc.disc[0][0].weight.data\n",
    "        \n",
    "        # Check that some discriminator weights changed\n",
    "        assert not torch.all(torch.eq(old_weight, new_weight))\n",
    "        num_steps += 1\n",
    "        if num_steps >= max_tests:\n",
    "            break\n",
    "\n",
    "test_disc_reasonable()\n",
    "test_disc_loss()\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_loss(gen, disc, criterion, num_images, z_dim, device):\n",
    "    '''\n",
    "    Return the loss of the generator given inputs.\n",
    "    Parameters:\n",
    "        gen: the generator model, which returns an image given z-dimensional noise\n",
    "        disc: the discriminator model, which returns a single-dimensional prediction of real/fake\n",
    "        criterion: the loss function, which should be used to compare \n",
    "               the discriminator's predictions to the ground truth reality of the images \n",
    "               (e.g. fake = 0, real = 1)\n",
    "        num_images: the number of images the generator should produce, \n",
    "                which is also the length of the real images\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        device: the device type\n",
    "    Returns:\n",
    "        gen_loss: a torch scalar loss value for the current batch\n",
    "    '''\n",
    "    #     These are the steps you will need to complete:\n",
    "    #       1) Create noise vectors and generate a batch of fake images. \n",
    "    #           Remember to pass the device argument to the get_noise function.\n",
    "    #       2) Get the discriminator's prediction of the fake image.\n",
    "    #       3) Calculate the generator's loss. Remember the generator wants\n",
    "    #          the discriminator to think that its fake images are real\n",
    "    #     *Important*: You should NOT write your own loss function here - use criterion(pred, true)!\n",
    "\n",
    "    fake_noise = get_noise(num_images, z_dim, device=device)\n",
    "    fake = gen(fake_noise)\n",
    "    disc_fake_pred = disc(fake)\n",
    "    gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "def test_gen_reasonable(num_images=10):\n",
    "    z_dim = 64\n",
    "    gen = torch.zeros_like\n",
    "    disc = nn.Identity()\n",
    "    criterion = torch.mul # Multiply\n",
    "    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, 'cpu')\n",
    "    assert torch.all(torch.abs(gen_loss_tensor) < 1e-5)\n",
    "    #Verify shape. Related to gen_noise parametrization\n",
    "    assert tuple(gen_loss_tensor.shape) == (num_images, z_dim)\n",
    "\n",
    "    gen = torch.ones_like\n",
    "    disc = nn.Identity()\n",
    "    criterion = torch.mul # Multiply\n",
    "    real = torch.zeros(num_images, 1)\n",
    "    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, 'cpu')\n",
    "    assert torch.all(torch.abs(gen_loss_tensor - 1) < 1e-5)\n",
    "    #Verify shape. Related to gen_noise parametrization\n",
    "    assert tuple(gen_loss_tensor.shape) == (num_images, z_dim)\n",
    "    \n",
    "\n",
    "def test_gen_loss(num_images):\n",
    "    z_dim = 64\n",
    "    gen = Generator(z_dim).to(device)\n",
    "    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "    disc = Discriminator().to(device) \n",
    "    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "    \n",
    "    gen_loss = get_gen_loss(gen, disc, criterion, num_images, z_dim, device)\n",
    "    \n",
    "    # Check that the loss is reasonable\n",
    "    assert (gen_loss - 0.7).abs() < 0.1\n",
    "    gen_loss.backward()\n",
    "    old_weight = gen.gen[0][0].weight.clone()\n",
    "    gen_opt.step()\n",
    "    new_weight = gen.gen[0][0].weight\n",
    "    assert not torch.all(torch.eq(old_weight, new_weight))\n",
    "\n",
    "\n",
    "test_gen_reasonable(10)\n",
    "test_gen_loss(18)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4beb1f01c51484dbb99528798970712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: Generator loss: 0.6993939344882961, discriminator loss: 0.6979003940820702\n",
      "Step 1000: Generator loss: 0.7080386548042296, discriminator loss: 0.6858605178594595\n",
      "Step 1500: Generator loss: 0.7182726676464074, discriminator loss: 0.6748698900938033\n",
      "Step 2000: Generator loss: 0.7272632989883419, discriminator loss: 0.664324694752693\n",
      "Step 2500: Generator loss: 0.735707288622856, discriminator loss: 0.6534168376922608\n",
      "Step 3000: Generator loss: 0.7447998908758157, discriminator loss: 0.6413002980947492\n",
      "Step 3500: Generator loss: 0.7514740092754356, discriminator loss: 0.631210120916366\n",
      "Step 4000: Generator loss: 0.7552924493551253, discriminator loss: 0.6225622603893278\n",
      "Step 4500: Generator loss: 0.7617049475908284, discriminator loss: 0.6124617586135862\n",
      "Step 5000: Generator loss: 0.7714958199262617, discriminator loss: 0.5994006869792943\n",
      "Step 5500: Generator loss: 0.7858519797325138, discriminator loss: 0.5845355319976809\n",
      "Step 6000: Generator loss: 0.804184844970704, discriminator loss: 0.5652707673311234\n",
      "Step 6500: Generator loss: 0.8261378856897346, discriminator loss: 0.5428206172585486\n",
      "Step 7000: Generator loss: 0.8490233135223387, discriminator loss: 0.518791040062904\n",
      "Step 7500: Generator loss: 0.870529717683793, discriminator loss: 0.4942371752262116\n",
      "Step 8000: Generator loss: 0.8817144951820366, discriminator loss: 0.47395395445823674\n",
      "Step 8500: Generator loss: 0.8890210609436041, discriminator loss: 0.4570694985985755\n",
      "Step 9000: Generator loss: 0.9031128044128418, discriminator loss: 0.43794933897256866\n",
      "Step 9500: Generator loss: 0.9255983603000644, discriminator loss: 0.4182781212329865\n",
      "Step 10000: Generator loss: 0.9525789060592653, discriminator loss: 0.39983654141426084\n",
      "Step 10500: Generator loss: 0.9942162334918972, discriminator loss: 0.3793442267179489\n",
      "Step 11000: Generator loss: 1.0422240951061246, discriminator loss: 0.35945958465337774\n",
      "Step 11500: Generator loss: 1.1018993432521818, discriminator loss: 0.33268998467922195\n",
      "Step 12000: Generator loss: 1.171408896446228, discriminator loss: 0.30530799210071613\n",
      "Step 12500: Generator loss: 1.2400220139026625, discriminator loss: 0.27902464166283636\n",
      "Step 13000: Generator loss: 1.3114911465644825, discriminator loss: 0.2542773959040643\n",
      "Step 13500: Generator loss: 1.3778291585445406, discriminator loss: 0.23379862707853324\n",
      "Step 14000: Generator loss: 1.4447437999248505, discriminator loss: 0.21321159264445305\n",
      "Step 14500: Generator loss: 1.5170286495685565, discriminator loss: 0.19295456069707861\n",
      "Step 15000: Generator loss: 1.587704209327695, discriminator loss: 0.1744846777617931\n",
      "Step 15500: Generator loss: 1.6537894592285134, discriminator loss: 0.1595847599804403\n",
      "Step 16000: Generator loss: 1.7270826106071477, discriminator loss: 0.14609921924769867\n",
      "Step 16500: Generator loss: 1.8195138835906979, discriminator loss: 0.13052893383800995\n",
      "Step 17000: Generator loss: 1.9207387707233448, discriminator loss: 0.11627152082324019\n",
      "Step 17500: Generator loss: 2.024961668491363, discriminator loss: 0.10319820402562614\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e0c3144b52436eae866c9522280a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18000: Generator loss: 2.1308159518241876, discriminator loss: 0.09077370567619804\n",
      "Step 18500: Generator loss: 2.191973841667177, discriminator loss: 0.08498991207778445\n",
      "Step 19000: Generator loss: 2.2711414213180574, discriminator loss: 0.08049246792495243\n",
      "Step 19500: Generator loss: 2.407256175518036, discriminator loss: 0.07016036789119247\n",
      "Step 20000: Generator loss: 2.5344805245399473, discriminator loss: 0.06095814440399411\n",
      "Step 20500: Generator loss: 2.65186920404434, discriminator loss: 0.0535435343980789\n",
      "Step 21000: Generator loss: 2.7722719902992243, discriminator loss: 0.04778721243143084\n",
      "Step 21500: Generator loss: 2.9272347812652595, discriminator loss: 0.040323337152600294\n",
      "Step 22000: Generator loss: 3.0780123586654637, discriminator loss: 0.03473978361859921\n",
      "Step 22500: Generator loss: 3.2266921567916858, discriminator loss: 0.029447516169399026\n",
      "Step 23000: Generator loss: 3.3758419923782337, discriminator loss: 0.025352543197572224\n",
      "Step 23500: Generator loss: 3.5384329614639283, discriminator loss: 0.02100482689961792\n",
      "Step 24000: Generator loss: 3.702488877296449, discriminator loss: 0.01776046322844922\n",
      "Step 24500: Generator loss: 3.8312316870689416, discriminator loss: 0.015849597338587047\n",
      "Step 25000: Generator loss: 3.897741003036498, discriminator loss: 0.016338099675253015\n",
      "Step 25500: Generator loss: 4.09035487794876, discriminator loss: 0.01402230178751051\n",
      "Step 26000: Generator loss: 4.237969657897946, discriminator loss: 0.012036014968529338\n",
      "Step 26500: Generator loss: 4.335958201408386, discriminator loss: 0.010669199436903004\n",
      "Step 27000: Generator loss: 4.459434539794924, discriminator loss: 0.009269868425093588\n",
      "Step 27500: Generator loss: 4.570793211936955, discriminator loss: 0.008117706209421153\n",
      "Step 28000: Generator loss: 4.643677348136899, discriminator loss: 0.008157933140173556\n",
      "Step 28500: Generator loss: 4.800990040779112, discriminator loss: 0.006746373845264317\n",
      "Step 29000: Generator loss: 4.962754182815551, discriminator loss: 0.0057629175256006466\n",
      "Step 29500: Generator loss: 5.021937883377077, discriminator loss: 0.005963139065075665\n",
      "Step 30000: Generator loss: 5.006559355735777, discriminator loss: 0.005756643673870715\n",
      "Step 30500: Generator loss: 5.132437817573548, discriminator loss: 0.0052508656135760255\n",
      "Step 31000: Generator loss: 5.194459140777586, discriminator loss: 0.005683845246210695\n",
      "Step 31500: Generator loss: 5.353279354095457, discriminator loss: 0.00546575754927471\n",
      "Step 32000: Generator loss: 5.416991087913509, discriminator loss: 0.004467182420194142\n",
      "Step 32500: Generator loss: 5.351055788040163, discriminator loss: 0.00518416307074949\n",
      "Step 33000: Generator loss: 5.557880854606632, discriminator loss: 0.004897183670196681\n",
      "Step 33500: Generator loss: 5.707833596229558, discriminator loss: 0.004968867036513987\n",
      "Step 34000: Generator loss: 5.79359975433349, discriminator loss: 0.004256927445298059\n",
      "Step 34500: Generator loss: 5.927076042175294, discriminator loss: 0.003355473358184101\n",
      "Step 35000: Generator loss: 6.008216858863835, discriminator loss: 0.0033507205483037992\n",
      "Step 35500: Generator loss: 6.110423371314999, discriminator loss: 0.002523483150638636\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fa8f2e6e03406b9fe9cdb72d1ef96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 36000: Generator loss: 6.18589045906067, discriminator loss: 0.0025074490837287177\n",
      "Step 36500: Generator loss: 6.3238221302032445, discriminator loss: 0.001963862172677182\n",
      "Step 37000: Generator loss: 6.4295064945220926, discriminator loss: 0.001793203457375058\n",
      "Step 37500: Generator loss: 6.460905838012693, discriminator loss: 0.001610902429907582\n",
      "Step 38000: Generator loss: 6.482361884117128, discriminator loss: 0.0019686816246248778\n",
      "Step 38500: Generator loss: 6.528716180801397, discriminator loss: 0.0015609821006655699\n",
      "Step 39000: Generator loss: 6.564290469169616, discriminator loss: 0.0016862937905825688\n",
      "Step 39500: Generator loss: 6.6359624719619745, discriminator loss: 0.0015851516433758666\n",
      "Step 40000: Generator loss: 6.695620276451106, discriminator loss: 0.0015884570208145325\n",
      "Step 40500: Generator loss: 6.784950223922733, discriminator loss: 0.0013818238903768353\n",
      "Step 41000: Generator loss: 6.829920237541196, discriminator loss: 0.001141122463857756\n",
      "Step 41500: Generator loss: 6.830260463714594, discriminator loss: 0.001402652299846522\n",
      "Step 42000: Generator loss: 6.850630152702335, discriminator loss: 0.0010498357238247988\n",
      "Step 42500: Generator loss: 7.037384836196899, discriminator loss: 0.0011825076086679467\n",
      "Step 43000: Generator loss: 7.0869902744293265, discriminator loss: 0.0008668253478244876\n",
      "Step 43500: Generator loss: 7.153361785888664, discriminator loss: 0.0009317940564942544\n",
      "Step 44000: Generator loss: 7.101409150123596, discriminator loss: 0.0009979641115060085\n",
      "Step 44500: Generator loss: 7.186730006217959, discriminator loss: 0.0008243210668442773\n",
      "Step 45000: Generator loss: 7.314011384963984, discriminator loss: 0.0008059952761395832\n",
      "Step 45500: Generator loss: 7.361970684051523, discriminator loss: 0.0008049572887830434\n",
      "Step 46000: Generator loss: 7.3564002971649165, discriminator loss: 0.0008341528503806345\n",
      "Step 46500: Generator loss: 7.466578096389764, discriminator loss: 0.0006175130906631241\n",
      "Step 47000: Generator loss: 7.580873322486872, discriminator loss: 0.00060130177353858\n",
      "Step 47500: Generator loss: 7.556676992416379, discriminator loss: 0.0008411090009612962\n",
      "Step 48000: Generator loss: 7.5805910339355425, discriminator loss: 0.000550157279067207\n",
      "Step 48500: Generator loss: 7.639811943054198, discriminator loss: 0.0006563214638736097\n",
      "Step 49000: Generator loss: 7.680934172630313, discriminator loss: 0.0006969302620273089\n",
      "Step 49500: Generator loss: 7.704233423233037, discriminator loss: 0.0005154878908069803\n",
      "Step 50000: Generator loss: 7.764636559486388, discriminator loss: 0.0005615209736279212\n",
      "Step 50500: Generator loss: 7.775067420005796, discriminator loss: 0.000715933578409022\n",
      "Step 51000: Generator loss: 7.713977839469908, discriminator loss: 0.000764254965528379\n",
      "Step 51500: Generator loss: 7.669184382438661, discriminator loss: 0.0005072744311764839\n",
      "Step 52000: Generator loss: 7.672965393066411, discriminator loss: 0.0005467182963038794\n",
      "Step 52500: Generator loss: 7.58180169391632, discriminator loss: 0.0006303442255011759\n",
      "Step 53000: Generator loss: 7.583903956413272, discriminator loss: 0.0006483095011208209\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd7ab27975540d0ac7a650f80080445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 53500: Generator loss: 7.455185989379882, discriminator loss: 0.0009666795334778726\n",
      "Step 54000: Generator loss: 7.404110818862911, discriminator loss: 0.0010498339180485354\n",
      "Step 54500: Generator loss: 7.526917459487916, discriminator loss: 0.0009882478250074203\n",
      "Step 55000: Generator loss: 7.540378606796256, discriminator loss: 0.0010716278547770355\n",
      "Step 55500: Generator loss: 7.5598270044326785, discriminator loss: 0.0010825680026318877\n",
      "Step 56000: Generator loss: 7.610348425865179, discriminator loss: 0.0009613686563097869\n",
      "Step 56500: Generator loss: 7.6187466382980364, discriminator loss: 0.0010261215986683968\n",
      "Step 57000: Generator loss: 7.549893829345699, discriminator loss: 0.0009698770476970824\n",
      "Step 57500: Generator loss: 7.557130338668829, discriminator loss: 0.000748683928046376\n",
      "Step 58000: Generator loss: 7.457009174346927, discriminator loss: 0.0008120456190663374\n",
      "Step 58500: Generator loss: 7.4577755165100115, discriminator loss: 0.0006090665439260196\n",
      "Step 59000: Generator loss: 7.601617814064022, discriminator loss: 0.0005472838307032365\n",
      "Step 59500: Generator loss: 7.746875337600708, discriminator loss: 0.0004617097433656453\n",
      "Step 60000: Generator loss: 7.82985265827179, discriminator loss: 0.0005308032937755336\n",
      "Step 60500: Generator loss: 7.812695275306699, discriminator loss: 0.0009089605603658123\n",
      "Step 61000: Generator loss: 7.726762814521791, discriminator loss: 0.0004990018669923298\n",
      "Step 61500: Generator loss: 7.760884920120242, discriminator loss: 0.00048226960113970536\n",
      "Step 62000: Generator loss: 7.69959842777252, discriminator loss: 0.000569873109459878\n",
      "Step 62500: Generator loss: 7.683584723472598, discriminator loss: 0.0007792510635335925\n",
      "Step 63000: Generator loss: 7.941458255767828, discriminator loss: 0.0008800847012898885\n",
      "Step 63500: Generator loss: 8.040639141082758, discriminator loss: 0.0009652836947934702\n",
      "Step 64000: Generator loss: 8.091345894813537, discriminator loss: 0.0010932831726386213\n",
      "Step 64500: Generator loss: 8.023012165069582, discriminator loss: 0.0011541526452638205\n",
      "Step 65000: Generator loss: 7.930724945068364, discriminator loss: 0.0013592649627826193\n",
      "Step 65500: Generator loss: 7.988468802452083, discriminator loss: 0.0010057410942390565\n",
      "Step 66000: Generator loss: 7.9025489463806124, discriminator loss: 0.0015023653024109076\n",
      "Step 66500: Generator loss: 7.949894782066351, discriminator loss: 0.0012538595664082091\n",
      "Step 67000: Generator loss: 7.9970608100891045, discriminator loss: 0.0011163828280405142\n",
      "Step 67500: Generator loss: 8.040697914123538, discriminator loss: 0.0009741987197194252\n",
      "Step 68000: Generator loss: 7.966226348876962, discriminator loss: 0.001437097131507471\n",
      "Step 68500: Generator loss: 7.778106360435488, discriminator loss: 0.0020129353877855454\n",
      "Step 69000: Generator loss: 7.777886068344126, discriminator loss: 0.0011919349385425446\n",
      "Step 69500: Generator loss: 7.8231065664291455, discriminator loss: 0.001426039922400377\n",
      "Step 70000: Generator loss: 7.91487313747405, discriminator loss: 0.001101822032884229\n",
      "Step 70500: Generator loss: 8.065460366249088, discriminator loss: 0.0012722372843418277\n",
      "Step 71000: Generator loss: 8.052780100822451, discriminator loss: 0.0010392008140916009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210555b4f09d46b2a1b48c5e37f20291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 71500: Generator loss: 8.01082518863679, discriminator loss: 0.001329496576683595\n",
      "Step 72000: Generator loss: 8.051769951820367, discriminator loss: 0.0010120898298337125\n",
      "Step 72500: Generator loss: 8.003169421195986, discriminator loss: 0.0012809899318381214\n",
      "Step 73000: Generator loss: 7.966368641853338, discriminator loss: 0.0012948504530941125\n",
      "Step 73500: Generator loss: 7.85841377258301, discriminator loss: 0.0013765439179260273\n",
      "Step 74000: Generator loss: 7.888444650650028, discriminator loss: 0.0011799225378781553\n",
      "Step 74500: Generator loss: 7.903637798309327, discriminator loss: 0.001393980023800397\n",
      "Step 75000: Generator loss: 7.911183827400214, discriminator loss: 0.001174127414065878\n",
      "Step 75500: Generator loss: 7.894160784721377, discriminator loss: 0.0013010116130462855\n",
      "Step 76000: Generator loss: 7.8443930139541695, discriminator loss: 0.0012634391417377625\n",
      "Step 76500: Generator loss: 7.84530739879608, discriminator loss: 0.0011217552616726613\n",
      "Step 77000: Generator loss: 7.967371307373041, discriminator loss: 0.0010957442821818405\n",
      "Step 77500: Generator loss: 8.00192136669159, discriminator loss: 0.0011100753778591752\n",
      "Step 78000: Generator loss: 7.913767766952515, discriminator loss: 0.0014816253193421286\n",
      "Step 78500: Generator loss: 7.944529139518741, discriminator loss: 0.0008715614042594094\n",
      "Step 79000: Generator loss: 7.9845979413986194, discriminator loss: 0.0013546525521669535\n",
      "Step 79500: Generator loss: 8.01122173118591, discriminator loss: 0.0007348246421897785\n",
      "Step 80000: Generator loss: 8.035464454650876, discriminator loss: 0.0011153957787319092\n",
      "Step 80500: Generator loss: 8.020227024078363, discriminator loss: 0.0009401880484074344\n",
      "Step 81000: Generator loss: 8.015408762931829, discriminator loss: 0.0008270195531658824\n",
      "Step 81500: Generator loss: 8.076469523429877, discriminator loss: 0.0008149915479007179\n",
      "Step 82000: Generator loss: 8.162842112541194, discriminator loss: 0.0005121264710905958\n",
      "Step 82500: Generator loss: 8.180044365882877, discriminator loss: 0.0005895990436256401\n",
      "Step 83000: Generator loss: 7.888339740753178, discriminator loss: 0.000978264614823274\n",
      "Step 83500: Generator loss: 7.745088217735288, discriminator loss: 0.0007151534911536141\n",
      "Step 84000: Generator loss: 7.8999738864898745, discriminator loss: 0.000573323898599483\n",
      "Step 84500: Generator loss: 8.018367483139045, discriminator loss: 0.0006392784528143241\n",
      "Step 85000: Generator loss: 7.9096945056915295, discriminator loss: 0.0011921654762409152\n",
      "Step 85500: Generator loss: 7.884736863136289, discriminator loss: 0.0008240012477035641\n",
      "Step 86000: Generator loss: 7.998795425415036, discriminator loss: 0.0005592836232099214\n",
      "Step 86500: Generator loss: 8.153929842948912, discriminator loss: 0.00038235036091646213\n",
      "Step 87000: Generator loss: 8.16776875209809, discriminator loss: 0.0006096294363960621\n",
      "Step 87500: Generator loss: 8.073483160018919, discriminator loss: 0.0007443531888420694\n",
      "Step 88000: Generator loss: 8.082169425010678, discriminator loss: 0.0007326854339044083\n",
      "Step 88500: Generator loss: 8.201054639816283, discriminator loss: 0.0005779332390811765\n",
      "Step 89000: Generator loss: 8.079771314620977, discriminator loss: 0.0005368669082236\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75397300bed64af1bd81183c4566fd9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 89500: Generator loss: 8.19671516036986, discriminator loss: 0.0007493260935589206\n",
      "Step 90000: Generator loss: 8.099887481689448, discriminator loss: 0.0006787340520240823\n",
      "Step 90500: Generator loss: 8.117122070312504, discriminator loss: 0.0010045010144822302\n",
      "Step 91000: Generator loss: 8.14450086212158, discriminator loss: 0.00036997837547096386\n",
      "Step 91500: Generator loss: 8.314373381614686, discriminator loss: 0.0003130342854710764\n",
      "Step 92000: Generator loss: 8.308226802825926, discriminator loss: 0.0004735039481020066\n",
      "Step 92500: Generator loss: 8.260456989288318, discriminator loss: 0.000513322925165994\n",
      "Step 93000: Generator loss: 8.276922580718992, discriminator loss: 0.00036630874843103826\n",
      "Step 93500: Generator loss: 8.325628530502314, discriminator loss: 0.0003520040336006784\n",
      "Step 94000: Generator loss: 8.13155060768127, discriminator loss: 0.0010932063363434294\n",
      "Step 94500: Generator loss: 8.019897471427912, discriminator loss: 0.0004026217033679129\n",
      "Step 95000: Generator loss: 8.22088004684448, discriminator loss: 0.0003301970888569487\n",
      "Step 95500: Generator loss: 8.281130434989933, discriminator loss: 0.00041262009579804695\n",
      "Step 96000: Generator loss: 8.155184910774226, discriminator loss: 0.0006564486795687122\n",
      "Step 96500: Generator loss: 8.105838751792906, discriminator loss: 0.000512391192052746\n",
      "Step 97000: Generator loss: 7.817890785217285, discriminator loss: 0.0004530885820568073\n",
      "Step 97500: Generator loss: 7.666803657531734, discriminator loss: 0.0009188551912375259\n",
      "Step 98000: Generator loss: 7.754310307502747, discriminator loss: 0.0008153484596987254\n",
      "Step 98500: Generator loss: 7.986726935386657, discriminator loss: 0.0007262919915083331\n",
      "Step 99000: Generator loss: 7.829230150222772, discriminator loss: 0.0011189901907346204\n",
      "Step 99500: Generator loss: 7.82855535793305, discriminator loss: 0.0007201301414170299\n",
      "Step 100000: Generator loss: 7.808688400268559, discriminator loss: 0.0015491740590077828\n",
      "Step 100500: Generator loss: 7.80329092979431, discriminator loss: 0.0005901314084185285\n",
      "Step 101000: Generator loss: 7.786793592452997, discriminator loss: 0.0010581150644575262\n",
      "Step 101500: Generator loss: 7.768912708282471, discriminator loss: 0.0008636283102096058\n",
      "Step 102000: Generator loss: 7.878546023368833, discriminator loss: 0.000710620265163016\n",
      "Step 102500: Generator loss: 7.9630381469726546, discriminator loss: 0.000635599932575132\n",
      "Step 103000: Generator loss: 7.991513051033016, discriminator loss: 0.0006439007305889398\n",
      "Step 103500: Generator loss: 8.025560915946958, discriminator loss: 0.0007593778025184289\n",
      "Step 104000: Generator loss: 8.125449388504025, discriminator loss: 0.0004476840911083856\n",
      "Step 104500: Generator loss: 8.146649530410762, discriminator loss: 0.0006733910854964057\n",
      "Step 105000: Generator loss: 8.05623586082458, discriminator loss: 0.0005108365131018223\n",
      "Step 105500: Generator loss: 8.02147305393219, discriminator loss: 0.0005933365158562091\n",
      "Step 106000: Generator loss: 7.987394826889047, discriminator loss: 0.00066282097727526\n",
      "Step 106500: Generator loss: 7.995875925064095, discriminator loss: 0.000539819772122428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34abed8a0b74ab1abf864aa796f33bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 107000: Generator loss: 7.852130910873411, discriminator loss: 0.0005300217514741234\n",
      "Step 107500: Generator loss: 7.9387787551879905, discriminator loss: 0.0004039119664230383\n",
      "Step 108000: Generator loss: 7.964718221664429, discriminator loss: 0.0008722986294014842\n",
      "Step 108500: Generator loss: 7.873566144943234, discriminator loss: 0.0005706608326581775\n",
      "Step 109000: Generator loss: 7.86692781543732, discriminator loss: 0.000791861574078212\n",
      "Step 109500: Generator loss: 7.980957825660708, discriminator loss: 0.0005411213487677744\n",
      "Step 110000: Generator loss: 7.979106715202336, discriminator loss: 0.0006555286397051535\n",
      "Step 110500: Generator loss: 7.85677174091339, discriminator loss: 0.0005606920668215024\n",
      "Step 111000: Generator loss: 7.994804561614983, discriminator loss: 0.00046521862820372926\n",
      "Step 111500: Generator loss: 8.059341718673702, discriminator loss: 0.0005314530142641156\n",
      "Step 112000: Generator loss: 8.234363448143007, discriminator loss: 0.0003330495721893384\n",
      "Step 112500: Generator loss: 8.327687279701234, discriminator loss: 0.0005063931168988344\n",
      "Step 113000: Generator loss: 8.350867610931386, discriminator loss: 0.00031599152626586146\n",
      "Step 113500: Generator loss: 8.378496062278753, discriminator loss: 0.0008683040818723386\n",
      "Step 114000: Generator loss: 8.462184492111206, discriminator loss: 0.00030941814520338095\n",
      "Step 114500: Generator loss: 8.534992347717289, discriminator loss: 0.0002908775119576603\n",
      "Step 115000: Generator loss: 8.608804601669307, discriminator loss: 0.0002796626725466922\n",
      "Step 115500: Generator loss: 8.637760084152218, discriminator loss: 0.00044373682736477356\n",
      "Step 116000: Generator loss: 8.585357072830206, discriminator loss: 0.0003346013925765873\n",
      "Step 116500: Generator loss: 8.694370185852055, discriminator loss: 0.0003649580554047133\n",
      "Step 117000: Generator loss: 8.68531227493286, discriminator loss: 0.0001915523177740396\n",
      "Step 117500: Generator loss: 8.76927498054504, discriminator loss: 0.00025050466539687483\n",
      "Step 118000: Generator loss: 8.76461260604858, discriminator loss: 0.00018244901756406774\n",
      "Step 118500: Generator loss: 8.736108238220218, discriminator loss: 0.00046414227959758143\n",
      "Step 119000: Generator loss: 8.638690727233884, discriminator loss: 0.00016658302202995402\n",
      "Step 119500: Generator loss: 8.620108148574833, discriminator loss: 0.00019899991388956556\n",
      "Step 120000: Generator loss: 8.649679071426398, discriminator loss: 0.00026744550246803517\n",
      "Step 120500: Generator loss: 8.630364263534556, discriminator loss: 0.0002276122181647225\n",
      "Step 121000: Generator loss: 8.72822060966493, discriminator loss: 0.0001518515730422222\n",
      "Step 121500: Generator loss: 8.905280622482302, discriminator loss: 0.0001177823041944067\n",
      "Step 122000: Generator loss: 8.869300069808956, discriminator loss: 0.0003010589981422523\n",
      "Step 122500: Generator loss: 8.904299030303946, discriminator loss: 0.0001631483140372438\n",
      "Step 123000: Generator loss: 8.912504423141476, discriminator loss: 0.00021352052058500707\n",
      "Step 123500: Generator loss: 8.775548917770383, discriminator loss: 0.0002645581865945135\n",
      "Step 124000: Generator loss: 8.804888250350947, discriminator loss: 0.0002094168890762377\n",
      "Step 124500: Generator loss: 8.872052196502683, discriminator loss: 0.0002059380331193097\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2192adfae8fd49399bbb57f1a2fb8194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 125000: Generator loss: 8.906205696105959, discriminator loss: 0.00012525196610658885\n",
      "Step 125500: Generator loss: 8.995532814025882, discriminator loss: 0.00012478916451073025\n",
      "Step 126000: Generator loss: 9.080310636520377, discriminator loss: 0.0001480824395548552\n",
      "Step 126500: Generator loss: 9.063779481887822, discriminator loss: 0.00011169582125148742\n",
      "Step 127000: Generator loss: 9.151626085281375, discriminator loss: 9.535020703333422e-05\n",
      "Step 127500: Generator loss: 9.16340373611451, discriminator loss: 0.00025287924092117463\n",
      "Step 128000: Generator loss: 9.314557550430305, discriminator loss: 7.646446240687499e-05\n",
      "Step 128500: Generator loss: 9.374814405441285, discriminator loss: 0.0002326942194413277\n",
      "Step 129000: Generator loss: 9.342246852874757, discriminator loss: 6.651421429705808e-05\n",
      "Step 129500: Generator loss: 9.415981567382815, discriminator loss: 9.921057476458373e-05\n",
      "Step 130000: Generator loss: 9.471274404525767, discriminator loss: 9.305274429061679e-05\n",
      "Step 130500: Generator loss: 9.519025880813608, discriminator loss: 9.601558325812234e-05\n",
      "Step 131000: Generator loss: 9.548366546630854, discriminator loss: 7.290240334987176e-05\n",
      "Step 131500: Generator loss: 9.668955787658705, discriminator loss: 5.590422274690358e-05\n",
      "Step 132000: Generator loss: 9.617982700347907, discriminator loss: 0.0002780914536342608\n",
      "Step 132500: Generator loss: 9.504670310974113, discriminator loss: 6.657751715829363e-05\n",
      "Step 133000: Generator loss: 9.416258939743045, discriminator loss: 0.00017717888298648184\n",
      "Step 133500: Generator loss: 9.076136983871464, discriminator loss: 0.00015789047673024452\n",
      "Step 134000: Generator loss: 9.176531250000004, discriminator loss: 0.0001198852728557539\n",
      "Step 134500: Generator loss: 9.345626127243044, discriminator loss: 0.00012876510396745292\n",
      "Step 135000: Generator loss: 9.228910230636586, discriminator loss: 0.00032055074819072597\n",
      "Step 135500: Generator loss: 9.361519186019898, discriminator loss: 9.88431762016261e-05\n",
      "Step 136000: Generator loss: 9.495083156585684, discriminator loss: 7.457414346572478e-05\n",
      "Step 136500: Generator loss: 9.633364145278938, discriminator loss: 7.425292976404312e-05\n",
      "Step 137000: Generator loss: 9.592293794631956, discriminator loss: 0.00012841187790036207\n",
      "Step 137500: Generator loss: 9.479070247650144, discriminator loss: 0.00012483499251538885\n",
      "Step 138000: Generator loss: 9.495366712570183, discriminator loss: 7.48423822733457e-05\n",
      "Step 138500: Generator loss: 9.593720508575444, discriminator loss: 6.989027712188548e-05\n",
      "Step 139000: Generator loss: 9.638742654800417, discriminator loss: 8.630080091097628e-05\n",
      "Step 139500: Generator loss: 9.854047864913936, discriminator loss: 7.408294688139001e-05\n",
      "Step 140000: Generator loss: 9.860254093170155, discriminator loss: 0.0001264598834713979\n",
      "Step 140500: Generator loss: 9.881141836166389, discriminator loss: 8.919115958997286e-05\n",
      "Step 141000: Generator loss: 9.842919918060288, discriminator loss: 8.921210478729336e-05\n",
      "Step 141500: Generator loss: 9.645821231842046, discriminator loss: 0.00024641384652932164\n",
      "Step 142000: Generator loss: 9.636639673233027, discriminator loss: 8.870661327091502e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0d213b167e43c5a0eba4a3de7e4225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 142500: Generator loss: 9.64136783409118, discriminator loss: 0.00012003475741585142\n",
      "Step 143000: Generator loss: 9.676340110778808, discriminator loss: 7.311202804703494e-05\n",
      "Step 143500: Generator loss: 9.76557581710815, discriminator loss: 7.892244411414139e-05\n",
      "Step 144000: Generator loss: 9.70099215126038, discriminator loss: 0.00012281203069142063\n",
      "Step 144500: Generator loss: 9.411455114364612, discriminator loss: 0.0003423388913761301\n",
      "Step 145000: Generator loss: 9.176090270996095, discriminator loss: 0.00011326139526499903\n",
      "Step 145500: Generator loss: 9.300341924667357, discriminator loss: 0.00016832525591598817\n",
      "Step 146000: Generator loss: 9.347526535034177, discriminator loss: 0.00010246773176186252\n",
      "Step 146500: Generator loss: 9.40922862625122, discriminator loss: 0.00011383433989976771\n",
      "Step 147000: Generator loss: 9.386606513977055, discriminator loss: 0.00026158602979558057\n",
      "Step 147500: Generator loss: 9.32380242156983, discriminator loss: 0.00019579224674816948\n",
      "Step 148000: Generator loss: 9.27275434494019, discriminator loss: 0.0001040477114147506\n",
      "Step 148500: Generator loss: 9.39903225326538, discriminator loss: 0.00010570163289230551\n",
      "Step 149000: Generator loss: 9.448645088195788, discriminator loss: 0.00011072885691100965\n",
      "Step 149500: Generator loss: 9.575995763778682, discriminator loss: 7.035085063398575e-05\n",
      "Step 150000: Generator loss: 9.59237658500672, discriminator loss: 0.00013791268774366474\n",
      "Step 150500: Generator loss: 9.660076179504406, discriminator loss: 9.17531933446298e-05\n",
      "Step 151000: Generator loss: 9.724823816299446, discriminator loss: 7.066737096465665e-05\n",
      "Step 151500: Generator loss: 9.746956348419179, discriminator loss: 7.790827542703476e-05\n",
      "Step 152000: Generator loss: 9.749884773254394, discriminator loss: 0.0001954938872004277\n",
      "Step 152500: Generator loss: 9.615645589828498, discriminator loss: 8.00821928933147e-05\n",
      "Step 153000: Generator loss: 9.629972955703726, discriminator loss: 0.00021459644804417617\n",
      "Step 153500: Generator loss: 9.630712839126575, discriminator loss: 5.3367507483926626e-05\n",
      "Step 154000: Generator loss: 9.770363388061531, discriminator loss: 4.22240719417459e-05\n",
      "Step 154500: Generator loss: 9.814309606552138, discriminator loss: 0.00023107379149223575\n",
      "Step 155000: Generator loss: 9.605447063446032, discriminator loss: 0.00010129262705595479\n",
      "Step 155500: Generator loss: 9.579200002670287, discriminator loss: 5.728442587133033e-05\n",
      "Step 156000: Generator loss: 9.52274942779541, discriminator loss: 9.146185806457643e-05\n",
      "Step 156500: Generator loss: 9.515285051345828, discriminator loss: 7.756213262837257e-05\n",
      "Step 157000: Generator loss: 9.644678009033198, discriminator loss: 9.74275651060453e-05\n",
      "Step 157500: Generator loss: 9.65819908142089, discriminator loss: 6.646326565532958e-05\n",
      "Step 158000: Generator loss: 9.831820310592649, discriminator loss: 5.357156003083218e-05\n",
      "Step 158500: Generator loss: 9.872510026931774, discriminator loss: 7.415879136897277e-05\n",
      "Step 159000: Generator loss: 9.856076511383058, discriminator loss: 8.30646183385397e-05\n",
      "Step 159500: Generator loss: 9.683877670288094, discriminator loss: 9.183652881984012e-05\n",
      "Step 160000: Generator loss: 9.757148378372188, discriminator loss: 6.057696121206389e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72895831f01842849606a1e78562cbfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 160500: Generator loss: 9.88571085357666, discriminator loss: 4.917359061437307e-05\n",
      "Step 161000: Generator loss: 10.023953302383411, discriminator loss: 3.8608257054875135e-05\n",
      "Step 161500: Generator loss: 10.064488235473636, discriminator loss: 6.509230570372899e-05\n",
      "Step 162000: Generator loss: 10.152782327651975, discriminator loss: 3.684449191132443e-05\n",
      "Step 162500: Generator loss: 10.19467779731751, discriminator loss: 4.796321269168402e-05\n",
      "Step 163000: Generator loss: 10.177417753219602, discriminator loss: 3.414452121796787e-05\n",
      "Step 163500: Generator loss: 10.321515624999991, discriminator loss: 2.808853533497311e-05\n",
      "Step 164000: Generator loss: 10.371677616119397, discriminator loss: 3.194629259087376e-05\n",
      "Step 164500: Generator loss: 10.27721604347229, discriminator loss: 5.1819057818647696e-05\n",
      "Step 165000: Generator loss: 10.27135887527466, discriminator loss: 2.9952322765893755e-05\n",
      "Step 165500: Generator loss: 10.356668500900259, discriminator loss: 3.931716732040515e-05\n",
      "Step 166000: Generator loss: 10.338229825973507, discriminator loss: 4.147914917302841e-05\n",
      "Step 166500: Generator loss: 10.383557613372801, discriminator loss: 4.062578185039458e-05\n",
      "Step 167000: Generator loss: 10.292476903915409, discriminator loss: 0.0001078369796487096\n",
      "Step 167500: Generator loss: 10.165562894821171, discriminator loss: 3.3789761826483284e-05\n",
      "Step 168000: Generator loss: 10.141194782257083, discriminator loss: 0.0002417320395652498\n",
      "Step 168500: Generator loss: 9.981023857116691, discriminator loss: 5.185084122786066e-05\n",
      "Step 169000: Generator loss: 10.062718704223625, discriminator loss: 3.2075569633889246e-05\n",
      "Step 169500: Generator loss: 10.186342830657969, discriminator loss: 2.8719790298055182e-05\n",
      "Step 170000: Generator loss: 10.113212242126462, discriminator loss: 7.653690327424562e-05\n",
      "Step 170500: Generator loss: 10.220377305984503, discriminator loss: 2.4935767996794318e-05\n",
      "Step 171000: Generator loss: 10.308945764541624, discriminator loss: 4.490067504229955e-05\n",
      "Step 171500: Generator loss: 10.334488996505742, discriminator loss: 2.1534840883759925e-05\n",
      "Step 172000: Generator loss: 10.455244461059573, discriminator loss: 2.1070557491839293e-05\n",
      "Step 172500: Generator loss: 10.47426931762694, discriminator loss: 3.589259971886347e-05\n",
      "Step 173000: Generator loss: 10.526406291961674, discriminator loss: 2.548262613345285e-05\n",
      "Step 173500: Generator loss: 10.527622495651249, discriminator loss: 3.94037090918573e-05\n",
      "Step 174000: Generator loss: 10.549060167312621, discriminator loss: 5.502235040694358e-05\n",
      "Step 174500: Generator loss: 10.530164409637445, discriminator loss: 1.602620369521901e-05\n",
      "Step 175000: Generator loss: 10.531311223983767, discriminator loss: 3.7898699645666064e-05\n",
      "Step 175500: Generator loss: 10.544944993972772, discriminator loss: 2.6064069807034685e-05\n",
      "Step 176000: Generator loss: 10.623263563156128, discriminator loss: 1.6862742215380436e-05\n",
      "Step 176500: Generator loss: 10.750967298507685, discriminator loss: 1.8674452956474857e-05\n",
      "Step 177000: Generator loss: 10.71101721763611, discriminator loss: 6.20477006596048e-05\n",
      "Step 177500: Generator loss: 10.616071952819816, discriminator loss: 2.7439730049081842e-05\n",
      "Step 178000: Generator loss: 10.652285387039187, discriminator loss: 1.8978327550939863e-05\n"
     ]
    }
   ],
   "source": [
    "cur_step = 0\n",
    "mean_generator_loss = 0\n",
    "mean_discriminator_loss = 0\n",
    "test_generator = True # Whether the generator should be tested\n",
    "gen_loss = False\n",
    "error = False\n",
    "for epoch in range(n_epochs):\n",
    "  \n",
    "    # Dataloader returns the batches\n",
    "    for real, _ in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "\n",
    "        # Flatten the batch of real images from the dataset\n",
    "        real = real.view(cur_batch_size, -1).to(device)\n",
    "\n",
    "        ### Update discriminator ###\n",
    "        # Zero out the gradients before backpropagation\n",
    "        disc_opt.zero_grad()\n",
    "\n",
    "        # Calculate discriminator loss\n",
    "        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)\n",
    "\n",
    "        # Update gradients\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "\n",
    "        # Update optimizer\n",
    "        disc_opt.step()\n",
    "\n",
    "        # For testing purposes, to keep track of the generator weights\n",
    "        if test_generator:\n",
    "            old_generator_weights = gen.gen[0][0].weight.detach().clone()\n",
    "\n",
    "        ### Update generator ###\n",
    "        #     Hint: This code will look a lot like the discriminator updates!\n",
    "        #     These are the steps you will need to complete:\n",
    "        #       1) Zero out the gradients.\n",
    "        #       2) Calculate the generator loss, assigning it to gen_loss.\n",
    "        #       3) Backprop through the generator: update the gradients and optimizer.\n",
    "        #### START CODE HERE ####\n",
    "        gen_opt.zero_grad()\n",
    "        gen_loss = get_gen_loss(gen, disc, criterion, cur_batch_size, z_dim, device)\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "        #### END CODE HERE ####\n",
    "\n",
    "        # For testing purposes, to check that your code changes the generator weights\n",
    "        if test_generator:\n",
    "            try:\n",
    "                assert lr > 0.0000002 or (gen.gen[0][0].weight.grad.abs().max() < 0.0005 and epoch == 0)\n",
    "                assert torch.any(gen.gen[0][0].weight.detach().clone() != old_generator_weights)\n",
    "            except:\n",
    "                error = True\n",
    "                print(\"Runtime tests have failed\")\n",
    "\n",
    "        # Keep track of the average discriminator loss\n",
    "        mean_discriminator_loss += disc_loss.item() / display_step\n",
    "\n",
    "        # Keep track of the average generator loss\n",
    "        mean_generator_loss += gen_loss.item() / display_step\n",
    "\n",
    "        ### Visualization code ###\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            print(f\"Step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}\")\n",
    "            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "            fake = gen(fake_noise)\n",
    "            mean_generator_loss = 0\n",
    "            mean_discriminator_loss = 0\n",
    "        cur_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gen, 'genmodel_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(disc, 'discmodel_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_noise = get_noise(cur_batch_size, z_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = gen(fake_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data=fake.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.48138594e+04,  1.16837501e-15,  7.37445098e-13,\n",
       "        -1.37951301e-15,  6.32896920e-14,  1.48169566e-09,\n",
       "         7.34258834e-12,  3.35667154e-11,  1.21348508e-16,\n",
       "        -2.40633061e-15,  2.23905282e-15,  1.02071059e+00,\n",
       "        -1.24453492e-15,  2.34485012e-13,  1.71233936e-15,\n",
       "         1.77745199e-12,  1.43771593e-15, -3.77217070e-16,\n",
       "         1.75909440e-06,  6.56868223e-14,  1.69155818e-11,\n",
       "         6.54433994e-03,  7.25700259e-01,  2.57873516e-16,\n",
       "         6.05640829e-01,  2.00923235e-13,  1.68343722e-15,\n",
       "         3.58247902e-08,  4.14030234e-16,  8.83496170e+01],\n",
       "       [ 9.48138594e+04,  2.12514925e-14,  3.13835243e-12,\n",
       "         5.73104769e-13,  3.04223322e-05,  1.38000154e+00,\n",
       "         1.33226013e+00,  1.23621821e+00,  2.18783922e-10,\n",
       "         7.35574169e-15,  1.30210225e-14,  3.96847863e-05,\n",
       "         4.43718142e-14,  4.35258485e-14,  1.82315500e-11,\n",
       "         8.74650297e-10,  4.78096092e-14,  1.45172994e-14,\n",
       "         6.25565044e-10,  4.14026621e-14,  3.76515964e-04,\n",
       "         1.34781942e-07,  1.59274510e-04,  2.73451726e-13,\n",
       "         6.05073929e-01,  5.21263659e-01,  1.84502597e-14,\n",
       "         4.03177917e-01,  1.14513188e-09,  8.83496170e+01],\n",
       "       [ 9.48138594e+04,  1.16837501e-15,  6.89760532e-13,\n",
       "        -1.37951280e-15,  2.10957310e-13,  1.03241327e-08,\n",
       "         7.36162867e-12,  5.17361431e-10,  1.21348719e-16,\n",
       "        -2.40633061e-15,  2.23905282e-15,  1.02070713e+00,\n",
       "        -1.24503679e-15,  3.92043406e-14,  1.36342860e-15,\n",
       "         7.23401590e-12,  1.43771593e-15, -3.77217070e-16,\n",
       "         4.85732187e-07,  1.35035898e-14,  1.32776470e-10,\n",
       "         6.80956338e-03,  7.25700259e-01,  2.57880319e-16,\n",
       "         6.05637848e-01,  2.39164293e-13,  1.68343722e-15,\n",
       "         1.69681215e-07,  3.39556291e-16,  8.83496170e+01],\n",
       "       [ 9.48138594e+04,  1.16837501e-15,  8.76066357e-15,\n",
       "        -1.37953673e-15,  2.30264255e-15,  3.55820678e-10,\n",
       "         1.19786749e-13,  9.11445398e-12,  1.21348137e-16,\n",
       "        -2.40633061e-15,  2.23905282e-15,  1.02071071e+00,\n",
       "        -1.24700582e-15,  2.66847481e-15,  1.20981186e-15,\n",
       "         1.21487212e-13,  1.43771593e-15, -3.77217070e-16,\n",
       "         9.23537186e-07,  1.13010088e-15,  1.57012099e-12,\n",
       "         4.44164325e-04,  7.25700259e-01,  2.57864808e-16,\n",
       "         6.05632722e-01,  2.11512237e-15,  1.68343722e-15,\n",
       "         1.50843462e-08, -1.12775542e-16,  8.83496170e+01],\n",
       "       [ 9.48138594e+04,  1.16837501e-15,  2.60331063e-13,\n",
       "        -1.37953376e-15,  6.41553342e-15,  2.21193419e-09,\n",
       "         1.95895253e-12,  6.04978567e-11,  1.21348163e-16,\n",
       "        -2.40633061e-15,  2.23905282e-15,  1.02071023e+00,\n",
       "        -1.24621427e-15,  2.92826574e-14,  1.32983612e-15,\n",
       "         2.48729010e-12,  1.43771593e-15, -3.77217070e-16,\n",
       "         1.46347099e-06,  2.45521224e-15,  9.42582470e-12,\n",
       "         1.39962300e-03,  7.25700259e-01,  2.57865708e-16,\n",
       "         6.05642259e-01,  6.94944997e-14,  1.68343722e-15,\n",
       "         9.05768687e-08, -3.15070713e-17,  8.83496170e+01],\n",
       "       [ 9.48138594e+04,  4.73635964e-15,  5.14762874e-13,\n",
       "         1.00836596e-13,  1.81437754e-05,  1.38009953e+00,\n",
       "         1.33226538e+00,  1.23632431e+00,  6.21412088e-11,\n",
       "        -9.88231857e-16,  4.10825930e-15,  2.37703898e-05,\n",
       "         5.81899819e-15,  7.10128240e-15,  4.22801898e-12,\n",
       "         1.63737343e-10,  8.23629935e-15,  2.17859097e-15,\n",
       "         1.15518671e-10,  5.31219508e-15,  2.64681963e-04,\n",
       "         5.45036443e-08,  7.53510758e-05,  5.74465402e-14,\n",
       "         6.05223954e-01,  5.21272004e-01,  4.84390487e-15,\n",
       "         4.03188676e-01,  3.71175368e-10,  8.83496170e+01],\n",
       "       [ 9.48138594e+04,  1.16837501e-15,  3.42269656e-16,\n",
       "        -1.37953673e-15,  2.07417278e-15,  1.84156681e-06,\n",
       "         7.67049979e-10,  1.20026422e-09,  1.21348137e-16,\n",
       "        -2.40633061e-15,  2.23905282e-15,  1.47510434e-07,\n",
       "        -1.24701175e-15,  8.19000392e-16,  1.20729417e-15,\n",
       "         6.86303618e-12,  1.43771593e-15, -3.77217070e-16,\n",
       "         4.31836546e-11,  1.03991664e-15,  1.27109776e-13,\n",
       "         5.34939009e-13,  7.25700200e-01,  2.57864781e-16,\n",
       "         1.08930533e-06,  5.69812969e-13,  1.68343722e-15,\n",
       "         5.45164824e-07, -1.20120508e-16,  8.83496170e+01]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdscaler.inverse_transform(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94813.859375</td>\n",
       "      <td>1.145012e-14</td>\n",
       "      <td>1.798995e-12</td>\n",
       "      <td>2.698096e-13</td>\n",
       "      <td>2.214089e-05</td>\n",
       "      <td>1.380045e+00</td>\n",
       "      <td>1.332260e+00</td>\n",
       "      <td>1.236281e+00</td>\n",
       "      <td>1.234889e-10</td>\n",
       "      <td>2.382825e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>2.367150e-04</td>\n",
       "      <td>8.518502e-08</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>1.587450e-13</td>\n",
       "      <td>6.051279e-01</td>\n",
       "      <td>5.212645e-01</td>\n",
       "      <td>1.211188e-14</td>\n",
       "      <td>4.032198e-01</td>\n",
       "      <td>8.124181e-10</td>\n",
       "      <td>88.349617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94813.859375</td>\n",
       "      <td>1.168375e-15</td>\n",
       "      <td>6.321139e-12</td>\n",
       "      <td>-1.378310e-15</td>\n",
       "      <td>2.975302e-12</td>\n",
       "      <td>6.261858e-08</td>\n",
       "      <td>4.602014e-11</td>\n",
       "      <td>5.589284e-09</td>\n",
       "      <td>1.213853e-16</td>\n",
       "      <td>-2.406331e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.016725e-09</td>\n",
       "      <td>1.399751e-02</td>\n",
       "      <td>0.725700</td>\n",
       "      <td>2.585282e-16</td>\n",
       "      <td>6.056389e-01</td>\n",
       "      <td>2.840503e-12</td>\n",
       "      <td>1.683437e-15</td>\n",
       "      <td>6.078702e-07</td>\n",
       "      <td>5.557251e-15</td>\n",
       "      <td>88.349617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94813.859375</td>\n",
       "      <td>1.168379e-15</td>\n",
       "      <td>1.163605e-11</td>\n",
       "      <td>-1.320732e-15</td>\n",
       "      <td>1.084141e-10</td>\n",
       "      <td>4.512992e-08</td>\n",
       "      <td>2.465428e-09</td>\n",
       "      <td>2.543273e-09</td>\n",
       "      <td>1.269183e-16</td>\n",
       "      <td>-2.406321e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>3.001892e-09</td>\n",
       "      <td>2.714750e-02</td>\n",
       "      <td>0.725700</td>\n",
       "      <td>2.856627e-16</td>\n",
       "      <td>6.056342e-01</td>\n",
       "      <td>3.962971e-11</td>\n",
       "      <td>1.683453e-15</td>\n",
       "      <td>1.216383e-06</td>\n",
       "      <td>1.727578e-13</td>\n",
       "      <td>88.349617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94813.859375</td>\n",
       "      <td>1.168384e-15</td>\n",
       "      <td>1.279755e-11</td>\n",
       "      <td>-1.344555e-15</td>\n",
       "      <td>1.888343e-12</td>\n",
       "      <td>2.474307e-09</td>\n",
       "      <td>2.047639e-09</td>\n",
       "      <td>4.068998e-11</td>\n",
       "      <td>1.227499e-16</td>\n",
       "      <td>-2.406324e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>4.627434e-11</td>\n",
       "      <td>1.290347e-02</td>\n",
       "      <td>0.725700</td>\n",
       "      <td>2.642858e-16</td>\n",
       "      <td>6.056412e-01</td>\n",
       "      <td>5.227355e-11</td>\n",
       "      <td>1.683458e-15</td>\n",
       "      <td>7.133251e-08</td>\n",
       "      <td>6.905088e-14</td>\n",
       "      <td>88.349617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94813.859375</td>\n",
       "      <td>1.168375e-15</td>\n",
       "      <td>1.235099e-12</td>\n",
       "      <td>-1.379411e-15</td>\n",
       "      <td>1.852833e-12</td>\n",
       "      <td>1.627774e-08</td>\n",
       "      <td>2.125558e-11</td>\n",
       "      <td>8.787300e-10</td>\n",
       "      <td>1.213534e-16</td>\n",
       "      <td>-2.406331e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>3.910836e-10</td>\n",
       "      <td>1.577358e-02</td>\n",
       "      <td>0.725700</td>\n",
       "      <td>2.579465e-16</td>\n",
       "      <td>6.056389e-01</td>\n",
       "      <td>6.140811e-13</td>\n",
       "      <td>1.683437e-15</td>\n",
       "      <td>1.925502e-07</td>\n",
       "      <td>1.577614e-15</td>\n",
       "      <td>88.349617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>94813.859375</td>\n",
       "      <td>5.268463e-15</td>\n",
       "      <td>8.423571e-13</td>\n",
       "      <td>1.303283e-13</td>\n",
       "      <td>1.541098e-05</td>\n",
       "      <td>1.380091e+00</td>\n",
       "      <td>1.332263e+00</td>\n",
       "      <td>1.236346e+00</td>\n",
       "      <td>6.819788e-11</td>\n",
       "      <td>-6.262313e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.941581e-04</td>\n",
       "      <td>5.416538e-08</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>6.227654e-14</td>\n",
       "      <td>6.052554e-01</td>\n",
       "      <td>5.212693e-01</td>\n",
       "      <td>5.393157e-15</td>\n",
       "      <td>4.032387e-01</td>\n",
       "      <td>3.823848e-10</td>\n",
       "      <td>88.349617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>94813.859375</td>\n",
       "      <td>1.168375e-15</td>\n",
       "      <td>3.416913e-16</td>\n",
       "      <td>-1.379537e-15</td>\n",
       "      <td>2.074095e-15</td>\n",
       "      <td>1.430512e-09</td>\n",
       "      <td>2.162515e-15</td>\n",
       "      <td>3.202851e-13</td>\n",
       "      <td>1.213481e-16</td>\n",
       "      <td>-2.406331e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>7.104779e-16</td>\n",
       "      <td>9.589246e-15</td>\n",
       "      <td>0.725700</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>2.702260e-07</td>\n",
       "      <td>5.343878e-16</td>\n",
       "      <td>1.683437e-15</td>\n",
       "      <td>2.194947e-09</td>\n",
       "      <td>-1.227390e-16</td>\n",
       "      <td>88.349617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0             1             2             3             4   \\\n",
       "0  94813.859375  1.145012e-14  1.798995e-12  2.698096e-13  2.214089e-05   \n",
       "1  94813.859375  1.168375e-15  6.321139e-12 -1.378310e-15  2.975302e-12   \n",
       "2  94813.859375  1.168379e-15  1.163605e-11 -1.320732e-15  1.084141e-10   \n",
       "3  94813.859375  1.168384e-15  1.279755e-11 -1.344555e-15  1.888343e-12   \n",
       "4  94813.859375  1.168375e-15  1.235099e-12 -1.379411e-15  1.852833e-12   \n",
       "5  94813.859375  5.268463e-15  8.423571e-13  1.303283e-13  1.541098e-05   \n",
       "6  94813.859375  1.168375e-15  3.416913e-16 -1.379537e-15  2.074095e-15   \n",
       "\n",
       "             5             6             7             8             9   ...  \\\n",
       "0  1.380045e+00  1.332260e+00  1.236281e+00  1.234889e-10  2.382825e-15  ...   \n",
       "1  6.261858e-08  4.602014e-11  5.589284e-09  1.213853e-16 -2.406331e-15  ...   \n",
       "2  4.512992e-08  2.465428e-09  2.543273e-09  1.269183e-16 -2.406321e-15  ...   \n",
       "3  2.474307e-09  2.047639e-09  4.068998e-11  1.227499e-16 -2.406324e-15  ...   \n",
       "4  1.627774e-08  2.125558e-11  8.787300e-10  1.213534e-16 -2.406331e-15  ...   \n",
       "5  1.380091e+00  1.332263e+00  1.236346e+00  6.819788e-11 -6.262313e-16  ...   \n",
       "6  1.430512e-09  2.162515e-15  3.202851e-13  1.213481e-16 -2.406331e-15  ...   \n",
       "\n",
       "             20            21        22            23            24  \\\n",
       "0  2.367150e-04  8.518502e-08  0.000121  1.587450e-13  6.051279e-01   \n",
       "1  1.016725e-09  1.399751e-02  0.725700  2.585282e-16  6.056389e-01   \n",
       "2  3.001892e-09  2.714750e-02  0.725700  2.856627e-16  6.056342e-01   \n",
       "3  4.627434e-11  1.290347e-02  0.725700  2.642858e-16  6.056412e-01   \n",
       "4  3.910836e-10  1.577358e-02  0.725700  2.579465e-16  6.056389e-01   \n",
       "5  1.941581e-04  5.416538e-08  0.000098  6.227654e-14  6.052554e-01   \n",
       "6  7.104779e-16  9.589246e-15  0.725700  2.578648e-16  2.702260e-07   \n",
       "\n",
       "             25            26            27            28         29  \n",
       "0  5.212645e-01  1.211188e-14  4.032198e-01  8.124181e-10  88.349617  \n",
       "1  2.840503e-12  1.683437e-15  6.078702e-07  5.557251e-15  88.349617  \n",
       "2  3.962971e-11  1.683453e-15  1.216383e-06  1.727578e-13  88.349617  \n",
       "3  5.227355e-11  1.683458e-15  7.133251e-08  6.905088e-14  88.349617  \n",
       "4  6.140811e-13  1.683437e-15  1.925502e-07  1.577614e-15  88.349617  \n",
       "5  5.212693e-01  5.393157e-15  4.032387e-01  3.823848e-10  88.349617  \n",
       "6  5.343878e-16  1.683437e-15  2.194947e-09 -1.227390e-16  88.349617  \n",
       "\n",
       "[7 rows x 30 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_new_data(n_times=5):\n",
    "    data=[]\n",
    "    for _ in range(n_times):\n",
    "        fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "        fake = gen(fake_noise)\n",
    "        new_data=fake.detach().numpy()\n",
    "        data.append(stdscaler.inverse_transform(new_data))\n",
    "    return pd.DataFrame(np.vstack(data))\n",
    "\n",
    "create_new_data(n_times=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniconda",
   "language": "python",
   "name": "miniconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
