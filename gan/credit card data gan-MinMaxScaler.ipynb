{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.cluster as cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../credit_card_kaggle/creditcard.csv')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = list(data.columns[ data.columns != 'Class' ])\n",
    "label_cols = ['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmaxscaler=MinMaxScaler()\n",
    "minmaxscaler.fit(data[data_cols])\n",
    "data[data_cols] = minmaxscaler.transform(data[data_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.935192</td>\n",
       "      <td>0.766490</td>\n",
       "      <td>0.881365</td>\n",
       "      <td>0.313023</td>\n",
       "      <td>0.763439</td>\n",
       "      <td>0.267669</td>\n",
       "      <td>0.266815</td>\n",
       "      <td>0.786444</td>\n",
       "      <td>0.475312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561184</td>\n",
       "      <td>0.522992</td>\n",
       "      <td>0.663793</td>\n",
       "      <td>0.391253</td>\n",
       "      <td>0.585122</td>\n",
       "      <td>0.394557</td>\n",
       "      <td>0.418976</td>\n",
       "      <td>0.312697</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.978542</td>\n",
       "      <td>0.770067</td>\n",
       "      <td>0.840298</td>\n",
       "      <td>0.271796</td>\n",
       "      <td>0.766120</td>\n",
       "      <td>0.262192</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>0.786298</td>\n",
       "      <td>0.453981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557840</td>\n",
       "      <td>0.480237</td>\n",
       "      <td>0.666938</td>\n",
       "      <td>0.336440</td>\n",
       "      <td>0.587290</td>\n",
       "      <td>0.446013</td>\n",
       "      <td>0.416345</td>\n",
       "      <td>0.313423</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.935217</td>\n",
       "      <td>0.753118</td>\n",
       "      <td>0.868141</td>\n",
       "      <td>0.268766</td>\n",
       "      <td>0.762329</td>\n",
       "      <td>0.281122</td>\n",
       "      <td>0.270177</td>\n",
       "      <td>0.788042</td>\n",
       "      <td>0.410603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565477</td>\n",
       "      <td>0.546030</td>\n",
       "      <td>0.678939</td>\n",
       "      <td>0.289354</td>\n",
       "      <td>0.559515</td>\n",
       "      <td>0.402727</td>\n",
       "      <td>0.415489</td>\n",
       "      <td>0.311911</td>\n",
       "      <td>0.014739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.941878</td>\n",
       "      <td>0.765304</td>\n",
       "      <td>0.868484</td>\n",
       "      <td>0.213661</td>\n",
       "      <td>0.765647</td>\n",
       "      <td>0.275559</td>\n",
       "      <td>0.266803</td>\n",
       "      <td>0.789434</td>\n",
       "      <td>0.414999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559734</td>\n",
       "      <td>0.510277</td>\n",
       "      <td>0.662607</td>\n",
       "      <td>0.223826</td>\n",
       "      <td>0.614245</td>\n",
       "      <td>0.389197</td>\n",
       "      <td>0.417669</td>\n",
       "      <td>0.314371</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.938617</td>\n",
       "      <td>0.776520</td>\n",
       "      <td>0.864251</td>\n",
       "      <td>0.269796</td>\n",
       "      <td>0.762975</td>\n",
       "      <td>0.263984</td>\n",
       "      <td>0.268968</td>\n",
       "      <td>0.782484</td>\n",
       "      <td>0.490950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561327</td>\n",
       "      <td>0.547271</td>\n",
       "      <td>0.663392</td>\n",
       "      <td>0.401270</td>\n",
       "      <td>0.566343</td>\n",
       "      <td>0.507497</td>\n",
       "      <td>0.420561</td>\n",
       "      <td>0.317490</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0  0.000000  0.935192  0.766490  0.881365  0.313023  0.763439  0.267669   \n",
       "1  0.000000  0.978542  0.770067  0.840298  0.271796  0.766120  0.262192   \n",
       "2  0.000006  0.935217  0.753118  0.868141  0.268766  0.762329  0.281122   \n",
       "3  0.000006  0.941878  0.765304  0.868484  0.213661  0.765647  0.275559   \n",
       "4  0.000012  0.938617  0.776520  0.864251  0.269796  0.762975  0.263984   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0  0.266815  0.786444  0.475312  ...  0.561184  0.522992  0.663793  0.391253   \n",
       "1  0.264875  0.786298  0.453981  ...  0.557840  0.480237  0.666938  0.336440   \n",
       "2  0.270177  0.788042  0.410603  ...  0.565477  0.546030  0.678939  0.289354   \n",
       "3  0.266803  0.789434  0.414999  ...  0.559734  0.510277  0.662607  0.223826   \n",
       "4  0.268968  0.782484  0.490950  ...  0.561327  0.547271  0.663392  0.401270   \n",
       "\n",
       "        V25       V26       V27       V28    Amount  Class  \n",
       "0  0.585122  0.394557  0.418976  0.312697  0.005824      0  \n",
       "1  0.587290  0.446013  0.416345  0.313423  0.000105      0  \n",
       "2  0.559515  0.402727  0.415489  0.311911  0.014739      0  \n",
       "3  0.614245  0.389197  0.417669  0.314371  0.004807      0  \n",
       "4  0.566343  0.507497  0.420561  0.317490  0.002724      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc461c2bad0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST # Training dataset\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_block(input_dim, output_dim):\n",
    "    '''\n",
    "    Function for returning a block of the generator's neural network\n",
    "    given input and output dimensions.\n",
    "    Parameters:\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        output_dim: the dimension of the output vector, a scalar\n",
    "    Returns:\n",
    "        a generator neural network layer, with a linear transformation \n",
    "          followed by a batch normalization and then a relu activation\n",
    "    '''\n",
    "    return nn.Sequential(\n",
    "        # https://pytorch.org/docs/stable/nn.html.\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Verify the generator block function\n",
    "def test_gen_block(in_features, out_features, num_test=1000):\n",
    "    block = get_generator_block(in_features, out_features)\n",
    "\n",
    "    # Check the three parts\n",
    "    assert len(block) == 3\n",
    "    assert type(block[0]) == nn.Linear\n",
    "    assert type(block[1]) == nn.BatchNorm1d\n",
    "    assert type(block[2]) == nn.ReLU\n",
    "    \n",
    "    # Check the output shape\n",
    "    test_input = torch.randn(num_test, in_features)\n",
    "    test_output = block(test_input)\n",
    "    assert tuple(test_output.shape) == (num_test, out_features)\n",
    "    assert test_output.std() > 0.55\n",
    "    assert test_output.std() < 0.65\n",
    "\n",
    "test_gen_block(25, 12)\n",
    "test_gen_block(15, 28)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Generator\n",
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        im_dim: the dimension of the images, fitted for the dataset used, a scalar\n",
    "          (MNIST images are 28 x 28 = 784 so that is your default)\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, z_dim=10, im_dim=30, hidden_dim=20):\n",
    "        super(Generator, self).__init__()\n",
    "        # Build the neural network\n",
    "        self.gen = nn.Sequential(\n",
    "            get_generator_block(z_dim, hidden_dim),\n",
    "            get_generator_block(hidden_dim, hidden_dim * 2),\n",
    "            get_generator_block(hidden_dim * 2, hidden_dim * 4),\n",
    "            get_generator_block(hidden_dim * 4, hidden_dim * 8),\n",
    "            # There is a dropdown with hints if you need them!\n",
    "            nn.Linear(hidden_dim * 8, im_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
    "        returns generated images.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
    "        '''\n",
    "        return self.gen(noise)\n",
    "    \n",
    "    # Needed for grading\n",
    "    def get_gen(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            the sequential model\n",
    "        '''\n",
    "        return self.gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Verify the generator class\n",
    "def test_generator(z_dim, im_dim, hidden_dim, num_test=10000):\n",
    "    gen = Generator(z_dim, im_dim, hidden_dim).get_gen()\n",
    "    \n",
    "    # Check there are six modules in the sequential part\n",
    "    assert len(gen) == 6\n",
    "    assert str(gen.__getitem__(4)).replace(' ', '') == f'Linear(in_features={hidden_dim * 8},out_features={im_dim},bias=True)'\n",
    "    assert str(gen.__getitem__(5)).replace(' ', '') == 'Sigmoid()'\n",
    "    test_input = torch.randn(num_test, z_dim)\n",
    "    test_output = gen(test_input)\n",
    "\n",
    "    # Check that the output shape is correct\n",
    "    assert tuple(test_output.shape) == (num_test, im_dim)\n",
    "    assert test_output.max() < 1, \"Make sure to use a sigmoid\"\n",
    "    assert test_output.min() > 0, \"Make sure to use a sigmoid\"\n",
    "    assert test_output.std() > 0.05, \"Don't use batchnorm here\"\n",
    "    assert test_output.std() < 0.15, \"Don't use batchnorm here\"\n",
    "\n",
    "test_generator(5, 10, 20)\n",
    "test_generator(20, 8, 24)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(n_samples, z_dim, device='cpu'):\n",
    "    '''\n",
    "    Function for creating noise vectors: Given the dimensions (n_samples, z_dim),\n",
    "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
    "    Parameters:\n",
    "        n_samples: the number of samples to generate, a scalar\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        device: the device type\n",
    "    '''\n",
    "    # NOTE: To use this on GPU with device='cuda', make sure to pass the device \n",
    "    # argument to the function you use to generate the noise.\n",
    "    return torch.randn(n_samples,z_dim,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Verify the noise vector function\n",
    "def test_get_noise(n_samples, z_dim, device='cpu'):\n",
    "    noise = get_noise(n_samples, z_dim, device)\n",
    "    # Make sure a normal distribution was used\n",
    "    assert tuple(noise.shape) == (n_samples, z_dim)\n",
    "    assert torch.abs(noise.std() - torch.tensor(1.0)) < 0.01\n",
    "    assert str(noise.device).startswith(device)\n",
    "\n",
    "test_get_noise(1000, 100, 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    test_get_noise(1000, 32, 'cuda')\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_discriminator_block\n",
    "def get_discriminator_block(input_dim, output_dim):\n",
    "    '''\n",
    "    Discriminator Block\n",
    "    Function for returning a neural network of the discriminator given input and output dimensions.\n",
    "    Parameters:\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        output_dim: the dimension of the output vector, a scalar\n",
    "    Returns:\n",
    "        a discriminator neural network layer, with a linear transformation \n",
    "          followed by an nn.LeakyReLU activation with negative slope of 0.2 \n",
    "          (https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html)\n",
    "    '''\n",
    "    return nn.Sequential(\n",
    "         nn.Linear(input_dim, output_dim), #Layer 1\n",
    "         nn.LeakyReLU(0.2, inplace=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "#  Verify the discriminator block function\n",
    "def test_disc_block(in_features, out_features, num_test=10000):\n",
    "    block = get_discriminator_block(in_features, out_features)\n",
    "\n",
    "    # Check there are two parts\n",
    "    assert len(block) == 2\n",
    "    test_input = torch.randn(num_test, in_features)\n",
    "    test_output = block(test_input)\n",
    "\n",
    "    # Check that the shape is right\n",
    "    assert tuple(test_output.shape) == (num_test, out_features)\n",
    "    \n",
    "    # Check that the LeakyReLU slope is about 0.2\n",
    "    assert -test_output.min() / test_output.max() > 0.1\n",
    "    assert -test_output.min() / test_output.max() < 0.3\n",
    "    assert test_output.std() > 0.3\n",
    "    assert test_output.std() < 0.5\n",
    "    \n",
    "    assert str(block.__getitem__(0)).replace(' ', '') == f'Linear(in_features={in_features},out_features={out_features},bias=True)'        \n",
    "    assert str(block.__getitem__(1)).replace(' ', '').replace(',inplace=True', '') == 'LeakyReLU(negative_slope=0.2)'\n",
    "\n",
    "\n",
    "test_disc_block(25, 12)\n",
    "test_disc_block(15, 28)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Values:\n",
    "        im_dim: the dimension of the images, fitted for the dataset used, a scalar\n",
    "            (MNIST images are 28x28 = 784 so that is your default)\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, im_dim=30, hidden_dim=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            get_discriminator_block(im_dim, hidden_dim * 4),\n",
    "            get_discriminator_block(hidden_dim * 4, hidden_dim * 2),\n",
    "            get_discriminator_block(hidden_dim * 2, hidden_dim),\n",
    "            # Hint: You want to transform the final output into a single value,\n",
    "            #       so add one more linear map.\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of the discriminator: Given an image tensor, \n",
    "        returns a 1-dimension tensor representing fake/real.\n",
    "        Parameters:\n",
    "            image: a flattened image tensor with dimension (im_dim)\n",
    "        '''\n",
    "        return self.disc(image.float())\n",
    "    \n",
    "    # Needed for grading\n",
    "    def get_disc(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            the sequential model\n",
    "        '''\n",
    "        return self.disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Verify the discriminator class\n",
    "def test_discriminator(z_dim, hidden_dim, num_test=100):\n",
    "    \n",
    "    disc = Discriminator(z_dim, hidden_dim).get_disc()\n",
    "\n",
    "    # Check there are three parts\n",
    "    assert len(disc) == 4\n",
    "    assert type(disc.__getitem__(3)) == nn.Linear\n",
    "\n",
    "    # Check the linear layer is correct\n",
    "    test_input = torch.randn(num_test, z_dim)\n",
    "    test_output = disc(test_input)\n",
    "    assert tuple(test_output.shape) == (num_test, 1)\n",
    "\n",
    "test_discriminator(5, 10)\n",
    "test_discriminator(20, 8)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your parameters\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 10\n",
    "z_dim = 10\n",
    "hidden_dim=20\n",
    "display_step = 500\n",
    "batch_size = 16\n",
    "lr = 0.00001\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "disc = Discriminator().to(device) \n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_disc_loss\n",
    "def get_disc_loss(gen, disc, criterion, real, num_images, z_dim, device):\n",
    "    '''\n",
    "    Return the loss of the discriminator given inputs.\n",
    "    Parameters:\n",
    "        gen: the generator model, which returns an image given z-dimensional noise\n",
    "        disc: the discriminator model, which returns a single-dimensional prediction of real/fake\n",
    "        criterion: the loss function, which should be used to compare \n",
    "               the discriminator's predictions to the ground truth reality of the images \n",
    "               (e.g. fake = 0, real = 1)\n",
    "        real: a batch of real images\n",
    "        num_images: the number of images the generator should produce, \n",
    "                which is also the length of the real images\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        device: the device type\n",
    "    Returns:\n",
    "        disc_loss: a torch scalar loss value for the current batch\n",
    "    '''\n",
    "    #     These are the steps you will need to complete:\n",
    "    #       1) Create noise vectors and generate a batch (num_images) of fake images. \n",
    "    #            Make sure to pass the device argument to the noise.\n",
    "    #       2) Get the discriminator's prediction of the fake image \n",
    "    #            and calculate the loss. Don't forget to detach the generator!\n",
    "    #            (Remember the loss function you set earlier -- criterion. You need a \n",
    "    #            'ground truth' tensor in order to calculate the loss. \n",
    "    #            For example, a ground truth tensor for a fake image is all zeros.)\n",
    "    #       3) Get the discriminator's prediction of the real image and calculate the loss.\n",
    "    #       4) Calculate the discriminator's loss by averaging the real and fake loss\n",
    "    #            and set it to disc_loss.\n",
    "\n",
    "    fake_noise = get_noise(num_images, z_dim, device=device)\n",
    "    fake = gen(fake_noise)\n",
    "    disc_fake_pred = disc(fake.detach())\n",
    "    disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "    disc_real_pred = disc(real)\n",
    "    disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "    disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "    return disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features= data[data_cols]\n",
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = data[['Class']]\n",
    "type(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train = TensorDataset(torch.tensor(features.values,dtype=torch.float64), torch.tensor(targets.values,dtype=torch.float64))\n",
    "train_loader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=DataLoader(train, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "def test_disc_reasonable(num_images=10):\n",
    "    z_dim = 64\n",
    "    gen = torch.zeros_like\n",
    "    disc = nn.Identity()\n",
    "    criterion = torch.mul # Multiply\n",
    "    real = torch.ones(num_images, 1)\n",
    "    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu')\n",
    "    assert tuple(disc_loss.shape) == (num_images, z_dim)\n",
    "    assert torch.all(torch.abs(disc_loss - 0.5) < 1e-5)\n",
    "\n",
    "    gen = torch.ones_like\n",
    "    disc = nn.Identity()\n",
    "    criterion = torch.mul # Multiply\n",
    "    real = torch.zeros(num_images, 1)\n",
    "    assert torch.all(torch.abs(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu')) < 1e-5)\n",
    "\n",
    "def test_disc_loss(max_tests = 10):\n",
    "    z_dim = 64\n",
    "    gen = Generator(z_dim).to(device)\n",
    "    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "    disc = Discriminator().to(device) \n",
    "    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "    num_steps = 0\n",
    "    for real in dataloader:\n",
    "        real=real[0]\n",
    "        cur_batch_size = len(real)\n",
    "        real = real.view(cur_batch_size, -1).to(device)\n",
    "#         real=torch.tensor(real).view(cur_batch_size, -1).to(device)\n",
    "\n",
    "        ### Update discriminator ###\n",
    "        # Zero out the gradient before backpropagation\n",
    "        disc_opt.zero_grad()\n",
    "\n",
    "        # Calculate discriminator loss\n",
    "        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)\n",
    "        assert (disc_loss - 0.68).abs() < 0.05\n",
    "\n",
    "        # Update gradients\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "\n",
    "        # Check that they detached correctly\n",
    "        assert gen.gen[0][0].weight.grad is None\n",
    "\n",
    "        # Update optimizer\n",
    "        old_weight = disc.disc[0][0].weight.data.clone()\n",
    "        disc_opt.step()\n",
    "        new_weight = disc.disc[0][0].weight.data\n",
    "        \n",
    "        # Check that some discriminator weights changed\n",
    "        assert not torch.all(torch.eq(old_weight, new_weight))\n",
    "        num_steps += 1\n",
    "        if num_steps >= max_tests:\n",
    "            break\n",
    "\n",
    "test_disc_reasonable()\n",
    "test_disc_loss()\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_loss(gen, disc, criterion, num_images, z_dim, device):\n",
    "    '''\n",
    "    Return the loss of the generator given inputs.\n",
    "    Parameters:\n",
    "        gen: the generator model, which returns an image given z-dimensional noise\n",
    "        disc: the discriminator model, which returns a single-dimensional prediction of real/fake\n",
    "        criterion: the loss function, which should be used to compare \n",
    "               the discriminator's predictions to the ground truth reality of the images \n",
    "               (e.g. fake = 0, real = 1)\n",
    "        num_images: the number of images the generator should produce, \n",
    "                which is also the length of the real images\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        device: the device type\n",
    "    Returns:\n",
    "        gen_loss: a torch scalar loss value for the current batch\n",
    "    '''\n",
    "    #     These are the steps you will need to complete:\n",
    "    #       1) Create noise vectors and generate a batch of fake images. \n",
    "    #           Remember to pass the device argument to the get_noise function.\n",
    "    #       2) Get the discriminator's prediction of the fake image.\n",
    "    #       3) Calculate the generator's loss. Remember the generator wants\n",
    "    #          the discriminator to think that its fake images are real\n",
    "    #     *Important*: You should NOT write your own loss function here - use criterion(pred, true)!\n",
    "\n",
    "    fake_noise = get_noise(num_images, z_dim, device=device)\n",
    "    fake = gen(fake_noise)\n",
    "    disc_fake_pred = disc(fake)\n",
    "    gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "def test_gen_reasonable(num_images=10):\n",
    "    z_dim = 64\n",
    "    gen = torch.zeros_like\n",
    "    disc = nn.Identity()\n",
    "    criterion = torch.mul # Multiply\n",
    "    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, 'cpu')\n",
    "    assert torch.all(torch.abs(gen_loss_tensor) < 1e-5)\n",
    "    #Verify shape. Related to gen_noise parametrization\n",
    "    assert tuple(gen_loss_tensor.shape) == (num_images, z_dim)\n",
    "\n",
    "    gen = torch.ones_like\n",
    "    disc = nn.Identity()\n",
    "    criterion = torch.mul # Multiply\n",
    "    real = torch.zeros(num_images, 1)\n",
    "    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, 'cpu')\n",
    "    assert torch.all(torch.abs(gen_loss_tensor - 1) < 1e-5)\n",
    "    #Verify shape. Related to gen_noise parametrization\n",
    "    assert tuple(gen_loss_tensor.shape) == (num_images, z_dim)\n",
    "    \n",
    "\n",
    "def test_gen_loss(num_images):\n",
    "    z_dim = 64\n",
    "    gen = Generator(z_dim).to(device)\n",
    "    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "    disc = Discriminator().to(device) \n",
    "    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "    \n",
    "    gen_loss = get_gen_loss(gen, disc, criterion, num_images, z_dim, device)\n",
    "    \n",
    "    # Check that the loss is reasonable\n",
    "    assert (gen_loss - 0.7).abs() < 0.1\n",
    "    gen_loss.backward()\n",
    "    old_weight = gen.gen[0][0].weight.clone()\n",
    "    gen_opt.step()\n",
    "    new_weight = gen.gen[0][0].weight\n",
    "    assert not torch.all(torch.eq(old_weight, new_weight))\n",
    "\n",
    "\n",
    "test_gen_reasonable(10)\n",
    "test_gen_loss(18)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743c4266c8904f788e634869992790ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: Generator loss: 0.6937802932262424, discriminator loss: 0.6956009175777437\n",
      "Step 1000: Generator loss: 0.6942389067411421, discriminator loss: 0.6904130228757862\n",
      "Step 1500: Generator loss: 0.6974336773157117, discriminator loss: 0.6881816349029537\n",
      "Step 2000: Generator loss: 0.6987867375612256, discriminator loss: 0.6868392893075951\n",
      "Step 2500: Generator loss: 0.697244793534279, discriminator loss: 0.687607634186745\n",
      "Step 3000: Generator loss: 0.6939527454376218, discriminator loss: 0.6890402016639698\n",
      "Step 3500: Generator loss: 0.6919104378223421, discriminator loss: 0.6910886815786361\n",
      "Step 4000: Generator loss: 0.6926310302019107, discriminator loss: 0.6929280650615686\n",
      "Step 4500: Generator loss: 0.6947240042686457, discriminator loss: 0.6932250317335127\n",
      "Step 5000: Generator loss: 0.6947778234481807, discriminator loss: 0.6927064278125759\n",
      "Step 5500: Generator loss: 0.6942368317842488, discriminator loss: 0.6926510924100868\n",
      "Step 6000: Generator loss: 0.6949858790636061, discriminator loss: 0.6927698823213572\n",
      "Step 6500: Generator loss: 0.6944614683389656, discriminator loss: 0.692500219702721\n",
      "Step 7000: Generator loss: 0.693324122786522, discriminator loss: 0.6928113946914669\n",
      "Step 7500: Generator loss: 0.6921381385326384, discriminator loss: 0.693465003609657\n",
      "Step 8000: Generator loss: 0.6896864407062525, discriminator loss: 0.6938384175300601\n",
      "Step 8500: Generator loss: 0.6935965782403952, discriminator loss: 0.6936736121177673\n",
      "Step 9000: Generator loss: 0.6953777232170111, discriminator loss: 0.6931993044614785\n",
      "Step 9500: Generator loss: 0.6946757321357729, discriminator loss: 0.6930464072227478\n",
      "Step 10000: Generator loss: 0.6933319705724718, discriminator loss: 0.6931859350204473\n",
      "Step 10500: Generator loss: 0.6929707328081133, discriminator loss: 0.6936241903305053\n",
      "Step 11000: Generator loss: 0.6929817863702775, discriminator loss: 0.6936188434362414\n",
      "Step 11500: Generator loss: 0.6933840999603271, discriminator loss: 0.6933717919588093\n",
      "Step 12000: Generator loss: 0.6935320900678623, discriminator loss: 0.6933099895715721\n",
      "Step 12500: Generator loss: 0.6941759667396545, discriminator loss: 0.6932247004508971\n",
      "Step 13000: Generator loss: 0.6929695005416865, discriminator loss: 0.6931980576515183\n",
      "Step 13500: Generator loss: 0.6926169475316999, discriminator loss: 0.693405975222587\n",
      "Step 14000: Generator loss: 0.6927965104579921, discriminator loss: 0.6933230434656141\n",
      "Step 14500: Generator loss: 0.6930444437265393, discriminator loss: 0.6933727835416796\n",
      "Step 15000: Generator loss: 0.6933249011039728, discriminator loss: 0.6932260482311253\n",
      "Step 15500: Generator loss: 0.693420313119888, discriminator loss: 0.6930180342197424\n",
      "Step 16000: Generator loss: 0.6930030444860462, discriminator loss: 0.6932145400047304\n",
      "Step 16500: Generator loss: 0.6930222266912459, discriminator loss: 0.693220829963684\n",
      "Step 17000: Generator loss: 0.6933448891639717, discriminator loss: 0.6932646421194072\n",
      "Step 17500: Generator loss: 0.6931259715557088, discriminator loss: 0.6931053017377853\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce8f2199ac940f0a1308a7c23db5758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18000: Generator loss: 0.6931782385110858, discriminator loss: 0.6931095813512808\n",
      "Step 18500: Generator loss: 0.69332435798645, discriminator loss: 0.6931282669305797\n",
      "Step 19000: Generator loss: 0.6931697527170185, discriminator loss: 0.6930964077711106\n",
      "Step 19500: Generator loss: 0.693090586662293, discriminator loss: 0.6932088235616682\n",
      "Step 20000: Generator loss: 0.6931847213506706, discriminator loss: 0.6930512892007828\n",
      "Step 20500: Generator loss: 0.6932008289098733, discriminator loss: 0.6931079108715057\n",
      "Step 21000: Generator loss: 0.6931259059906013, discriminator loss: 0.6932020499706267\n",
      "Step 21500: Generator loss: 0.6929983360767363, discriminator loss: 0.6932497819662097\n",
      "Step 22000: Generator loss: 0.6930529710054396, discriminator loss: 0.6932317289113998\n",
      "Step 22500: Generator loss: 0.693133000850678, discriminator loss: 0.6931378440856931\n",
      "Step 23000: Generator loss: 0.6931802737712859, discriminator loss: 0.693065739750862\n",
      "Step 23500: Generator loss: 0.6932120745182033, discriminator loss: 0.693134388208389\n",
      "Step 24000: Generator loss: 0.6932369973659522, discriminator loss: 0.6930234595537184\n",
      "Step 24500: Generator loss: 0.6931637068986891, discriminator loss: 0.6931084247827537\n",
      "Step 25000: Generator loss: 0.6930518629550938, discriminator loss: 0.6932626143693924\n",
      "Step 25500: Generator loss: 0.6931178904771803, discriminator loss: 0.6931757117509849\n",
      "Step 26000: Generator loss: 0.6932294073104857, discriminator loss: 0.6930758454799659\n",
      "Step 26500: Generator loss: 0.6930681072473529, discriminator loss: 0.6932134205102919\n",
      "Step 27000: Generator loss: 0.6931444319486617, discriminator loss: 0.6931355947256087\n",
      "Step 27500: Generator loss: 0.693125883221626, discriminator loss: 0.6932650498151779\n",
      "Step 28000: Generator loss: 0.6931427491903303, discriminator loss: 0.6931667623519897\n",
      "Step 28500: Generator loss: 0.6932034205198295, discriminator loss: 0.6930529869794849\n",
      "Step 29000: Generator loss: 0.6932256898880002, discriminator loss: 0.6931458213329321\n",
      "Step 29500: Generator loss: 0.69306994664669, discriminator loss: 0.6932640789747239\n",
      "Step 30000: Generator loss: 0.6930707882642742, discriminator loss: 0.6931787976026541\n",
      "Step 30500: Generator loss: 0.6933047184944156, discriminator loss: 0.6930061577558523\n",
      "Step 31000: Generator loss: 0.6931730422973623, discriminator loss: 0.6930939930677418\n",
      "Step 31500: Generator loss: 0.6930039931535715, discriminator loss: 0.6932369416952131\n",
      "Step 32000: Generator loss: 0.6930969285964963, discriminator loss: 0.693202892065048\n",
      "Step 32500: Generator loss: 0.6931916606426234, discriminator loss: 0.6931292865276344\n",
      "Step 33000: Generator loss: 0.6931725420951835, discriminator loss: 0.6931241040229787\n",
      "Step 33500: Generator loss: 0.6930887142419814, discriminator loss: 0.6931239087581638\n",
      "Step 34000: Generator loss: 0.6931858278512953, discriminator loss: 0.6931380764245981\n",
      "Step 34500: Generator loss: 0.6931784998178482, discriminator loss: 0.6931272597312933\n",
      "Step 35000: Generator loss: 0.6929889588356014, discriminator loss: 0.6932114169597627\n",
      "Step 35500: Generator loss: 0.693114791035652, discriminator loss: 0.6932065495252611\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567e693ba7924270a1e467b75416e4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 36000: Generator loss: 0.6932369309663781, discriminator loss: 0.6931197907924643\n",
      "Step 36500: Generator loss: 0.6933239244222639, discriminator loss: 0.6930213038921352\n",
      "Step 37000: Generator loss: 0.6930722787380218, discriminator loss: 0.6931658937931063\n",
      "Step 37500: Generator loss: 0.693096520662308, discriminator loss: 0.6932775589227674\n",
      "Step 38000: Generator loss: 0.6931644045114516, discriminator loss: 0.6931578643321993\n",
      "Step 38500: Generator loss: 0.6932294185161586, discriminator loss: 0.6931076608896259\n",
      "Step 39000: Generator loss: 0.6932572572231296, discriminator loss: 0.6931407837867739\n",
      "Step 39500: Generator loss: 0.6931188776493075, discriminator loss: 0.6932086136341091\n",
      "Step 40000: Generator loss: 0.6930559997558595, discriminator loss: 0.693181731939316\n",
      "Step 40500: Generator loss: 0.6931619434356692, discriminator loss: 0.6930731683969503\n",
      "Step 41000: Generator loss: 0.693241631746291, discriminator loss: 0.6930803045034409\n",
      "Step 41500: Generator loss: 0.6930148890018454, discriminator loss: 0.6932279783487317\n",
      "Step 42000: Generator loss: 0.6931771242618561, discriminator loss: 0.6931812307834622\n",
      "Step 42500: Generator loss: 0.6931472841501234, discriminator loss: 0.6932594573497772\n",
      "Step 43000: Generator loss: 0.6931202312707905, discriminator loss: 0.6931691128015514\n",
      "Step 43500: Generator loss: 0.693122435927391, discriminator loss: 0.6930938361883163\n",
      "Step 44000: Generator loss: 0.6931790221929545, discriminator loss: 0.6931373666524889\n",
      "Step 44500: Generator loss: 0.693212595224381, discriminator loss: 0.6931269589662548\n",
      "Step 45000: Generator loss: 0.6929770022630688, discriminator loss: 0.6932548120021814\n",
      "Step 45500: Generator loss: 0.6931404421329499, discriminator loss: 0.6931693497896195\n",
      "Step 46000: Generator loss: 0.6930505793094646, discriminator loss: 0.6930997101068502\n",
      "Step 46500: Generator loss: 0.6932345023155212, discriminator loss: 0.6930784080028537\n",
      "Step 47000: Generator loss: 0.6932178165912629, discriminator loss: 0.6932032811641698\n",
      "Step 47500: Generator loss: 0.6930078803300859, discriminator loss: 0.6932113029956823\n",
      "Step 48000: Generator loss: 0.6930191488265987, discriminator loss: 0.6932557154893882\n",
      "Step 48500: Generator loss: 0.6932845194339745, discriminator loss: 0.6930771334171294\n",
      "Step 49000: Generator loss: 0.6931749770641321, discriminator loss: 0.6931678897142403\n",
      "Step 49500: Generator loss: 0.6933270258903506, discriminator loss: 0.6931853359937672\n",
      "Step 50000: Generator loss: 0.6933057302236556, discriminator loss: 0.6930588846206669\n",
      "Step 50500: Generator loss: 0.6930838323831555, discriminator loss: 0.6931648763418194\n",
      "Step 51000: Generator loss: 0.6930479868650437, discriminator loss: 0.6932248736619953\n",
      "Step 51500: Generator loss: 0.6929550876617433, discriminator loss: 0.6931392836570736\n",
      "Step 52000: Generator loss: 0.6931009060144431, discriminator loss: 0.6931357651948928\n",
      "Step 52500: Generator loss: 0.6933465173244479, discriminator loss: 0.6931226619482037\n",
      "Step 53000: Generator loss: 0.6931972239017484, discriminator loss: 0.6931553808450702\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c099597e014ef49759af556e452ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 53500: Generator loss: 0.6932069171667098, discriminator loss: 0.6932012546062472\n",
      "Step 54000: Generator loss: 0.6930380769968041, discriminator loss: 0.6931514444351194\n",
      "Step 54500: Generator loss: 0.6927580029964456, discriminator loss: 0.6931317461729051\n",
      "Step 55000: Generator loss: 0.6931819849014272, discriminator loss: 0.6931507288217551\n",
      "Step 55500: Generator loss: 0.693349136829377, discriminator loss: 0.6931093963384621\n",
      "Step 56000: Generator loss: 0.6929059412479405, discriminator loss: 0.6931620485782618\n",
      "Step 56500: Generator loss: 0.6931610492467883, discriminator loss: 0.6931422309875488\n",
      "Step 57000: Generator loss: 0.693561178684235, discriminator loss: 0.6930864367485055\n",
      "Step 57500: Generator loss: 0.6929484236240379, discriminator loss: 0.6931564953327186\n",
      "Step 58000: Generator loss: 0.6931137953996654, discriminator loss: 0.6932242262363435\n",
      "Step 58500: Generator loss: 0.6932656743526461, discriminator loss: 0.6931242840290069\n",
      "Step 59000: Generator loss: 0.6930378698110583, discriminator loss: 0.6931942297220236\n",
      "Step 59500: Generator loss: 0.6932078704833987, discriminator loss: 0.6931097818613061\n",
      "Step 60000: Generator loss: 0.6932323329448704, discriminator loss: 0.6931248502731335\n",
      "Step 60500: Generator loss: 0.6930516695976259, discriminator loss: 0.6931945103406901\n",
      "Step 61000: Generator loss: 0.6932592401504524, discriminator loss: 0.693138732910156\n",
      "Step 61500: Generator loss: 0.6930593180656435, discriminator loss: 0.6931470155715942\n",
      "Step 62000: Generator loss: 0.6930469566583634, discriminator loss: 0.6931413965225225\n",
      "Step 62500: Generator loss: 0.6933022853136064, discriminator loss: 0.6931158987283708\n",
      "Step 63000: Generator loss: 0.6932179549932482, discriminator loss: 0.6931204154491423\n",
      "Step 63500: Generator loss: 0.6932349460124972, discriminator loss: 0.6931545077562326\n",
      "Step 64000: Generator loss: 0.6928767460584636, discriminator loss: 0.693262484312058\n",
      "Step 64500: Generator loss: 0.6931316219568259, discriminator loss: 0.6931044090986255\n",
      "Step 65000: Generator loss: 0.6932611775398252, discriminator loss: 0.6930788216590882\n",
      "Step 65500: Generator loss: 0.6930761569738385, discriminator loss: 0.6932177431583408\n",
      "Step 66000: Generator loss: 0.693140473604202, discriminator loss: 0.6930963233709331\n",
      "Step 66500: Generator loss: 0.6932651381492616, discriminator loss: 0.693063641428948\n",
      "Step 67000: Generator loss: 0.6929991014003756, discriminator loss: 0.6932749806642531\n",
      "Step 67500: Generator loss: 0.6931023992300033, discriminator loss: 0.6932554010152817\n",
      "Step 68000: Generator loss: 0.693336900949478, discriminator loss: 0.6929928275346756\n",
      "Step 68500: Generator loss: 0.6932139686346059, discriminator loss: 0.6931157476902008\n",
      "Step 69000: Generator loss: 0.6930887235403058, discriminator loss: 0.693217344880104\n",
      "Step 69500: Generator loss: 0.6932241616249083, discriminator loss: 0.6931328916549682\n",
      "Step 70000: Generator loss: 0.6931457834243773, discriminator loss: 0.6931329606771465\n",
      "Step 70500: Generator loss: 0.6930672756433492, discriminator loss: 0.6931914885044094\n",
      "Step 71000: Generator loss: 0.6931487902402884, discriminator loss: 0.6931339426040652\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13a538977df471992db413396ee4a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 71500: Generator loss: 0.6931528751850128, discriminator loss: 0.6930872075557712\n",
      "Step 72000: Generator loss: 0.693116456270218, discriminator loss: 0.6931922713518137\n",
      "Step 72500: Generator loss: 0.6931819571256636, discriminator loss: 0.6931742216348649\n",
      "Step 73000: Generator loss: 0.6932364553213118, discriminator loss: 0.6930702828168868\n",
      "Step 73500: Generator loss: 0.6932396502494815, discriminator loss: 0.6931118789911264\n",
      "Step 74000: Generator loss: 0.6930022830963143, discriminator loss: 0.6932291146516802\n",
      "Step 74500: Generator loss: 0.693064860105515, discriminator loss: 0.6931170156002042\n",
      "Step 75000: Generator loss: 0.6932318074703213, discriminator loss: 0.6930702426433558\n",
      "Step 75500: Generator loss: 0.693140394330024, discriminator loss: 0.6931773524284366\n",
      "Step 76000: Generator loss: 0.6930951344966882, discriminator loss: 0.6932227196693421\n",
      "Step 76500: Generator loss: 0.6933082357645027, discriminator loss: 0.6931141824722292\n",
      "Step 77000: Generator loss: 0.6931371147632599, discriminator loss: 0.6930581738948823\n",
      "Step 77500: Generator loss: 0.6930827498435972, discriminator loss: 0.6931811429262165\n",
      "Step 78000: Generator loss: 0.6931354597806925, discriminator loss: 0.6932100629806516\n",
      "Step 78500: Generator loss: 0.6931689580678936, discriminator loss: 0.6931173442602153\n",
      "Step 79000: Generator loss: 0.6932555397748944, discriminator loss: 0.6930746477842337\n",
      "Step 79500: Generator loss: 0.6933613090515133, discriminator loss: 0.6931338571310052\n",
      "Step 80000: Generator loss: 0.6926930118799217, discriminator loss: 0.6932390084266661\n",
      "Step 80500: Generator loss: 0.6932532171010974, discriminator loss: 0.6932227545976639\n",
      "Step 81000: Generator loss: 0.6932464857101448, discriminator loss: 0.6930583142042167\n",
      "Step 81500: Generator loss: 0.6930276815891268, discriminator loss: 0.6930228976011276\n",
      "Step 82000: Generator loss: 0.6930423281192781, discriminator loss: 0.6931671992540359\n",
      "Step 82500: Generator loss: 0.6931422950029371, discriminator loss: 0.693304637551308\n",
      "Step 83000: Generator loss: 0.6932246577739714, discriminator loss: 0.6931213778257372\n",
      "Step 83500: Generator loss: 0.6933536884784695, discriminator loss: 0.6930249341726303\n",
      "Step 84000: Generator loss: 0.6930895838737489, discriminator loss: 0.6931874815225597\n",
      "Step 84500: Generator loss: 0.6930721036195756, discriminator loss: 0.6932179421186447\n",
      "Step 85000: Generator loss: 0.6933017668724059, discriminator loss: 0.6930732457637782\n",
      "Step 85500: Generator loss: 0.6931036800146104, discriminator loss: 0.6931404156684875\n",
      "Step 86000: Generator loss: 0.6930474696159364, discriminator loss: 0.6932297086715704\n",
      "Step 86500: Generator loss: 0.6931562507152557, discriminator loss: 0.6931709495782845\n",
      "Step 87000: Generator loss: 0.693188555598259, discriminator loss: 0.6930524260997775\n",
      "Step 87500: Generator loss: 0.6931814084053043, discriminator loss: 0.6930785553455356\n",
      "Step 88000: Generator loss: 0.693021852731704, discriminator loss: 0.6932457106113435\n",
      "Step 88500: Generator loss: 0.6931578429937355, discriminator loss: 0.6932271858453747\n",
      "Step 89000: Generator loss: 0.6932269878387453, discriminator loss: 0.6931072486639025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c038f890596d4320a726efc4c9bd01a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 89500: Generator loss: 0.6931485297679906, discriminator loss: 0.6930755022764207\n",
      "Step 90000: Generator loss: 0.6931490972042088, discriminator loss: 0.693125736713409\n",
      "Step 90500: Generator loss: 0.6932704430818553, discriminator loss: 0.6930914655923843\n",
      "Step 91000: Generator loss: 0.6932102364301675, discriminator loss: 0.693111545324326\n",
      "Step 91500: Generator loss: 0.6930682387351994, discriminator loss: 0.6931170759201051\n",
      "Step 92000: Generator loss: 0.6929907631874079, discriminator loss: 0.6932034519910819\n",
      "Step 92500: Generator loss: 0.693193194150925, discriminator loss: 0.6931760228872301\n",
      "Step 93000: Generator loss: 0.6932525177001961, discriminator loss: 0.6930727956295019\n",
      "Step 93500: Generator loss: 0.6931763848066326, discriminator loss: 0.6931273059844972\n",
      "Step 94000: Generator loss: 0.6930645072460173, discriminator loss: 0.6932639986276626\n",
      "Step 94500: Generator loss: 0.6931824064254766, discriminator loss: 0.6931614285707466\n",
      "Step 95000: Generator loss: 0.6932680002450946, discriminator loss: 0.6930261056423196\n",
      "Step 95500: Generator loss: 0.69318671798706, discriminator loss: 0.6930793792009362\n",
      "Step 96000: Generator loss: 0.6931118130683902, discriminator loss: 0.6932584676742554\n",
      "Step 96500: Generator loss: 0.6931789387464522, discriminator loss: 0.6931471865177158\n",
      "Step 97000: Generator loss: 0.6931022111177446, discriminator loss: 0.6931078333854674\n",
      "Step 97500: Generator loss: 0.6929601050615316, discriminator loss: 0.6931158435344694\n",
      "Step 98000: Generator loss: 0.6930644137859349, discriminator loss: 0.6931629658937462\n",
      "Step 98500: Generator loss: 0.6933829182386393, discriminator loss: 0.6931148811578751\n",
      "Step 99000: Generator loss: 0.6932462698221212, discriminator loss: 0.6930609931945803\n",
      "Step 99500: Generator loss: 0.6931092865467068, discriminator loss: 0.6931876308918005\n",
      "Step 100000: Generator loss: 0.693081985116005, discriminator loss: 0.6931787433624276\n",
      "Step 100500: Generator loss: 0.6931406478881831, discriminator loss: 0.6931931879520418\n",
      "Step 101000: Generator loss: 0.6932684535980225, discriminator loss: 0.6930541098117834\n",
      "Step 101500: Generator loss: 0.693256272792816, discriminator loss: 0.6931323838233949\n",
      "Step 102000: Generator loss: 0.693166440486907, discriminator loss: 0.6931928812265394\n",
      "Step 102500: Generator loss: 0.6930070294141768, discriminator loss: 0.6931561679840088\n",
      "Step 103000: Generator loss: 0.6930736045837402, discriminator loss: 0.6931698693037034\n",
      "Step 103500: Generator loss: 0.6929934870004653, discriminator loss: 0.6931475776433943\n",
      "Step 104000: Generator loss: 0.693309473633766, discriminator loss: 0.6931493314504619\n",
      "Step 104500: Generator loss: 0.6931938775777823, discriminator loss: 0.6931655651330947\n",
      "Step 105000: Generator loss: 0.6929418197870256, discriminator loss: 0.6932139213085169\n",
      "Step 105500: Generator loss: 0.6931241056919097, discriminator loss: 0.6931294401884079\n",
      "Step 106000: Generator loss: 0.6932117204666136, discriminator loss: 0.6932184129953384\n",
      "Step 106500: Generator loss: 0.6934892255067826, discriminator loss: 0.6930902985334396\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6acee1028ec44e4ae9848c62b4778d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 107000: Generator loss: 0.6931241565942763, discriminator loss: 0.6931486827135088\n",
      "Step 107500: Generator loss: 0.6930514671802529, discriminator loss: 0.6932014391422279\n",
      "Step 108000: Generator loss: 0.6930765466690066, discriminator loss: 0.6931855576038353\n",
      "Step 108500: Generator loss: 0.6931275407075884, discriminator loss: 0.6930574538707736\n",
      "Step 109000: Generator loss: 0.6930305230617522, discriminator loss: 0.6931218513250348\n",
      "Step 109500: Generator loss: 0.6933168945312493, discriminator loss: 0.6931617326736454\n",
      "Step 110000: Generator loss: 0.6930373722314835, discriminator loss: 0.6931750806570056\n",
      "Step 110500: Generator loss: 0.6931821098327634, discriminator loss: 0.693150516152382\n",
      "Step 111000: Generator loss: 0.6931912842988972, discriminator loss: 0.6931275262832642\n",
      "Step 111500: Generator loss: 0.6932091323137277, discriminator loss: 0.6931425899267191\n",
      "Step 112000: Generator loss: 0.6933558392524715, discriminator loss: 0.6931285516023642\n",
      "Step 112500: Generator loss: 0.6929435589313506, discriminator loss: 0.693138091325761\n",
      "Step 113000: Generator loss: 0.6931457648277282, discriminator loss: 0.693156713247299\n",
      "Step 113500: Generator loss: 0.6930588698387149, discriminator loss: 0.6931512804031367\n",
      "Step 114000: Generator loss: 0.6932293273210524, discriminator loss: 0.6931280126571661\n",
      "Step 114500: Generator loss: 0.6932504435777662, discriminator loss: 0.6931746354103087\n",
      "Step 115000: Generator loss: 0.6927625882625584, discriminator loss: 0.6931215920448296\n",
      "Step 115500: Generator loss: 0.6933412209749228, discriminator loss: 0.6931421893835071\n",
      "Step 116000: Generator loss: 0.6931562926769254, discriminator loss: 0.6931794801950456\n",
      "Step 116500: Generator loss: 0.6934022367000579, discriminator loss: 0.6930726714134213\n",
      "Step 117000: Generator loss: 0.6932285714149476, discriminator loss: 0.6931125975847253\n",
      "Step 117500: Generator loss: 0.6930981768369673, discriminator loss: 0.6932542011737822\n",
      "Step 118000: Generator loss: 0.6931015837192533, discriminator loss: 0.6931381851434704\n",
      "Step 118500: Generator loss: 0.6930460635423652, discriminator loss: 0.6931097477674482\n",
      "Step 119000: Generator loss: 0.6931442699432376, discriminator loss: 0.6931394742727283\n",
      "Step 119500: Generator loss: 0.6931034041643138, discriminator loss: 0.6930983774662022\n",
      "Step 120000: Generator loss: 0.6931447560787197, discriminator loss: 0.693139009237289\n",
      "Step 120500: Generator loss: 0.6932769200801845, discriminator loss: 0.6931758234500887\n",
      "Step 121000: Generator loss: 0.6930563503503795, discriminator loss: 0.6931954013109209\n",
      "Step 121500: Generator loss: 0.6932786910533904, discriminator loss: 0.6931466029882433\n",
      "Step 122000: Generator loss: 0.6932583587169645, discriminator loss: 0.6931021419763569\n",
      "Step 122500: Generator loss: 0.6931995565891267, discriminator loss: 0.6930617547035215\n",
      "Step 123000: Generator loss: 0.6930906890630716, discriminator loss: 0.6932161980867392\n",
      "Step 123500: Generator loss: 0.6930748319625851, discriminator loss: 0.6931842489242548\n",
      "Step 124000: Generator loss: 0.6929964362382892, discriminator loss: 0.6930863500833511\n",
      "Step 124500: Generator loss: 0.6932698596715928, discriminator loss: 0.6931359928846359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8b4e2d0c0340f4969251c024d01c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 125000: Generator loss: 0.6932632522583002, discriminator loss: 0.6931748557090761\n",
      "Step 125500: Generator loss: 0.6928656479120252, discriminator loss: 0.6931262168884282\n",
      "Step 126000: Generator loss: 0.6935334653854364, discriminator loss: 0.6931577869653701\n",
      "Step 126500: Generator loss: 0.692934020519256, discriminator loss: 0.6931302894353871\n",
      "Step 127000: Generator loss: 0.6932428342103947, discriminator loss: 0.6931304525136945\n",
      "Step 127500: Generator loss: 0.6935575381517414, discriminator loss: 0.6931603447198872\n",
      "Step 128000: Generator loss: 0.6934125179052353, discriminator loss: 0.6930666397809973\n",
      "Step 128500: Generator loss: 0.6927049038410188, discriminator loss: 0.6932200343608851\n",
      "Step 129000: Generator loss: 0.692866448402405, discriminator loss: 0.6932265551090244\n",
      "Step 129500: Generator loss: 0.6930309489965432, discriminator loss: 0.6930230928659438\n",
      "Step 130000: Generator loss: 0.6931900807619104, discriminator loss: 0.6931209154129035\n",
      "Step 130500: Generator loss: 0.6931344436407084, discriminator loss: 0.693170321941376\n",
      "Step 131000: Generator loss: 0.693666654825211, discriminator loss: 0.6931468602418905\n",
      "Step 131500: Generator loss: 0.6929720693826681, discriminator loss: 0.6932409943342212\n",
      "Step 132000: Generator loss: 0.6932715378999708, discriminator loss: 0.6932286794185634\n",
      "Step 132500: Generator loss: 0.693144225239754, discriminator loss: 0.6929909057617186\n",
      "Step 133000: Generator loss: 0.6933514919281007, discriminator loss: 0.6930622560977938\n",
      "Step 133500: Generator loss: 0.6931806833744049, discriminator loss: 0.6931854904890067\n",
      "Step 134000: Generator loss: 0.6929637731313706, discriminator loss: 0.6932644042968751\n",
      "Step 134500: Generator loss: 0.6931294049024588, discriminator loss: 0.6931131110191343\n",
      "Step 135000: Generator loss: 0.6931065332889559, discriminator loss: 0.6931874191761023\n",
      "Step 135500: Generator loss: 0.693074523687362, discriminator loss: 0.6930657222270967\n",
      "Step 136000: Generator loss: 0.6931565530300138, discriminator loss: 0.693134266018867\n",
      "Step 136500: Generator loss: 0.6930329965353023, discriminator loss: 0.693242100358009\n",
      "Step 137000: Generator loss: 0.6933849222660062, discriminator loss: 0.693170773267746\n",
      "Step 137500: Generator loss: 0.6931954686641699, discriminator loss: 0.6930463656187057\n",
      "Step 138000: Generator loss: 0.693108864665032, discriminator loss: 0.6931107592582699\n",
      "Step 138500: Generator loss: 0.6932625508308411, discriminator loss: 0.6932507981061931\n",
      "Step 139000: Generator loss: 0.6932035049200049, discriminator loss: 0.6931482691764829\n",
      "Step 139500: Generator loss: 0.6929976264238367, discriminator loss: 0.693147061109543\n",
      "Step 140000: Generator loss: 0.6931340720653538, discriminator loss: 0.6931615041494364\n",
      "Step 140500: Generator loss: 0.6930296123027802, discriminator loss: 0.6930689409971234\n",
      "Step 141000: Generator loss: 0.693212577104569, discriminator loss: 0.6930867762565617\n",
      "Step 141500: Generator loss: 0.6930638051033021, discriminator loss: 0.6932198263406757\n",
      "Step 142000: Generator loss: 0.6931476593017585, discriminator loss: 0.6931674932241444\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457bfb877a884799a65580ed63650961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 142500: Generator loss: 0.6933146018981937, discriminator loss: 0.6931725144386293\n",
      "Step 143000: Generator loss: 0.6933557585477832, discriminator loss: 0.6931802872419367\n",
      "Step 143500: Generator loss: 0.6932153031826016, discriminator loss: 0.6931383942365643\n",
      "Step 144000: Generator loss: 0.6929997502565379, discriminator loss: 0.6930513316392902\n",
      "Step 144500: Generator loss: 0.6930408828258506, discriminator loss: 0.6931836196184159\n",
      "Step 145000: Generator loss: 0.6931558328866958, discriminator loss: 0.6931562303304676\n",
      "Step 145500: Generator loss: 0.6930744310617453, discriminator loss: 0.693116318583489\n",
      "Step 146000: Generator loss: 0.6930607420206072, discriminator loss: 0.6931698403358454\n",
      "Step 146500: Generator loss: 0.6933952302932731, discriminator loss: 0.6931635723114016\n",
      "Step 147000: Generator loss: 0.6930134036540989, discriminator loss: 0.693105811595917\n",
      "Step 147500: Generator loss: 0.6929807766675944, discriminator loss: 0.693130001783371\n",
      "Step 148000: Generator loss: 0.6932540885210039, discriminator loss: 0.6932027502059935\n",
      "Step 148500: Generator loss: 0.6932488669157034, discriminator loss: 0.6931781421899789\n",
      "Step 149000: Generator loss: 0.6930555932521824, discriminator loss: 0.6930591386556633\n",
      "Step 149500: Generator loss: 0.6936162377595896, discriminator loss: 0.6931271213293078\n",
      "Step 150000: Generator loss: 0.6931015269756318, discriminator loss: 0.6932565081119536\n",
      "Step 150500: Generator loss: 0.6927377494573593, discriminator loss: 0.6932083635330198\n",
      "Step 151000: Generator loss: 0.6932719938755032, discriminator loss: 0.6930313082933425\n",
      "Step 151500: Generator loss: 0.6933093937635422, discriminator loss: 0.6931452875137333\n",
      "Step 152000: Generator loss: 0.6935125458240513, discriminator loss: 0.6932034120559698\n",
      "Step 152500: Generator loss: 0.6924468010663992, discriminator loss: 0.6932623771429063\n",
      "Step 153000: Generator loss: 0.6926109021902087, discriminator loss: 0.6930199614763255\n",
      "Step 153500: Generator loss: 0.6936836580038076, discriminator loss: 0.6931919184923175\n",
      "Step 154000: Generator loss: 0.6939956529140477, discriminator loss: 0.6931033565998075\n",
      "Step 154500: Generator loss: 0.6925941027402874, discriminator loss: 0.6931324518918983\n",
      "Step 155000: Generator loss: 0.6920551239252085, discriminator loss: 0.6930557599067682\n",
      "Step 155500: Generator loss: 0.693044789552688, discriminator loss: 0.6932142684459681\n",
      "Step 156000: Generator loss: 0.6943134411573403, discriminator loss: 0.6933273439407345\n",
      "Step 156500: Generator loss: 0.6941367977857591, discriminator loss: 0.6930743523836135\n",
      "Step 157000: Generator loss: 0.6930182837247848, discriminator loss: 0.6930748032331459\n",
      "Step 157500: Generator loss: 0.6926841403245926, discriminator loss: 0.6931469720602037\n",
      "Step 158000: Generator loss: 0.6931300020217893, discriminator loss: 0.6931771215200434\n",
      "Step 158500: Generator loss: 0.6932668145895001, discriminator loss: 0.6931559631824499\n",
      "Step 159000: Generator loss: 0.6928282686471945, discriminator loss: 0.693141404867173\n",
      "Step 159500: Generator loss: 0.6939606577157973, discriminator loss: 0.6931211152076723\n",
      "Step 160000: Generator loss: 0.6933375090360646, discriminator loss: 0.6931717487573618\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c45da21f98c4cafb9a307511c984640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 160500: Generator loss: 0.6925645201206216, discriminator loss: 0.6932294864654537\n",
      "Step 161000: Generator loss: 0.6927841820716862, discriminator loss: 0.6930708869695665\n",
      "Step 161500: Generator loss: 0.6930803084373472, discriminator loss: 0.6929909937381743\n",
      "Step 162000: Generator loss: 0.692919366836548, discriminator loss: 0.6932393482923499\n",
      "Step 162500: Generator loss: 0.693483819365501, discriminator loss: 0.6932316728830341\n",
      "Step 163000: Generator loss: 0.6936026912927629, discriminator loss: 0.6931049360036854\n",
      "Step 163500: Generator loss: 0.692899534106255, discriminator loss: 0.6931036133766175\n",
      "Step 164000: Generator loss: 0.6932748746871946, discriminator loss: 0.6932038239240645\n",
      "Step 164500: Generator loss: 0.6931298092603695, discriminator loss: 0.6932085360288617\n",
      "Step 165000: Generator loss: 0.6934347605705264, discriminator loss: 0.6930510747432705\n",
      "Step 165500: Generator loss: 0.6929430503845218, discriminator loss: 0.693115846157074\n",
      "Step 166000: Generator loss: 0.6932386847734451, discriminator loss: 0.6932127047777176\n",
      "Step 166500: Generator loss: 0.6928596024513249, discriminator loss: 0.6931778633594511\n",
      "Step 167000: Generator loss: 0.6930376135110851, discriminator loss: 0.6931313726902013\n",
      "Step 167500: Generator loss: 0.6937400447130201, discriminator loss: 0.6930833019018177\n",
      "Step 168000: Generator loss: 0.6929720132350918, discriminator loss: 0.6931194205284132\n",
      "Step 168500: Generator loss: 0.6927967386245728, discriminator loss: 0.6931936159133913\n",
      "Step 169000: Generator loss: 0.692941661477089, discriminator loss: 0.6931652309894559\n",
      "Step 169500: Generator loss: 0.6933914957046512, discriminator loss: 0.6931527411937719\n",
      "Step 170000: Generator loss: 0.693397294163704, discriminator loss: 0.6931694244146349\n",
      "Step 170500: Generator loss: 0.6931347665786741, discriminator loss: 0.6931332758665089\n",
      "Step 171000: Generator loss: 0.6932197567224504, discriminator loss: 0.693086175560951\n",
      "Step 171500: Generator loss: 0.6930468643903737, discriminator loss: 0.6931160559654242\n",
      "Step 172000: Generator loss: 0.6931792631149295, discriminator loss: 0.693146468877793\n",
      "Step 172500: Generator loss: 0.6928471523523339, discriminator loss: 0.6931740142107011\n",
      "Step 173000: Generator loss: 0.6935279899835581, discriminator loss: 0.6931456980705263\n",
      "Step 173500: Generator loss: 0.6929666745662691, discriminator loss: 0.6931854915618898\n",
      "Step 174000: Generator loss: 0.6931164833307271, discriminator loss: 0.693138156175613\n",
      "Step 174500: Generator loss: 0.693248315215112, discriminator loss: 0.6930945618152615\n",
      "Step 175000: Generator loss: 0.6931145224571231, discriminator loss: 0.6931852436065674\n",
      "Step 175500: Generator loss: 0.6932060046195988, discriminator loss: 0.6931375956535344\n",
      "Step 176000: Generator loss: 0.6932022652626038, discriminator loss: 0.6931090269088741\n",
      "Step 176500: Generator loss: 0.6932356690168372, discriminator loss: 0.6931353613138201\n",
      "Step 177000: Generator loss: 0.6928834005594258, discriminator loss: 0.6931753953695303\n",
      "Step 177500: Generator loss: 0.6932430200576788, discriminator loss: 0.6931877354383468\n",
      "Step 178000: Generator loss: 0.693067003846169, discriminator loss: 0.6931209853887567\n"
     ]
    }
   ],
   "source": [
    "cur_step = 0\n",
    "mean_generator_loss = 0\n",
    "mean_discriminator_loss = 0\n",
    "test_generator = True # Whether the generator should be tested\n",
    "gen_loss = False\n",
    "error = False\n",
    "for epoch in range(n_epochs):\n",
    "  \n",
    "    # Dataloader returns the batches\n",
    "    for real, _ in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "\n",
    "        # Flatten the batch of real images from the dataset\n",
    "        real = real.view(cur_batch_size, -1).to(device)\n",
    "\n",
    "        ### Update discriminator ###\n",
    "        # Zero out the gradients before backpropagation\n",
    "        disc_opt.zero_grad()\n",
    "\n",
    "        # Calculate discriminator loss\n",
    "        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)\n",
    "\n",
    "        # Update gradients\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "\n",
    "        # Update optimizer\n",
    "        disc_opt.step()\n",
    "\n",
    "        # For testing purposes, to keep track of the generator weights\n",
    "        if test_generator:\n",
    "            old_generator_weights = gen.gen[0][0].weight.detach().clone()\n",
    "\n",
    "        ### Update generator ###\n",
    "        #     Hint: This code will look a lot like the discriminator updates!\n",
    "        #     These are the steps you will need to complete:\n",
    "        #       1) Zero out the gradients.\n",
    "        #       2) Calculate the generator loss, assigning it to gen_loss.\n",
    "        #       3) Backprop through the generator: update the gradients and optimizer.\n",
    "        #### START CODE HERE ####\n",
    "        gen_opt.zero_grad()\n",
    "        gen_loss = get_gen_loss(gen, disc, criterion, cur_batch_size, z_dim, device)\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "        #### END CODE HERE ####\n",
    "\n",
    "        # For testing purposes, to check that your code changes the generator weights\n",
    "        if test_generator:\n",
    "            try:\n",
    "                assert lr > 0.0000002 or (gen.gen[0][0].weight.grad.abs().max() < 0.0005 and epoch == 0)\n",
    "                assert torch.any(gen.gen[0][0].weight.detach().clone() != old_generator_weights)\n",
    "            except:\n",
    "                error = True\n",
    "                print(\"Runtime tests have failed\")\n",
    "\n",
    "        # Keep track of the average discriminator loss\n",
    "        mean_discriminator_loss += disc_loss.item() / display_step\n",
    "\n",
    "        # Keep track of the average generator loss\n",
    "        mean_generator_loss += gen_loss.item() / display_step\n",
    "\n",
    "        ### Visualization code ###\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            print(f\"Step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}\")\n",
    "            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "            fake = gen(fake_noise)\n",
    "            mean_generator_loss = 0\n",
    "            mean_discriminator_loss = 0\n",
    "        cur_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gen, 'genmodel_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(disc, 'discmodel_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_noise = get_noise(cur_batch_size, z_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = gen(fake_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data=fake.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.99537109e+04, -2.38821935e-03,  4.67908096e+00,\n",
       "         2.52836204e+00, -7.13576198e-01,  1.90590787e+00,\n",
       "        -1.47535744e+01,  6.51574707e+00, -7.68414259e+00,\n",
       "        -4.05615282e+00,  1.49024025e-01, -1.94922876e+00,\n",
       "         2.86578608e+00,  4.66719389e-01,  4.13640118e+00,\n",
       "        -2.21028313e-01, -1.57623339e+00, -4.97476149e+00,\n",
       "        -1.78648576e-01, -1.04865098e+00,  1.62485657e+01,\n",
       "         5.09280825e+00,  4.77982235e+00,  4.82304192e+00,\n",
       "        -3.93244445e-01,  1.00302577e+00, -6.45077288e-01,\n",
       "         1.57242596e+00,  9.32176781e+00,  3.92154427e+01],\n",
       "       [ 1.39731047e+05, -1.29017389e+00,  1.84246123e+00,\n",
       "         2.72249132e-01, -4.37335491e-01,  7.10338306e+00,\n",
       "        -6.18834352e+00,  3.88558807e+01, -2.73385835e+00,\n",
       "        -2.17951107e+00,  4.18208694e+00,  1.91975963e+00,\n",
       "         1.40272617e+00, -1.25648582e+00,  2.37768435e+00,\n",
       "         1.86720550e-01, -3.49970460e-01, -3.92531347e+00,\n",
       "         4.93980587e-01, -1.19621384e+00,  2.17213764e+01,\n",
       "        -1.76286364e+00,  4.26621389e+00,  9.81792068e+00,\n",
       "         1.73626077e+00, -3.39315391e+00, -3.56714249e-01,\n",
       "        -1.15795956e+01, -6.00427103e+00,  2.45314960e+01],\n",
       "       [ 5.17559297e+04, -2.57539129e+00,  6.53740120e+00,\n",
       "        -5.79039574e+00,  2.03447914e+00,  3.66676784e+00,\n",
       "         7.44512320e+00, -7.50079203e+00, -4.09963751e+00,\n",
       "        -9.23919380e-01,  1.74877644e+00,  2.94924319e-01,\n",
       "         2.54690838e+00,  1.03812850e+00,  4.76668358e+00,\n",
       "         1.39943147e+00, -1.23813736e+00,  1.60396516e+00,\n",
       "         1.91864520e-01,  7.54675388e-01, -1.06372118e+00,\n",
       "        -5.12010717e+00,  3.52260470e+00, -9.70961571e+00,\n",
       "         7.50346184e-01,  2.10823923e-01, -3.55733037e-01,\n",
       "        -2.08656645e+00,  8.82201862e+00,  2.44702515e+02],\n",
       "       [ 4.14094023e+04, -4.11441296e-01,  1.39465885e+01,\n",
       "         2.87408018e+00, -1.49128306e+00,  1.53599405e+01,\n",
       "         2.09554958e+01, -6.87348700e+00, -1.03337803e+01,\n",
       "        -3.96687293e+00,  4.80335760e+00, -1.51403308e-01,\n",
       "         1.49548158e-01,  8.50164652e-01,  3.80158472e+00,\n",
       "        -7.14052975e-01,  1.26616788e+00,  8.67442191e-01,\n",
       "         5.79603791e-01,  3.18057567e-01,  1.23813713e+00,\n",
       "        -5.94242620e+00,  4.38069725e+00, -9.59596455e-01,\n",
       "         2.43526235e-01,  2.62121129e+00, -2.03603685e-01,\n",
       "        -1.74210680e+00,  6.88166809e+00,  7.37700577e+01],\n",
       "       [ 1.15701867e+05,  5.02003096e-02,  9.25412846e+00,\n",
       "         2.25190163e+00,  6.91934633e+00, -1.18914557e+01,\n",
       "         2.59234943e+01, -3.44536781e+01,  1.26052122e+01,\n",
       "        -3.69005013e+00, -1.06419287e+01,  3.09899974e+00,\n",
       "        -8.63128066e-01, -3.01313543e+00, -6.36391401e+00,\n",
       "         6.13780737e-01,  2.07021785e+00,  3.19257689e+00,\n",
       "         6.76310480e-01,  2.25746465e+00, -2.71285725e+01,\n",
       "         5.24041843e+00, -2.56130409e+00,  1.01571817e+01,\n",
       "        -3.31349850e-01,  1.11561644e+00,  5.21196067e-01,\n",
       "        -2.35947537e+00, -9.06595516e+00,  7.37286758e+01],\n",
       "       [ 9.58628438e+04,  3.60995024e-01, -4.30304146e+00,\n",
       "         3.43423796e+00, -7.91555941e-02,  4.22453260e+00,\n",
       "        -1.68986073e+01, -2.66098833e+00, -5.52052689e+00,\n",
       "        -4.01935625e+00,  5.72830343e+00,  1.02470529e+00,\n",
       "        -1.18492043e+00, -6.53830588e-01, -3.66361666e+00,\n",
       "        -5.75292647e-01,  1.43221104e+00, -9.22997189e+00,\n",
       "        -8.59076381e-02, -1.81381404e+00,  2.08048973e+01,\n",
       "        -4.11810827e+00,  1.40120625e+00,  1.05804672e+01,\n",
       "         4.33649361e-01, -7.94241786e-01, -5.56290329e-01,\n",
       "        -1.55982196e+00, -7.08751011e+00,  5.53627853e+01],\n",
       "       [ 1.12003039e+05, -2.05076551e+00,  2.61231542e+00,\n",
       "        -4.09067690e-01,  3.20998979e+00, -2.87506714e+01,\n",
       "        -5.70160103e+00,  9.94325876e-01, -1.07599697e+01,\n",
       "        -5.67791033e+00, -6.81694651e+00,  3.48787260e+00,\n",
       "         1.76483405e+00, -1.87283897e+00, -3.47561216e+00,\n",
       "         2.92289329e+00, -8.66424441e-01,  2.01128149e+00,\n",
       "         1.29455134e-01,  7.87756920e-01,  1.90894377e+00,\n",
       "         8.16883945e+00, -1.32347488e+00,  1.38829648e+00,\n",
       "         3.32246572e-01,  2.98286855e-01,  2.12640122e-01,\n",
       "         1.04111540e+00, -4.62391376e+00,  1.76448181e+02]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmaxscaler.inverse_transform(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114738.757812</td>\n",
       "      <td>-0.495996</td>\n",
       "      <td>0.325738</td>\n",
       "      <td>0.071241</td>\n",
       "      <td>4.110361</td>\n",
       "      <td>-10.919909</td>\n",
       "      <td>10.442595</td>\n",
       "      <td>-18.487663</td>\n",
       "      <td>9.639565</td>\n",
       "      <td>-4.181394</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.237973</td>\n",
       "      <td>5.893458</td>\n",
       "      <td>-1.324298</td>\n",
       "      <td>3.071975</td>\n",
       "      <td>0.169218</td>\n",
       "      <td>-0.516669</td>\n",
       "      <td>-0.138654</td>\n",
       "      <td>0.024732</td>\n",
       "      <td>-8.608523</td>\n",
       "      <td>73.486557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96399.515625</td>\n",
       "      <td>-3.911854</td>\n",
       "      <td>8.167215</td>\n",
       "      <td>-1.107892</td>\n",
       "      <td>8.016469</td>\n",
       "      <td>-19.422447</td>\n",
       "      <td>17.240328</td>\n",
       "      <td>-16.974100</td>\n",
       "      <td>5.418760</td>\n",
       "      <td>-3.025454</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.275404</td>\n",
       "      <td>0.561579</td>\n",
       "      <td>-0.915233</td>\n",
       "      <td>7.910861</td>\n",
       "      <td>-0.035102</td>\n",
       "      <td>1.339738</td>\n",
       "      <td>0.594294</td>\n",
       "      <td>-1.988063</td>\n",
       "      <td>-2.897775</td>\n",
       "      <td>202.804993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56187.730469</td>\n",
       "      <td>-1.431320</td>\n",
       "      <td>10.096639</td>\n",
       "      <td>-0.868973</td>\n",
       "      <td>-1.922716</td>\n",
       "      <td>11.985427</td>\n",
       "      <td>6.276679</td>\n",
       "      <td>-0.667818</td>\n",
       "      <td>-13.125577</td>\n",
       "      <td>-0.846630</td>\n",
       "      <td>...</td>\n",
       "      <td>12.990332</td>\n",
       "      <td>-4.363430</td>\n",
       "      <td>5.303897</td>\n",
       "      <td>-3.397654</td>\n",
       "      <td>1.447364</td>\n",
       "      <td>0.583149</td>\n",
       "      <td>-0.913898</td>\n",
       "      <td>-7.136986</td>\n",
       "      <td>11.239240</td>\n",
       "      <td>72.408485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144120.046875</td>\n",
       "      <td>-0.813232</td>\n",
       "      <td>0.745212</td>\n",
       "      <td>2.014179</td>\n",
       "      <td>-0.168681</td>\n",
       "      <td>2.168889</td>\n",
       "      <td>-13.471707</td>\n",
       "      <td>26.283175</td>\n",
       "      <td>-6.772091</td>\n",
       "      <td>-6.392673</td>\n",
       "      <td>...</td>\n",
       "      <td>20.970816</td>\n",
       "      <td>2.439697</td>\n",
       "      <td>4.788126</td>\n",
       "      <td>12.338380</td>\n",
       "      <td>0.484754</td>\n",
       "      <td>-0.974162</td>\n",
       "      <td>-0.796711</td>\n",
       "      <td>-11.433218</td>\n",
       "      <td>-3.341536</td>\n",
       "      <td>20.354683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84552.312500</td>\n",
       "      <td>-1.428741</td>\n",
       "      <td>9.166338</td>\n",
       "      <td>2.127709</td>\n",
       "      <td>1.902657</td>\n",
       "      <td>-3.239834</td>\n",
       "      <td>12.916766</td>\n",
       "      <td>-8.606132</td>\n",
       "      <td>-5.255139</td>\n",
       "      <td>-3.059400</td>\n",
       "      <td>...</td>\n",
       "      <td>2.937529</td>\n",
       "      <td>2.963536</td>\n",
       "      <td>0.053884</td>\n",
       "      <td>4.811018</td>\n",
       "      <td>0.603229</td>\n",
       "      <td>1.124938</td>\n",
       "      <td>0.549955</td>\n",
       "      <td>1.735524</td>\n",
       "      <td>-1.720056</td>\n",
       "      <td>162.852325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>70316.609375</td>\n",
       "      <td>1.061002</td>\n",
       "      <td>3.285070</td>\n",
       "      <td>3.510750</td>\n",
       "      <td>-0.709053</td>\n",
       "      <td>8.961229</td>\n",
       "      <td>-12.310706</td>\n",
       "      <td>-9.315220</td>\n",
       "      <td>-4.258889</td>\n",
       "      <td>-2.695735</td>\n",
       "      <td>...</td>\n",
       "      <td>22.058533</td>\n",
       "      <td>-5.206933</td>\n",
       "      <td>2.004431</td>\n",
       "      <td>5.750990</td>\n",
       "      <td>0.607375</td>\n",
       "      <td>-1.756846</td>\n",
       "      <td>-0.390757</td>\n",
       "      <td>2.820733</td>\n",
       "      <td>-7.632590</td>\n",
       "      <td>49.209957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>62327.832031</td>\n",
       "      <td>-0.628858</td>\n",
       "      <td>3.776641</td>\n",
       "      <td>0.031833</td>\n",
       "      <td>0.609544</td>\n",
       "      <td>-0.229271</td>\n",
       "      <td>-13.618052</td>\n",
       "      <td>6.784653</td>\n",
       "      <td>-6.628531</td>\n",
       "      <td>-3.567304</td>\n",
       "      <td>...</td>\n",
       "      <td>7.888371</td>\n",
       "      <td>0.930790</td>\n",
       "      <td>4.450389</td>\n",
       "      <td>-5.400101</td>\n",
       "      <td>-0.097051</td>\n",
       "      <td>1.289788</td>\n",
       "      <td>-0.208453</td>\n",
       "      <td>1.942836</td>\n",
       "      <td>9.643490</td>\n",
       "      <td>62.704918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1          2         3         4          5   \\\n",
       "0  114738.757812 -0.495996   0.325738  0.071241  4.110361 -10.919909   \n",
       "1   96399.515625 -3.911854   8.167215 -1.107892  8.016469 -19.422447   \n",
       "2   56187.730469 -1.431320  10.096639 -0.868973 -1.922716  11.985427   \n",
       "3  144120.046875 -0.813232   0.745212  2.014179 -0.168681   2.168889   \n",
       "4   84552.312500 -1.428741   9.166338  2.127709  1.902657  -3.239834   \n",
       "5   70316.609375  1.061002   3.285070  3.510750 -0.709053   8.961229   \n",
       "6   62327.832031 -0.628858   3.776641  0.031833  0.609544  -0.229271   \n",
       "\n",
       "          6          7          8         9   ...         20        21  \\\n",
       "0  10.442595 -18.487663   9.639565 -4.181394  ... -15.237973  5.893458   \n",
       "1  17.240328 -16.974100   5.418760 -3.025454  ... -19.275404  0.561579   \n",
       "2   6.276679  -0.667818 -13.125577 -0.846630  ...  12.990332 -4.363430   \n",
       "3 -13.471707  26.283175  -6.772091 -6.392673  ...  20.970816  2.439697   \n",
       "4  12.916766  -8.606132  -5.255139 -3.059400  ...   2.937529  2.963536   \n",
       "5 -12.310706  -9.315220  -4.258889 -2.695735  ...  22.058533 -5.206933   \n",
       "6 -13.618052   6.784653  -6.628531 -3.567304  ...   7.888371  0.930790   \n",
       "\n",
       "         22         23        24        25        26         27         28  \\\n",
       "0 -1.324298   3.071975  0.169218 -0.516669 -0.138654   0.024732  -8.608523   \n",
       "1 -0.915233   7.910861 -0.035102  1.339738  0.594294  -1.988063  -2.897775   \n",
       "2  5.303897  -3.397654  1.447364  0.583149 -0.913898  -7.136986  11.239240   \n",
       "3  4.788126  12.338380  0.484754 -0.974162 -0.796711 -11.433218  -3.341536   \n",
       "4  0.053884   4.811018  0.603229  1.124938  0.549955   1.735524  -1.720056   \n",
       "5  2.004431   5.750990  0.607375 -1.756846 -0.390757   2.820733  -7.632590   \n",
       "6  4.450389  -5.400101 -0.097051  1.289788 -0.208453   1.942836   9.643490   \n",
       "\n",
       "           29  \n",
       "0   73.486557  \n",
       "1  202.804993  \n",
       "2   72.408485  \n",
       "3   20.354683  \n",
       "4  162.852325  \n",
       "5   49.209957  \n",
       "6   62.704918  \n",
       "\n",
       "[7 rows x 30 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_new_data(n_times=5):\n",
    "    data=[]\n",
    "    for _ in range(n_times):\n",
    "        fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "        fake = gen(fake_noise)\n",
    "        new_data=fake.detach().numpy()\n",
    "        data.append(minmaxscaler.inverse_transform(new_data))\n",
    "    return pd.DataFrame(np.vstack(data))\n",
    "\n",
    "create_new_data(n_times=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.168375e-15</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>-1.379537e-15</td>\n",
       "      <td>2.074095e-15</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.487313e-15</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.213481e-16</td>\n",
       "      <td>-2.406331e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.654067e-16</td>\n",
       "      <td>-3.568593e-16</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>4.473266e-15</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>1.683437e-15</td>\n",
       "      <td>-3.660091e-16</td>\n",
       "      <td>-1.227390e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.654067e-16 -3.568593e-16  2.578648e-16  4.473266e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('../credit_card_kaggle/creditcard.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>90777.179688</td>\n",
       "      <td>-0.955830</td>\n",
       "      <td>4.904205</td>\n",
       "      <td>1.029115</td>\n",
       "      <td>1.495160</td>\n",
       "      <td>-2.132203</td>\n",
       "      <td>1.322069</td>\n",
       "      <td>-2.726856</td>\n",
       "      <td>-3.246976</td>\n",
       "      <td>-3.432107</td>\n",
       "      <td>...</td>\n",
       "      <td>5.445347</td>\n",
       "      <td>0.272328</td>\n",
       "      <td>2.135392</td>\n",
       "      <td>3.774948</td>\n",
       "      <td>0.457344</td>\n",
       "      <td>0.175984</td>\n",
       "      <td>-0.196586</td>\n",
       "      <td>-2.359477</td>\n",
       "      <td>-0.386688</td>\n",
       "      <td>85.639893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28118.839844</td>\n",
       "      <td>1.132654</td>\n",
       "      <td>5.102355</td>\n",
       "      <td>1.974553</td>\n",
       "      <td>2.787741</td>\n",
       "      <td>12.167484</td>\n",
       "      <td>14.033566</td>\n",
       "      <td>16.310694</td>\n",
       "      <td>6.586971</td>\n",
       "      <td>1.643862</td>\n",
       "      <td>...</td>\n",
       "      <td>14.242802</td>\n",
       "      <td>4.902923</td>\n",
       "      <td>2.351805</td>\n",
       "      <td>5.252159</td>\n",
       "      <td>0.553893</td>\n",
       "      <td>1.477281</td>\n",
       "      <td>0.483806</td>\n",
       "      <td>4.138632</td>\n",
       "      <td>6.827394</td>\n",
       "      <td>43.368999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>31586.849609</td>\n",
       "      <td>-5.815773</td>\n",
       "      <td>-10.813290</td>\n",
       "      <td>-6.231070</td>\n",
       "      <td>-2.977193</td>\n",
       "      <td>-51.078560</td>\n",
       "      <td>-21.252434</td>\n",
       "      <td>-36.173141</td>\n",
       "      <td>-20.713285</td>\n",
       "      <td>-8.755115</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.179539</td>\n",
       "      <td>-16.335352</td>\n",
       "      <td>-5.138299</td>\n",
       "      <td>-11.660846</td>\n",
       "      <td>-0.922489</td>\n",
       "      <td>-3.931865</td>\n",
       "      <td>-1.342366</td>\n",
       "      <td>-15.707978</td>\n",
       "      <td>-10.492925</td>\n",
       "      <td>12.630581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>70007.949219</td>\n",
       "      <td>-1.597516</td>\n",
       "      <td>1.113627</td>\n",
       "      <td>-0.256436</td>\n",
       "      <td>-0.601841</td>\n",
       "      <td>-7.547033</td>\n",
       "      <td>-11.007743</td>\n",
       "      <td>-12.896372</td>\n",
       "      <td>-7.672051</td>\n",
       "      <td>-4.562226</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.639644</td>\n",
       "      <td>-2.978846</td>\n",
       "      <td>0.688233</td>\n",
       "      <td>0.198825</td>\n",
       "      <td>0.055503</td>\n",
       "      <td>-0.809952</td>\n",
       "      <td>-0.548776</td>\n",
       "      <td>-4.802675</td>\n",
       "      <td>-6.044104</td>\n",
       "      <td>55.223634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>90142.933594</td>\n",
       "      <td>-0.775554</td>\n",
       "      <td>4.633291</td>\n",
       "      <td>1.109696</td>\n",
       "      <td>0.738410</td>\n",
       "      <td>0.121257</td>\n",
       "      <td>-0.531104</td>\n",
       "      <td>-3.650452</td>\n",
       "      <td>-3.928678</td>\n",
       "      <td>-3.454251</td>\n",
       "      <td>...</td>\n",
       "      <td>9.265893</td>\n",
       "      <td>0.603183</td>\n",
       "      <td>2.631333</td>\n",
       "      <td>4.959139</td>\n",
       "      <td>0.420844</td>\n",
       "      <td>0.208589</td>\n",
       "      <td>-0.294161</td>\n",
       "      <td>-1.905589</td>\n",
       "      <td>-1.962419</td>\n",
       "      <td>78.285992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>113983.943359</td>\n",
       "      <td>-0.143181</td>\n",
       "      <td>8.587939</td>\n",
       "      <td>2.431238</td>\n",
       "      <td>2.889458</td>\n",
       "      <td>6.261565</td>\n",
       "      <td>14.085660</td>\n",
       "      <td>5.121525</td>\n",
       "      <td>0.090044</td>\n",
       "      <td>-2.268887</td>\n",
       "      <td>...</td>\n",
       "      <td>16.813184</td>\n",
       "      <td>3.900380</td>\n",
       "      <td>3.938228</td>\n",
       "      <td>7.890928</td>\n",
       "      <td>0.795609</td>\n",
       "      <td>1.182075</td>\n",
       "      <td>0.082618</td>\n",
       "      <td>0.535096</td>\n",
       "      <td>4.177539</td>\n",
       "      <td>109.044353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>153830.468750</td>\n",
       "      <td>1.449201</td>\n",
       "      <td>17.591249</td>\n",
       "      <td>6.063201</td>\n",
       "      <td>10.310723</td>\n",
       "      <td>22.234766</td>\n",
       "      <td>32.328087</td>\n",
       "      <td>59.128685</td>\n",
       "      <td>14.748790</td>\n",
       "      <td>1.841690</td>\n",
       "      <td>...</td>\n",
       "      <td>28.069431</td>\n",
       "      <td>12.828651</td>\n",
       "      <td>6.740103</td>\n",
       "      <td>13.914111</td>\n",
       "      <td>2.567192</td>\n",
       "      <td>4.182652</td>\n",
       "      <td>1.293710</td>\n",
       "      <td>9.079445</td>\n",
       "      <td>18.672047</td>\n",
       "      <td>376.584747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0            1            2            3            4   \\\n",
       "count    2100.000000  2100.000000  2100.000000  2100.000000  2100.000000   \n",
       "mean    90777.179688    -0.955830     4.904205     1.029115     1.495160   \n",
       "std     28118.839844     1.132654     5.102355     1.974553     2.787741   \n",
       "min     31586.849609    -5.815773   -10.813290    -6.231070    -2.977193   \n",
       "25%     70007.949219    -1.597516     1.113627    -0.256436    -0.601841   \n",
       "50%     90142.933594    -0.775554     4.633291     1.109696     0.738410   \n",
       "75%    113983.943359    -0.143181     8.587939     2.431238     2.889458   \n",
       "max    153830.468750     1.449201    17.591249     6.063201    10.310723   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  2100.000000  2100.000000  2100.000000  2100.000000  2100.000000  ...   \n",
       "mean     -2.132203     1.322069    -2.726856    -3.246976    -3.432107  ...   \n",
       "std      12.167484    14.033566    16.310694     6.586971     1.643862  ...   \n",
       "min     -51.078560   -21.252434   -36.173141   -20.713285    -8.755115  ...   \n",
       "25%      -7.547033   -11.007743   -12.896372    -7.672051    -4.562226  ...   \n",
       "50%       0.121257    -0.531104    -3.650452    -3.928678    -3.454251  ...   \n",
       "75%       6.261565    14.085660     5.121525     0.090044    -2.268887  ...   \n",
       "max      22.234766    32.328087    59.128685    14.748790     1.841690  ...   \n",
       "\n",
       "                20           21           22           23           24  \\\n",
       "count  2100.000000  2100.000000  2100.000000  2100.000000  2100.000000   \n",
       "mean      5.445347     0.272328     2.135392     3.774948     0.457344   \n",
       "std      14.242802     4.902923     2.351805     5.252159     0.553893   \n",
       "min     -33.179539   -16.335352    -5.138299   -11.660846    -0.922489   \n",
       "25%      -2.639644    -2.978846     0.688233     0.198825     0.055503   \n",
       "50%       9.265893     0.603183     2.631333     4.959139     0.420844   \n",
       "75%      16.813184     3.900380     3.938228     7.890928     0.795609   \n",
       "max      28.069431    12.828651     6.740103    13.914111     2.567192   \n",
       "\n",
       "                25           26           27           28           29  \n",
       "count  2100.000000  2100.000000  2100.000000  2100.000000  2100.000000  \n",
       "mean      0.175984    -0.196586    -2.359477    -0.386688    85.639893  \n",
       "std       1.477281     0.483806     4.138632     6.827394    43.368999  \n",
       "min      -3.931865    -1.342366   -15.707978   -10.492925    12.630581  \n",
       "25%      -0.809952    -0.548776    -4.802675    -6.044104    55.223634  \n",
       "50%       0.208589    -0.294161    -1.905589    -1.962419    78.285992  \n",
       "75%       1.182075     0.082618     0.535096     4.177539   109.044353  \n",
       "max       4.182652     1.293710     9.079445    18.672047   376.584747  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_data = create_new_data(n_times=300)\n",
    "syn_data.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniconda",
   "language": "python",
   "name": "miniconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
