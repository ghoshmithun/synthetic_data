{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb4e746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First download punblicly available synthea dataset and uzip it\n",
    "\n",
    "!wget https://storage.googleapis.com/synthea-public/synthea_sample_data_csv_apr2020.zip\n",
    "!unzip synthea_sample_data_csv_apr2020.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec3bef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MARITAL', 'RACE', 'ETHNICITY', 'GENDER', 'BIRTHPLACE', 'CITY', 'STATE',\n",
      "       'COUNTY', 'ZIP', 'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/patients.csv')\n",
    "df.drop(['Id', 'BIRTHDATE', 'DEATHDATE', 'SSN', 'DRIVERS', 'PASSPORT', 'PREFIX',\n",
    "       'FIRST', 'ADDRESS', 'LAST', 'SUFFIX', 'MAIDEN','LAT', 'LON',], axis=1, inplace=True)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e1140",
   "metadata": {},
   "source": [
    "Next, read patients data and remove fields such as id, date, SSN, name etc. Note, that we are trying to generate synthetic data which can be used to train our deep learning models for some other tasks. For such a model, we don't require fields like id, date, SSN etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce812e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data configuration\n",
    "\n",
    "\n",
    "file_name = \"csv/patients.csv\"\n",
    "columns_to_drop = ['Id', 'BIRTHDATE', 'DEATHDATE', 'SSN', 'DRIVERS', 'PASSPORT', 'PREFIX', 'FIRST', 'ADDRESS', 'LAST', 'SUFFIX', 'MAIDEN','LAT', 'LON']\n",
    "categorical_features = ['MARITAL', 'RACE', 'ETHNICITY', 'GENDER', 'BIRTHPLACE', 'CITY', 'STATE', 'COUNTY', 'ZIP']\n",
    "continuous_features = ['HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE']\n",
    "col1, col2 = 'num_of_doors', 'price'\n",
    "col_group_by = 'body_style'\n",
    "\n",
    "# training configuration\n",
    "noise_dim = 32\n",
    "dim = 128\n",
    "batch_size = 32\n",
    "\n",
    "log_step = 100\n",
    "epochs = 5000+1\n",
    "learning_rate = 5e-4\n",
    "models_dir = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2ef295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MARITAL', 'RACE', 'ETHNICITY', 'GENDER', 'BIRTHPLACE', 'CITY', 'STATE',\n",
      "       'COUNTY', 'ZIP', 'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(file_name)\n",
    "df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9ac8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MARITAL</th>\n",
       "      <th>RACE</th>\n",
       "      <th>ETHNICITY</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>BIRTHPLACE</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>ZIP</th>\n",
       "      <th>HEALTHCARE_EXPENSES</th>\n",
       "      <th>HEALTHCARE_COVERAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>271227.08</td>\n",
       "      <td>1334.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>132</td>\n",
       "      <td>793946.01</td>\n",
       "      <td>3204.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>236</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>574111.90</td>\n",
       "      <td>2606.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>291</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>68</td>\n",
       "      <td>935630.30</td>\n",
       "      <td>8756.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>125</td>\n",
       "      <td>598763.07</td>\n",
       "      <td>3772.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MARITAL  RACE  ETHNICITY  GENDER  BIRTHPLACE  CITY  STATE  COUNTY  ZIP  \\\n",
       "0        0     4          0       1         136    42      0       6    2   \n",
       "1        0     4          1       1          61   186      0       8  132   \n",
       "2        0     4          1       1         236    42      0       6    3   \n",
       "3        0     4          1       0         291   110      0       8   68   \n",
       "4       -1     4          1       1         189    24      0      12  125   \n",
       "\n",
       "   HEALTHCARE_EXPENSES  HEALTHCARE_COVERAGE  \n",
       "0            271227.08              1334.88  \n",
       "1            793946.01              3204.49  \n",
       "2            574111.90              2606.40  \n",
       "3            935630.30              8756.19  \n",
       "4            598763.07              3772.20  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next, we will encode all categorical features to integer values. We are simply encoding the features to numerical values and are not using one hot encoding as its not required for GANs.\n",
    "\n",
    "\n",
    "\n",
    "for column in categorical_features:\n",
    "      df[column] = df[column].astype('category').cat.codes\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf8c16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      MARITAL  RACE  ETHNICITY  GENDER  BIRTHPLACE  CITY  STATE  COUNTY  ZIP  \\\n",
      "0           0     4          0       1         136    42      0       6    2   \n",
      "1           0     4          1       1          61   186      0       8  132   \n",
      "2           0     4          1       1         236    42      0       6    3   \n",
      "3           0     4          1       0         291   110      0       8   68   \n",
      "4          -1     4          1       1         189    24      0      12  125   \n",
      "...       ...   ...        ...     ...         ...   ...    ...     ...  ...   \n",
      "1166        0     0          0       0         112    33      0       8  130   \n",
      "1167        1     4          1       1         255    21      0       4   80   \n",
      "1168        1     4          1       0          76   154      0      10   -1   \n",
      "1169        0     4          1       0         236   154      0      10   98   \n",
      "1170        0     4          1       0         289   154      0      10  102   \n",
      "\n",
      "      HEALTHCARE_COVERAGE  HEALTHCARE_EXPENSES  \n",
      "0                 1334.88                  2.0  \n",
      "1                 3204.49                  7.0  \n",
      "2                 2606.40                  5.0  \n",
      "3                 8756.19                  8.0  \n",
      "4                 3772.20                  5.0  \n",
      "...                   ...                  ...  \n",
      "1166             32086.31                 15.0  \n",
      "1167              3130.52                  9.0  \n",
      "1168             52391.24                 14.0  \n",
      "1169             13157.00                 12.0  \n",
      "1170             26565.65                 14.0  \n",
      "\n",
      "[1171 rows x 11 columns]\n",
      "      MARITAL  RACE  ETHNICITY  GENDER  BIRTHPLACE  CITY  STATE  COUNTY  ZIP  \\\n",
      "0           0     4          0       1         136    42      0       6    2   \n",
      "1           0     4          1       1          61   186      0       8  132   \n",
      "2           0     4          1       1         236    42      0       6    3   \n",
      "3           0     4          1       0         291   110      0       8   68   \n",
      "4          -1     4          1       1         189    24      0      12  125   \n",
      "...       ...   ...        ...     ...         ...   ...    ...     ...  ...   \n",
      "1166        0     0          0       0         112    33      0       8  130   \n",
      "1167        1     4          1       1         255    21      0       4   80   \n",
      "1168        1     4          1       0          76   154      0      10   -1   \n",
      "1169        0     4          1       0         236   154      0      10   98   \n",
      "1170        0     4          1       0         289   154      0      10  102   \n",
      "\n",
      "      HEALTHCARE_EXPENSES  HEALTHCARE_COVERAGE  \n",
      "0                     2.0                  0.0  \n",
      "1                     7.0                  0.0  \n",
      "2                     5.0                  0.0  \n",
      "3                     8.0                  0.0  \n",
      "4                     5.0                  0.0  \n",
      "...                   ...                  ...  \n",
      "1166                 15.0                  0.0  \n",
      "1167                  9.0                  0.0  \n",
      "1168                 14.0                  1.0  \n",
      "1169                 12.0                  0.0  \n",
      "1170                 14.0                  0.0  \n",
      "\n",
      "[1171 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "#Next, we will encode all continious features to equally sized bins. First, lets find the minimum and maximum values for HEALTHCARE_EXPENSES and HEALTHCARE_COVERAGE and then create bins based on these values.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "for column in continuous_features:\n",
    "  min = df[column].min()\n",
    "  max = df[column].max()\n",
    "  feature_bins = pd.cut(df[column], bins=np.linspace(min, max, 21), labels=False)\n",
    "  df.drop([column], axis=1, inplace=True)\n",
    "  df = pd.concat([df, feature_bins], axis=1)\n",
    "  print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bcae4a",
   "metadata": {},
   "source": [
    "Transform the data\n",
    "\n",
    "Next, we apply PowerTransformer on all the fields to get a Gaussian distribution for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7650aa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MARITAL      RACE  ETHNICITY    GENDER  BIRTHPLACE      CITY  STATE  \\\n",
      "0     0.334507  0.461541  -3.059874  1.040975    0.158023 -0.934665    0.0   \n",
      "1     0.334507  0.461541   0.326811  1.040975   -0.743838  0.991601    0.0   \n",
      "2     0.334507  0.461541   0.326811  1.040975    1.059635 -0.934665    0.0   \n",
      "3     0.334507  0.461541   0.326811 -0.960637    1.482022  0.117825    0.0   \n",
      "4    -1.275676  0.461541   0.326811  1.040975    0.662666 -1.324473    0.0   \n",
      "...        ...       ...        ...       ...         ...       ...    ...   \n",
      "1166  0.334507 -2.207146  -3.059874 -0.960637   -0.098914 -1.118375    0.0   \n",
      "1167  1.773476  0.461541   0.326811  1.040975    1.210032 -1.400048    0.0   \n",
      "1168  1.773476  0.461541   0.326811 -0.960637   -0.535455  0.646801    0.0   \n",
      "1169  0.334507  0.461541   0.326811 -0.960637    1.059635  0.646801    0.0   \n",
      "1170  0.334507  0.461541   0.326811 -0.960637    1.467316  0.646801    0.0   \n",
      "\n",
      "        COUNTY       ZIP  HEALTHCARE_EXPENSES  HEALTHCARE_COVERAGE  \n",
      "0    -0.526126 -0.334024            -0.819522            -0.187952  \n",
      "1    -0.004130  1.084866             0.259373            -0.187952  \n",
      "2    -0.526126 -0.245281            -0.111865            -0.187952  \n",
      "3    -0.004130  0.796616             0.426979            -0.187952  \n",
      "4     1.084068  1.060350            -0.111865            -0.187952  \n",
      "...        ...       ...                  ...                  ...  \n",
      "1166 -0.004130  1.077982             1.398831            -0.187952  \n",
      "1167 -1.028657  0.865198             0.585251            -0.187952  \n",
      "1168  0.533442 -1.036576             1.275817             5.320497  \n",
      "1169  0.533442  0.952695             1.016430            -0.187952  \n",
      "1170  0.533442  0.970188             1.275817            -0.187952  \n",
      "\n",
      "[1171 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mithu\\miniconda3\\envs\\dl\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3237: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "df[df.columns] = PowerTransformer(method='yeo-johnson', standardize=True, copy=True).fit_transform(df[df.columns])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6e99ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MARITAL      RACE  ETHNICITY    GENDER  BIRTHPLACE      CITY  STATE  \\\n",
      "0     0.334507  0.461541  -3.059874  1.040975    0.158023 -0.934665    0.0   \n",
      "1     0.334507  0.461541   0.326811  1.040975   -0.743838  0.991601    0.0   \n",
      "2     0.334507  0.461541   0.326811  1.040975    1.059635 -0.934665    0.0   \n",
      "3     0.334507  0.461541   0.326811 -0.960637    1.482022  0.117825    0.0   \n",
      "4    -1.275676  0.461541   0.326811  1.040975    0.662666 -1.324473    0.0   \n",
      "...        ...       ...        ...       ...         ...       ...    ...   \n",
      "1166  0.334507 -2.207146  -3.059874 -0.960637   -0.098914 -1.118375    0.0   \n",
      "1167  1.773476  0.461541   0.326811  1.040975    1.210032 -1.400048    0.0   \n",
      "1168  1.773476  0.461541   0.326811 -0.960637   -0.535455  0.646801    0.0   \n",
      "1169  0.334507  0.461541   0.326811 -0.960637    1.059635  0.646801    0.0   \n",
      "1170  0.334507  0.461541   0.326811 -0.960637    1.467316  0.646801    0.0   \n",
      "\n",
      "        COUNTY       ZIP  HEALTHCARE_EXPENSES  HEALTHCARE_COVERAGE  \n",
      "0    -0.526126 -0.334024            -0.819522            -0.187952  \n",
      "1    -0.004130  1.084866             0.259373            -0.187952  \n",
      "2    -0.526126 -0.245281            -0.111865            -0.187952  \n",
      "3    -0.004130  0.796616             0.426979            -0.187952  \n",
      "4     1.084068  1.060350            -0.111865            -0.187952  \n",
      "...        ...       ...                  ...                  ...  \n",
      "1166 -0.004130  1.077982             1.398831            -0.187952  \n",
      "1167 -1.028657  0.865198             0.585251            -0.187952  \n",
      "1168  0.533442 -1.036576             1.275817             5.320497  \n",
      "1169  0.533442  0.952695             1.016430            -0.187952  \n",
      "1170  0.533442  0.970188             1.275817            -0.187952  \n",
      "\n",
      "[1171 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mithu\\miniconda3\\envs\\dl\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3237: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "pw= PowerTransformer(method='yeo-johnson', standardize=True, copy=True)\n",
    "pwt=pw.fit_transform(df[df.columns])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b387438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns]=pwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "071945ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MARITAL</th>\n",
       "      <th>RACE</th>\n",
       "      <th>ETHNICITY</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>BIRTHPLACE</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>ZIP</th>\n",
       "      <th>HEALTHCARE_EXPENSES</th>\n",
       "      <th>HEALTHCARE_COVERAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.334507</td>\n",
       "      <td>0.461541</td>\n",
       "      <td>-3.059874</td>\n",
       "      <td>1.040975</td>\n",
       "      <td>0.158023</td>\n",
       "      <td>-0.934665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.526126</td>\n",
       "      <td>-0.334024</td>\n",
       "      <td>-0.819522</td>\n",
       "      <td>-0.187952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.334507</td>\n",
       "      <td>0.461541</td>\n",
       "      <td>0.326811</td>\n",
       "      <td>1.040975</td>\n",
       "      <td>-0.743838</td>\n",
       "      <td>0.991601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.004130</td>\n",
       "      <td>1.084866</td>\n",
       "      <td>0.259373</td>\n",
       "      <td>-0.187952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.334507</td>\n",
       "      <td>0.461541</td>\n",
       "      <td>0.326811</td>\n",
       "      <td>1.040975</td>\n",
       "      <td>1.059635</td>\n",
       "      <td>-0.934665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.526126</td>\n",
       "      <td>-0.245281</td>\n",
       "      <td>-0.111865</td>\n",
       "      <td>-0.187952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.334507</td>\n",
       "      <td>0.461541</td>\n",
       "      <td>0.326811</td>\n",
       "      <td>-0.960637</td>\n",
       "      <td>1.482022</td>\n",
       "      <td>0.117825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.004130</td>\n",
       "      <td>0.796616</td>\n",
       "      <td>0.426979</td>\n",
       "      <td>-0.187952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.275676</td>\n",
       "      <td>0.461541</td>\n",
       "      <td>0.326811</td>\n",
       "      <td>1.040975</td>\n",
       "      <td>0.662666</td>\n",
       "      <td>-1.324473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.084068</td>\n",
       "      <td>1.060350</td>\n",
       "      <td>-0.111865</td>\n",
       "      <td>-0.187952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>0.334507</td>\n",
       "      <td>-2.207146</td>\n",
       "      <td>-3.059874</td>\n",
       "      <td>-0.960637</td>\n",
       "      <td>-0.098914</td>\n",
       "      <td>-1.118375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.004130</td>\n",
       "      <td>1.077982</td>\n",
       "      <td>1.398831</td>\n",
       "      <td>-0.187952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>1.773476</td>\n",
       "      <td>0.461541</td>\n",
       "      <td>0.326811</td>\n",
       "      <td>1.040975</td>\n",
       "      <td>1.210032</td>\n",
       "      <td>-1.400048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.028657</td>\n",
       "      <td>0.865198</td>\n",
       "      <td>0.585251</td>\n",
       "      <td>-0.187952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>1.773476</td>\n",
       "      <td>0.461541</td>\n",
       "      <td>0.326811</td>\n",
       "      <td>-0.960637</td>\n",
       "      <td>-0.535455</td>\n",
       "      <td>0.646801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.533442</td>\n",
       "      <td>-1.036576</td>\n",
       "      <td>1.275817</td>\n",
       "      <td>5.320497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>0.334507</td>\n",
       "      <td>0.461541</td>\n",
       "      <td>0.326811</td>\n",
       "      <td>-0.960637</td>\n",
       "      <td>1.059635</td>\n",
       "      <td>0.646801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.533442</td>\n",
       "      <td>0.952695</td>\n",
       "      <td>1.016430</td>\n",
       "      <td>-0.187952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>0.334507</td>\n",
       "      <td>0.461541</td>\n",
       "      <td>0.326811</td>\n",
       "      <td>-0.960637</td>\n",
       "      <td>1.467316</td>\n",
       "      <td>0.646801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.533442</td>\n",
       "      <td>0.970188</td>\n",
       "      <td>1.275817</td>\n",
       "      <td>-0.187952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1171 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MARITAL      RACE  ETHNICITY    GENDER  BIRTHPLACE      CITY  STATE  \\\n",
       "0     0.334507  0.461541  -3.059874  1.040975    0.158023 -0.934665    0.0   \n",
       "1     0.334507  0.461541   0.326811  1.040975   -0.743838  0.991601    0.0   \n",
       "2     0.334507  0.461541   0.326811  1.040975    1.059635 -0.934665    0.0   \n",
       "3     0.334507  0.461541   0.326811 -0.960637    1.482022  0.117825    0.0   \n",
       "4    -1.275676  0.461541   0.326811  1.040975    0.662666 -1.324473    0.0   \n",
       "...        ...       ...        ...       ...         ...       ...    ...   \n",
       "1166  0.334507 -2.207146  -3.059874 -0.960637   -0.098914 -1.118375    0.0   \n",
       "1167  1.773476  0.461541   0.326811  1.040975    1.210032 -1.400048    0.0   \n",
       "1168  1.773476  0.461541   0.326811 -0.960637   -0.535455  0.646801    0.0   \n",
       "1169  0.334507  0.461541   0.326811 -0.960637    1.059635  0.646801    0.0   \n",
       "1170  0.334507  0.461541   0.326811 -0.960637    1.467316  0.646801    0.0   \n",
       "\n",
       "        COUNTY       ZIP  HEALTHCARE_EXPENSES  HEALTHCARE_COVERAGE  \n",
       "0    -0.526126 -0.334024            -0.819522            -0.187952  \n",
       "1    -0.004130  1.084866             0.259373            -0.187952  \n",
       "2    -0.526126 -0.245281            -0.111865            -0.187952  \n",
       "3    -0.004130  0.796616             0.426979            -0.187952  \n",
       "4     1.084068  1.060350            -0.111865            -0.187952  \n",
       "...        ...       ...                  ...                  ...  \n",
       "1166 -0.004130  1.077982             1.398831            -0.187952  \n",
       "1167 -1.028657  0.865198             0.585251            -0.187952  \n",
       "1168  0.533442 -1.036576             1.275817             5.320497  \n",
       "1169  0.533442  0.952695             1.016430            -0.187952  \n",
       "1170  0.533442  0.970188             1.275817            -0.187952  \n",
       "\n",
       "[1171 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3cbfc6",
   "metadata": {},
   "source": [
    "Next, lets define the neural network for generating synthetic data. We will be using a GAN network that comprises of an generator and discriminator that tries to beat each other and in the process learns the vector embedding for the data.\n",
    "\n",
    "The model was taken from a Github repository where it is used to generate synthetic data on credit card fraud data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cd21b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class GAN():\n",
    "    \n",
    "    def __init__(self, gan_args):\n",
    "        [self.batch_size, lr, self.noise_dim,\n",
    "         self.data_dim, layers_dim] = gan_args\n",
    "\n",
    "        self.generator = Generator(self.batch_size).\\\n",
    "            build_model(input_shape=(self.noise_dim,), dim=layers_dim, data_dim=self.data_dim)\n",
    "\n",
    "        self.discriminator = Discriminator(self.batch_size).\\\n",
    "            build_model(input_shape=(self.data_dim,), dim=layers_dim)\n",
    "\n",
    "        optimizer = Adam(lr, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.noise_dim,))\n",
    "        record = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(record)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def get_data_batch(self, train, batch_size, seed=0):\n",
    "        # # random sampling - some samples will have excessively low or high sampling, but easy to implement\n",
    "        # np.random.seed(seed)\n",
    "        # x = train.loc[ np.random.choice(train.index, batch_size) ].values\n",
    "        # iterate through shuffled indices, so every sample gets covered evenly\n",
    "\n",
    "        start_i = (batch_size * seed) % len(train)\n",
    "        stop_i = start_i + batch_size\n",
    "        shuffle_seed = (batch_size * seed) // len(train)\n",
    "        np.random.seed(shuffle_seed)\n",
    "        train_ix = np.random.choice(list(train.index), replace=False, size=len(train))  # wasteful to shuffle every time\n",
    "        train_ix = list(train_ix) + list(train_ix)  # duplicate to cover ranges past the end of the set\n",
    "        x = train.loc[train_ix[start_i: stop_i]].values\n",
    "        return np.reshape(x, (batch_size, -1))\n",
    "        \n",
    "    def train(self, data, train_arguments):\n",
    "        [cache_prefix, epochs, sample_interval] = train_arguments\n",
    "        \n",
    "        data_cols = data.columns\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((self.batch_size, 1))\n",
    "        fake = np.zeros((self.batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):    \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            batch_data = self.get_data_batch(data, self.batch_size)\n",
    "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_data = self.generator.predict(noise)\n",
    "    \n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(batch_data, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_data, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "    \n",
    "            # Plot the progress\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "    \n",
    "            # If at save interval => save generated events\n",
    "            if epoch % sample_interval == 0:\n",
    "                #Test here data generation step\n",
    "                # save model checkpoints\n",
    "                model_checkpoint_base_name = 'model/' + cache_prefix + '_{}_model_weights_step_{}.h5'\n",
    "                self.generator.save_weights(model_checkpoint_base_name.format('generator', epoch))\n",
    "                self.discriminator.save_weights(model_checkpoint_base_name.format('discriminator', epoch))\n",
    "\n",
    "                #Here is generating the data\n",
    "                z = tf.random.normal((432, self.noise_dim))\n",
    "                gen_data = self.generator(z)\n",
    "                print('generated_data')\n",
    "\n",
    "    def save(self, path, name):\n",
    "        assert os.path.isdir(path) == True, \\\n",
    "            \"Please provide a valid path. Path must be a directory.\"\n",
    "        model_path = os.path.join(path, name)\n",
    "        self.generator.save_weights(model_path)  # Load the generator\n",
    "        return\n",
    "    \n",
    "    def load(self, path):\n",
    "        assert os.path.isdir(path) == True, \\\n",
    "            \"Please provide a valid path. Path must be a directory.\"\n",
    "        self.generator = Generator(self.batch_size)\n",
    "        self.generator = self.generator.load_weights(path)\n",
    "        return self.generator\n",
    "    \n",
    "class Generator():\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "    def build_model(self, input_shape, dim, data_dim):\n",
    "        input= Input(shape=input_shape, batch_size=self.batch_size)\n",
    "        x = Dense(dim, activation='relu')(input)\n",
    "        x = Dense(dim * 2, activation='relu')(x)\n",
    "        x = Dense(dim * 4, activation='relu')(x)\n",
    "        x = Dense(data_dim)(x)\n",
    "        return Model(inputs=input, outputs=x)\n",
    "\n",
    "class Discriminator():\n",
    "    def __init__(self,batch_size):\n",
    "        self.batch_size=batch_size\n",
    "    \n",
    "    def build_model(self, input_shape, dim):\n",
    "        input = Input(shape=input_shape, batch_size=self.batch_size)\n",
    "        x = Dense(dim * 4, activation='relu')(input)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(dim * 2, activation='relu')(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(dim, activation='relu')(x)\n",
    "        x = Dense(1, activation='sigmoid')(x)\n",
    "        return Model(inputs=input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "880b7b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28f1768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "#Define the GAN and training parameters\n",
    "df[data_cols] = df[data_cols]\n",
    "\n",
    "print(df.shape[1])\n",
    "\n",
    "gan_args = [batch_size, learning_rate, noise_dim, df.shape[1], dim]\n",
    "train_args = ['', epochs, log_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2cf9ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n",
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": [
    "# !mkdir model\n",
    "# !mkdir model/gan\n",
    "# !mkdir model/gan/saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "deebad73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.676822, acc.: 45.31%] [G loss: 0.662941]\n",
      "generated_data\n",
      "1 [D loss: 0.642644, acc.: 50.00%] [G loss: 0.640900]\n",
      "2 [D loss: 0.625779, acc.: 50.00%] [G loss: 0.624852]\n",
      "3 [D loss: 0.593242, acc.: 50.00%] [G loss: 0.629532]\n",
      "4 [D loss: 0.578762, acc.: 50.00%] [G loss: 0.655295]\n",
      "5 [D loss: 0.537195, acc.: 50.00%] [G loss: 0.708575]\n",
      "6 [D loss: 0.489928, acc.: 60.94%] [G loss: 0.785676]\n",
      "7 [D loss: 0.454364, acc.: 78.12%] [G loss: 0.852673]\n",
      "8 [D loss: 0.466511, acc.: 67.19%] [G loss: 0.858180]\n",
      "9 [D loss: 0.483875, acc.: 57.81%] [G loss: 0.889254]\n",
      "10 [D loss: 0.533286, acc.: 53.12%] [G loss: 0.844190]\n",
      "11 [D loss: 0.601506, acc.: 48.44%] [G loss: 0.901749]\n",
      "12 [D loss: 0.549050, acc.: 53.12%] [G loss: 0.998198]\n",
      "13 [D loss: 0.455878, acc.: 87.50%] [G loss: 1.149815]\n",
      "14 [D loss: 0.459629, acc.: 85.94%] [G loss: 1.295619]\n",
      "15 [D loss: 0.475202, acc.: 82.81%] [G loss: 1.284856]\n",
      "16 [D loss: 0.533237, acc.: 68.75%] [G loss: 1.189641]\n",
      "17 [D loss: 0.629702, acc.: 57.81%] [G loss: 1.114784]\n",
      "18 [D loss: 0.664758, acc.: 45.31%] [G loss: 0.986707]\n",
      "19 [D loss: 0.665122, acc.: 57.81%] [G loss: 1.066667]\n",
      "20 [D loss: 0.668685, acc.: 53.12%] [G loss: 1.121121]\n",
      "21 [D loss: 0.641135, acc.: 54.69%] [G loss: 1.149354]\n",
      "22 [D loss: 0.598310, acc.: 71.88%] [G loss: 1.087966]\n",
      "23 [D loss: 0.577045, acc.: 81.25%] [G loss: 1.128638]\n",
      "24 [D loss: 0.531189, acc.: 87.50%] [G loss: 1.106168]\n",
      "25 [D loss: 0.492678, acc.: 87.50%] [G loss: 1.090337]\n",
      "26 [D loss: 0.472621, acc.: 90.62%] [G loss: 1.108765]\n",
      "27 [D loss: 0.433113, acc.: 95.31%] [G loss: 1.042287]\n",
      "28 [D loss: 0.424694, acc.: 92.19%] [G loss: 1.050064]\n",
      "29 [D loss: 0.399007, acc.: 96.88%] [G loss: 1.028836]\n",
      "30 [D loss: 0.384617, acc.: 98.44%] [G loss: 0.986857]\n",
      "31 [D loss: 0.386509, acc.: 89.06%] [G loss: 0.963694]\n",
      "32 [D loss: 0.402171, acc.: 82.81%] [G loss: 0.941035]\n",
      "33 [D loss: 0.391522, acc.: 87.50%] [G loss: 0.947977]\n",
      "34 [D loss: 0.380196, acc.: 84.38%] [G loss: 0.954886]\n",
      "35 [D loss: 0.393309, acc.: 85.94%] [G loss: 0.971836]\n",
      "36 [D loss: 0.426279, acc.: 75.00%] [G loss: 1.043190]\n",
      "37 [D loss: 0.381853, acc.: 90.62%] [G loss: 1.070103]\n",
      "38 [D loss: 0.361410, acc.: 98.44%] [G loss: 1.142746]\n",
      "39 [D loss: 0.386565, acc.: 89.06%] [G loss: 1.133963]\n",
      "40 [D loss: 0.407427, acc.: 82.81%] [G loss: 1.195483]\n",
      "41 [D loss: 0.386025, acc.: 87.50%] [G loss: 1.124124]\n",
      "42 [D loss: 0.440850, acc.: 76.56%] [G loss: 1.062502]\n",
      "43 [D loss: 0.470663, acc.: 73.44%] [G loss: 1.084060]\n",
      "44 [D loss: 0.466814, acc.: 78.12%] [G loss: 1.171304]\n",
      "45 [D loss: 0.414313, acc.: 89.06%] [G loss: 1.202227]\n",
      "46 [D loss: 0.415490, acc.: 90.62%] [G loss: 1.165305]\n",
      "47 [D loss: 0.392002, acc.: 87.50%] [G loss: 1.266481]\n",
      "48 [D loss: 0.382757, acc.: 92.19%] [G loss: 1.366287]\n",
      "49 [D loss: 0.358176, acc.: 93.75%] [G loss: 1.384809]\n",
      "50 [D loss: 0.348032, acc.: 92.19%] [G loss: 1.396713]\n",
      "51 [D loss: 0.324036, acc.: 95.31%] [G loss: 1.434154]\n",
      "52 [D loss: 0.306791, acc.: 93.75%] [G loss: 1.475528]\n",
      "53 [D loss: 0.298503, acc.: 93.75%] [G loss: 1.445394]\n",
      "54 [D loss: 0.289652, acc.: 95.31%] [G loss: 1.483224]\n",
      "55 [D loss: 0.274043, acc.: 95.31%] [G loss: 1.483447]\n",
      "56 [D loss: 0.267723, acc.: 95.31%] [G loss: 1.437592]\n",
      "57 [D loss: 0.262495, acc.: 96.88%] [G loss: 1.478938]\n",
      "58 [D loss: 0.281327, acc.: 93.75%] [G loss: 1.548850]\n",
      "59 [D loss: 0.231065, acc.: 96.88%] [G loss: 1.528577]\n",
      "60 [D loss: 0.224062, acc.: 96.88%] [G loss: 1.569003]\n",
      "61 [D loss: 0.228443, acc.: 96.88%] [G loss: 1.493695]\n",
      "62 [D loss: 0.223145, acc.: 98.44%] [G loss: 1.506164]\n",
      "63 [D loss: 0.225361, acc.: 98.44%] [G loss: 1.468241]\n",
      "64 [D loss: 0.258684, acc.: 93.75%] [G loss: 1.549464]\n",
      "65 [D loss: 0.318231, acc.: 93.75%] [G loss: 1.640963]\n",
      "66 [D loss: 0.329060, acc.: 90.62%] [G loss: 1.656212]\n",
      "67 [D loss: 0.446278, acc.: 76.56%] [G loss: 2.122335]\n",
      "68 [D loss: 0.291170, acc.: 92.19%] [G loss: 1.978522]\n",
      "69 [D loss: 0.441053, acc.: 78.12%] [G loss: 2.408698]\n",
      "70 [D loss: 0.183436, acc.: 96.88%] [G loss: 2.323311]\n",
      "71 [D loss: 0.288887, acc.: 96.88%] [G loss: 2.011269]\n",
      "72 [D loss: 0.184079, acc.: 95.31%] [G loss: 2.457588]\n",
      "73 [D loss: 0.145206, acc.: 96.88%] [G loss: 2.445905]\n",
      "74 [D loss: 0.144119, acc.: 100.00%] [G loss: 2.272025]\n",
      "75 [D loss: 0.185213, acc.: 98.44%] [G loss: 2.295567]\n",
      "76 [D loss: 0.185064, acc.: 96.88%] [G loss: 2.413133]\n",
      "77 [D loss: 0.205516, acc.: 93.75%] [G loss: 2.290816]\n",
      "78 [D loss: 0.321186, acc.: 87.50%] [G loss: 2.478843]\n",
      "79 [D loss: 0.228487, acc.: 89.06%] [G loss: 2.377862]\n",
      "80 [D loss: 0.274979, acc.: 89.06%] [G loss: 2.548653]\n",
      "81 [D loss: 0.175108, acc.: 93.75%] [G loss: 2.694128]\n",
      "82 [D loss: 0.148401, acc.: 95.31%] [G loss: 2.638564]\n",
      "83 [D loss: 0.177102, acc.: 96.88%] [G loss: 2.821461]\n",
      "84 [D loss: 0.148681, acc.: 96.88%] [G loss: 2.563342]\n",
      "85 [D loss: 0.178331, acc.: 96.88%] [G loss: 2.761232]\n",
      "86 [D loss: 0.125067, acc.: 98.44%] [G loss: 2.378986]\n",
      "87 [D loss: 0.352277, acc.: 85.94%] [G loss: 3.594955]\n",
      "88 [D loss: 0.165984, acc.: 93.75%] [G loss: 3.165232]\n",
      "89 [D loss: 0.184296, acc.: 95.31%] [G loss: 2.617048]\n",
      "90 [D loss: 0.237977, acc.: 93.75%] [G loss: 2.287455]\n",
      "91 [D loss: 0.297040, acc.: 87.50%] [G loss: 2.601429]\n",
      "92 [D loss: 0.506162, acc.: 75.00%] [G loss: 2.579227]\n",
      "93 [D loss: 0.430009, acc.: 82.81%] [G loss: 2.618139]\n",
      "94 [D loss: 0.475660, acc.: 84.38%] [G loss: 2.594338]\n",
      "95 [D loss: 0.383501, acc.: 85.94%] [G loss: 3.073344]\n",
      "96 [D loss: 0.257031, acc.: 87.50%] [G loss: 3.077834]\n",
      "97 [D loss: 0.291244, acc.: 89.06%] [G loss: 2.973920]\n",
      "98 [D loss: 0.142455, acc.: 98.44%] [G loss: 3.173769]\n",
      "99 [D loss: 0.179458, acc.: 93.75%] [G loss: 3.311869]\n",
      "100 [D loss: 0.146411, acc.: 96.88%] [G loss: 2.981128]\n",
      "generated_data\n",
      "101 [D loss: 0.406611, acc.: 82.81%] [G loss: 3.400918]\n",
      "102 [D loss: 0.302600, acc.: 90.62%] [G loss: 3.523191]\n",
      "103 [D loss: 0.443093, acc.: 85.94%] [G loss: 3.212499]\n",
      "104 [D loss: 0.406287, acc.: 87.50%] [G loss: 2.892142]\n",
      "105 [D loss: 0.420791, acc.: 90.62%] [G loss: 2.818935]\n",
      "106 [D loss: 0.422175, acc.: 89.06%] [G loss: 3.242045]\n",
      "107 [D loss: 0.379922, acc.: 90.62%] [G loss: 3.190654]\n",
      "108 [D loss: 0.431073, acc.: 87.50%] [G loss: 3.230652]\n",
      "109 [D loss: 0.391173, acc.: 89.06%] [G loss: 3.513757]\n",
      "110 [D loss: 0.366758, acc.: 89.06%] [G loss: 3.304186]\n",
      "111 [D loss: 0.591331, acc.: 73.44%] [G loss: 4.246596]\n",
      "112 [D loss: 0.318630, acc.: 85.94%] [G loss: 4.128961]\n",
      "113 [D loss: 0.336214, acc.: 90.62%] [G loss: 2.510561]\n",
      "114 [D loss: 0.658220, acc.: 65.62%] [G loss: 3.274843]\n",
      "115 [D loss: 0.281738, acc.: 87.50%] [G loss: 3.082622]\n",
      "116 [D loss: 0.512855, acc.: 79.69%] [G loss: 2.820874]\n",
      "117 [D loss: 0.375479, acc.: 84.38%] [G loss: 3.023219]\n",
      "118 [D loss: 0.297661, acc.: 90.62%] [G loss: 2.614038]\n",
      "119 [D loss: 0.492361, acc.: 79.69%] [G loss: 3.109774]\n",
      "120 [D loss: 0.373119, acc.: 81.25%] [G loss: 2.785764]\n",
      "121 [D loss: 0.774023, acc.: 68.75%] [G loss: 3.425642]\n",
      "122 [D loss: 0.646973, acc.: 75.00%] [G loss: 3.274285]\n",
      "123 [D loss: 0.708626, acc.: 70.31%] [G loss: 3.537617]\n",
      "124 [D loss: 0.431775, acc.: 84.38%] [G loss: 3.075768]\n",
      "125 [D loss: 0.614123, acc.: 76.56%] [G loss: 2.682912]\n",
      "126 [D loss: 0.546361, acc.: 82.81%] [G loss: 2.896179]\n",
      "127 [D loss: 0.449941, acc.: 82.81%] [G loss: 3.227995]\n",
      "128 [D loss: 0.411529, acc.: 87.50%] [G loss: 3.182335]\n",
      "129 [D loss: 0.360047, acc.: 84.38%] [G loss: 2.403690]\n",
      "130 [D loss: 0.593932, acc.: 62.50%] [G loss: 3.194555]\n",
      "131 [D loss: 0.321926, acc.: 82.81%] [G loss: 3.231421]\n",
      "132 [D loss: 0.309721, acc.: 84.38%] [G loss: 2.300936]\n",
      "133 [D loss: 0.427208, acc.: 81.25%] [G loss: 2.313752]\n",
      "134 [D loss: 0.275873, acc.: 89.06%] [G loss: 2.445568]\n",
      "135 [D loss: 0.271813, acc.: 90.62%] [G loss: 2.525413]\n",
      "136 [D loss: 0.283459, acc.: 90.62%] [G loss: 2.750007]\n",
      "137 [D loss: 0.292832, acc.: 89.06%] [G loss: 2.820353]\n",
      "138 [D loss: 0.203833, acc.: 92.19%] [G loss: 2.623639]\n",
      "139 [D loss: 0.272207, acc.: 87.50%] [G loss: 2.569670]\n",
      "140 [D loss: 0.311172, acc.: 84.38%] [G loss: 3.118917]\n",
      "141 [D loss: 0.239258, acc.: 89.06%] [G loss: 2.877961]\n",
      "142 [D loss: 0.451767, acc.: 75.00%] [G loss: 2.375232]\n",
      "143 [D loss: 0.361792, acc.: 85.94%] [G loss: 2.162444]\n",
      "144 [D loss: 0.518202, acc.: 78.12%] [G loss: 2.146907]\n",
      "145 [D loss: 0.400368, acc.: 90.62%] [G loss: 2.648660]\n",
      "146 [D loss: 0.398891, acc.: 90.62%] [G loss: 2.796371]\n",
      "147 [D loss: 0.325746, acc.: 90.62%] [G loss: 2.267900]\n",
      "148 [D loss: 0.375395, acc.: 90.62%] [G loss: 2.359799]\n",
      "149 [D loss: 0.288431, acc.: 92.19%] [G loss: 2.469212]\n",
      "150 [D loss: 0.282755, acc.: 92.19%] [G loss: 2.347044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 [D loss: 0.252455, acc.: 92.19%] [G loss: 2.345118]\n",
      "152 [D loss: 0.227604, acc.: 92.19%] [G loss: 2.321765]\n",
      "153 [D loss: 0.244772, acc.: 92.19%] [G loss: 2.308035]\n",
      "154 [D loss: 0.229997, acc.: 92.19%] [G loss: 2.416385]\n",
      "155 [D loss: 0.226556, acc.: 92.19%] [G loss: 2.461457]\n",
      "156 [D loss: 0.285188, acc.: 90.62%] [G loss: 2.686482]\n",
      "157 [D loss: 0.288245, acc.: 87.50%] [G loss: 2.844541]\n",
      "158 [D loss: 0.260719, acc.: 92.19%] [G loss: 2.992775]\n",
      "159 [D loss: 0.314593, acc.: 92.19%] [G loss: 3.331412]\n",
      "160 [D loss: 0.330104, acc.: 92.19%] [G loss: 3.408270]\n",
      "161 [D loss: 0.516173, acc.: 79.69%] [G loss: 4.095533]\n",
      "162 [D loss: 0.431447, acc.: 85.94%] [G loss: 3.343642]\n",
      "163 [D loss: 0.657634, acc.: 67.19%] [G loss: 3.645575]\n",
      "164 [D loss: 0.511177, acc.: 84.38%] [G loss: 3.774504]\n",
      "165 [D loss: 0.474639, acc.: 84.38%] [G loss: 3.065371]\n",
      "166 [D loss: 0.530119, acc.: 76.56%] [G loss: 3.705643]\n",
      "167 [D loss: 0.365883, acc.: 87.50%] [G loss: 3.260273]\n",
      "168 [D loss: 0.358383, acc.: 89.06%] [G loss: 2.652167]\n",
      "169 [D loss: 0.280469, acc.: 90.62%] [G loss: 2.308833]\n",
      "170 [D loss: 0.351197, acc.: 84.38%] [G loss: 3.127263]\n",
      "171 [D loss: 0.166192, acc.: 95.31%] [G loss: 3.166490]\n",
      "172 [D loss: 0.164563, acc.: 98.44%] [G loss: 2.732853]\n",
      "173 [D loss: 0.207923, acc.: 96.88%] [G loss: 2.457562]\n",
      "174 [D loss: 0.174258, acc.: 96.88%] [G loss: 2.630700]\n",
      "175 [D loss: 0.270994, acc.: 92.19%] [G loss: 2.823925]\n",
      "176 [D loss: 0.231898, acc.: 90.62%] [G loss: 2.505220]\n",
      "177 [D loss: 0.292932, acc.: 89.06%] [G loss: 2.732743]\n",
      "178 [D loss: 0.208223, acc.: 89.06%] [G loss: 2.712357]\n",
      "179 [D loss: 0.226729, acc.: 92.19%] [G loss: 2.812904]\n",
      "180 [D loss: 0.235376, acc.: 96.88%] [G loss: 2.905721]\n",
      "181 [D loss: 0.216200, acc.: 96.88%] [G loss: 2.725519]\n",
      "182 [D loss: 0.318773, acc.: 89.06%] [G loss: 3.051343]\n",
      "183 [D loss: 0.238795, acc.: 92.19%] [G loss: 2.976828]\n",
      "184 [D loss: 0.307033, acc.: 92.19%] [G loss: 2.626808]\n",
      "185 [D loss: 0.351046, acc.: 90.62%] [G loss: 2.817680]\n",
      "186 [D loss: 0.338750, acc.: 87.50%] [G loss: 2.620139]\n",
      "187 [D loss: 0.509904, acc.: 79.69%] [G loss: 2.893338]\n",
      "188 [D loss: 0.435456, acc.: 85.94%] [G loss: 2.680732]\n",
      "189 [D loss: 0.516336, acc.: 82.81%] [G loss: 2.424842]\n",
      "190 [D loss: 0.475276, acc.: 82.81%] [G loss: 2.604060]\n",
      "191 [D loss: 0.362982, acc.: 85.94%] [G loss: 2.765018]\n",
      "192 [D loss: 0.347483, acc.: 87.50%] [G loss: 2.684302]\n",
      "193 [D loss: 0.300441, acc.: 90.62%] [G loss: 3.152916]\n",
      "194 [D loss: 0.242254, acc.: 90.62%] [G loss: 2.879940]\n",
      "195 [D loss: 0.371888, acc.: 89.06%] [G loss: 3.694605]\n",
      "196 [D loss: 0.217061, acc.: 87.50%] [G loss: 3.516006]\n",
      "197 [D loss: 0.310951, acc.: 87.50%] [G loss: 2.601883]\n",
      "198 [D loss: 0.468328, acc.: 79.69%] [G loss: 3.136785]\n",
      "199 [D loss: 0.312871, acc.: 85.94%] [G loss: 3.010303]\n",
      "200 [D loss: 0.491933, acc.: 78.12%] [G loss: 3.221386]\n",
      "generated_data\n",
      "201 [D loss: 0.374284, acc.: 85.94%] [G loss: 3.314226]\n",
      "202 [D loss: 0.395470, acc.: 85.94%] [G loss: 3.404927]\n",
      "203 [D loss: 0.417634, acc.: 78.12%] [G loss: 3.354995]\n",
      "204 [D loss: 0.351064, acc.: 85.94%] [G loss: 3.498854]\n",
      "205 [D loss: 0.296308, acc.: 85.94%] [G loss: 3.338054]\n",
      "206 [D loss: 0.303011, acc.: 90.62%] [G loss: 3.708538]\n",
      "207 [D loss: 0.299592, acc.: 89.06%] [G loss: 3.911790]\n",
      "208 [D loss: 0.515892, acc.: 75.00%] [G loss: 4.472253]\n",
      "209 [D loss: 0.340544, acc.: 90.62%] [G loss: 3.502360]\n",
      "210 [D loss: 0.641608, acc.: 71.88%] [G loss: 3.999794]\n",
      "211 [D loss: 0.343517, acc.: 92.19%] [G loss: 4.104268]\n",
      "212 [D loss: 0.366629, acc.: 92.19%] [G loss: 2.979661]\n",
      "213 [D loss: 0.420817, acc.: 90.62%] [G loss: 2.457530]\n",
      "214 [D loss: 0.366049, acc.: 90.62%] [G loss: 2.797453]\n",
      "215 [D loss: 0.376696, acc.: 87.50%] [G loss: 2.881186]\n",
      "216 [D loss: 0.298928, acc.: 90.62%] [G loss: 2.565997]\n",
      "217 [D loss: 0.298953, acc.: 93.75%] [G loss: 2.536696]\n",
      "218 [D loss: 0.238719, acc.: 93.75%] [G loss: 2.767607]\n",
      "219 [D loss: 0.249958, acc.: 92.19%] [G loss: 2.597892]\n",
      "220 [D loss: 0.295435, acc.: 89.06%] [G loss: 2.521901]\n",
      "221 [D loss: 0.282327, acc.: 90.62%] [G loss: 2.527479]\n",
      "222 [D loss: 0.305092, acc.: 92.19%] [G loss: 2.205326]\n",
      "223 [D loss: 0.375963, acc.: 87.50%] [G loss: 2.242838]\n",
      "224 [D loss: 0.370537, acc.: 87.50%] [G loss: 2.429542]\n",
      "225 [D loss: 0.346436, acc.: 89.06%] [G loss: 2.279994]\n",
      "226 [D loss: 0.426405, acc.: 87.50%] [G loss: 2.338388]\n",
      "227 [D loss: 0.331230, acc.: 90.62%] [G loss: 2.256529]\n",
      "228 [D loss: 0.346870, acc.: 90.62%] [G loss: 2.380096]\n",
      "229 [D loss: 0.369749, acc.: 85.94%] [G loss: 2.698585]\n",
      "230 [D loss: 0.343714, acc.: 85.94%] [G loss: 2.531972]\n",
      "231 [D loss: 0.469726, acc.: 78.12%] [G loss: 2.509629]\n",
      "232 [D loss: 0.527220, acc.: 71.88%] [G loss: 2.896554]\n",
      "233 [D loss: 0.488246, acc.: 81.25%] [G loss: 2.934512]\n",
      "234 [D loss: 0.653257, acc.: 67.19%] [G loss: 3.258823]\n",
      "235 [D loss: 0.478737, acc.: 79.69%] [G loss: 3.084820]\n",
      "236 [D loss: 0.497368, acc.: 79.69%] [G loss: 2.792005]\n",
      "237 [D loss: 0.489751, acc.: 78.12%] [G loss: 3.007086]\n",
      "238 [D loss: 0.340377, acc.: 85.94%] [G loss: 2.835623]\n",
      "239 [D loss: 0.410155, acc.: 82.81%] [G loss: 3.090213]\n",
      "240 [D loss: 0.328468, acc.: 87.50%] [G loss: 3.200055]\n",
      "241 [D loss: 0.361062, acc.: 85.94%] [G loss: 2.899628]\n",
      "242 [D loss: 0.315341, acc.: 93.75%] [G loss: 2.911845]\n",
      "243 [D loss: 0.384091, acc.: 84.38%] [G loss: 3.799143]\n",
      "244 [D loss: 0.294859, acc.: 85.94%] [G loss: 3.553556]\n",
      "245 [D loss: 0.293689, acc.: 87.50%] [G loss: 2.543308]\n",
      "246 [D loss: 0.291849, acc.: 93.75%] [G loss: 2.802306]\n",
      "247 [D loss: 0.206876, acc.: 92.19%] [G loss: 3.254921]\n",
      "248 [D loss: 0.163170, acc.: 92.19%] [G loss: 3.043397]\n",
      "249 [D loss: 0.110116, acc.: 98.44%] [G loss: 2.500050]\n",
      "250 [D loss: 0.158190, acc.: 95.31%] [G loss: 2.759663]\n",
      "251 [D loss: 0.205377, acc.: 92.19%] [G loss: 2.959794]\n",
      "252 [D loss: 0.191673, acc.: 92.19%] [G loss: 2.887126]\n",
      "253 [D loss: 0.239196, acc.: 93.75%] [G loss: 2.490559]\n",
      "254 [D loss: 0.278626, acc.: 92.19%] [G loss: 2.554780]\n",
      "255 [D loss: 0.274150, acc.: 92.19%] [G loss: 2.535418]\n",
      "256 [D loss: 0.279480, acc.: 93.75%] [G loss: 2.606782]\n",
      "257 [D loss: 0.275792, acc.: 93.75%] [G loss: 2.550361]\n",
      "258 [D loss: 0.301534, acc.: 93.75%] [G loss: 2.648016]\n",
      "259 [D loss: 0.237866, acc.: 93.75%] [G loss: 2.569788]\n",
      "260 [D loss: 0.339583, acc.: 87.50%] [G loss: 2.726435]\n",
      "261 [D loss: 0.318965, acc.: 89.06%] [G loss: 2.704964]\n",
      "262 [D loss: 0.337230, acc.: 89.06%] [G loss: 2.670094]\n",
      "263 [D loss: 0.364594, acc.: 87.50%] [G loss: 2.765839]\n",
      "264 [D loss: 0.354758, acc.: 92.19%] [G loss: 2.353092]\n",
      "265 [D loss: 0.398507, acc.: 87.50%] [G loss: 2.391886]\n",
      "266 [D loss: 0.354136, acc.: 85.94%] [G loss: 2.625478]\n",
      "267 [D loss: 0.325655, acc.: 87.50%] [G loss: 2.419652]\n",
      "268 [D loss: 0.320402, acc.: 89.06%] [G loss: 2.668767]\n",
      "269 [D loss: 0.339307, acc.: 90.62%] [G loss: 3.008238]\n",
      "270 [D loss: 0.319254, acc.: 85.94%] [G loss: 2.837804]\n",
      "271 [D loss: 0.414480, acc.: 85.94%] [G loss: 2.709062]\n",
      "272 [D loss: 0.440633, acc.: 82.81%] [G loss: 2.978168]\n",
      "273 [D loss: 0.394263, acc.: 87.50%] [G loss: 2.619356]\n",
      "274 [D loss: 0.467925, acc.: 85.94%] [G loss: 2.476464]\n",
      "275 [D loss: 0.407904, acc.: 85.94%] [G loss: 2.696010]\n",
      "276 [D loss: 0.434063, acc.: 85.94%] [G loss: 2.647440]\n",
      "277 [D loss: 0.402278, acc.: 84.38%] [G loss: 2.932132]\n",
      "278 [D loss: 0.393157, acc.: 85.94%] [G loss: 2.707332]\n",
      "279 [D loss: 0.394283, acc.: 84.38%] [G loss: 2.690551]\n",
      "280 [D loss: 0.334531, acc.: 89.06%] [G loss: 2.862959]\n",
      "281 [D loss: 0.320037, acc.: 85.94%] [G loss: 2.635192]\n",
      "282 [D loss: 0.350511, acc.: 84.38%] [G loss: 3.057703]\n",
      "283 [D loss: 0.279416, acc.: 92.19%] [G loss: 3.012750]\n",
      "284 [D loss: 0.536200, acc.: 76.56%] [G loss: 4.203754]\n",
      "285 [D loss: 0.332591, acc.: 90.62%] [G loss: 3.846866]\n",
      "286 [D loss: 0.580036, acc.: 78.12%] [G loss: 3.785562]\n",
      "287 [D loss: 0.348873, acc.: 89.06%] [G loss: 3.364357]\n",
      "288 [D loss: 0.454888, acc.: 84.38%] [G loss: 2.800169]\n",
      "289 [D loss: 0.385406, acc.: 85.94%] [G loss: 2.720212]\n",
      "290 [D loss: 0.400324, acc.: 82.81%] [G loss: 2.945838]\n",
      "291 [D loss: 0.322563, acc.: 90.62%] [G loss: 3.137758]\n",
      "292 [D loss: 0.308133, acc.: 92.19%] [G loss: 2.602123]\n",
      "293 [D loss: 0.400914, acc.: 85.94%] [G loss: 2.932518]\n",
      "294 [D loss: 0.293611, acc.: 93.75%] [G loss: 3.053247]\n",
      "295 [D loss: 0.271210, acc.: 93.75%] [G loss: 2.475878]\n",
      "296 [D loss: 0.339672, acc.: 90.62%] [G loss: 2.720737]\n",
      "297 [D loss: 0.262624, acc.: 93.75%] [G loss: 3.022490]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298 [D loss: 0.244453, acc.: 93.75%] [G loss: 2.665296]\n",
      "299 [D loss: 0.307517, acc.: 92.19%] [G loss: 2.845500]\n",
      "300 [D loss: 0.219141, acc.: 92.19%] [G loss: 2.815912]\n",
      "generated_data\n",
      "301 [D loss: 0.260153, acc.: 92.19%] [G loss: 2.436272]\n",
      "302 [D loss: 0.223571, acc.: 93.75%] [G loss: 2.457398]\n",
      "303 [D loss: 0.180221, acc.: 95.31%] [G loss: 2.564689]\n",
      "304 [D loss: 0.166180, acc.: 96.88%] [G loss: 2.454525]\n",
      "305 [D loss: 0.152586, acc.: 98.44%] [G loss: 2.458951]\n",
      "306 [D loss: 0.164276, acc.: 96.88%] [G loss: 2.348035]\n",
      "307 [D loss: 0.157560, acc.: 96.88%] [G loss: 2.168251]\n",
      "308 [D loss: 0.175006, acc.: 96.88%] [G loss: 2.243910]\n",
      "309 [D loss: 0.192410, acc.: 96.88%] [G loss: 2.218220]\n",
      "310 [D loss: 0.221381, acc.: 96.88%] [G loss: 2.238924]\n",
      "311 [D loss: 0.203185, acc.: 96.88%] [G loss: 2.261120]\n",
      "312 [D loss: 0.304247, acc.: 90.62%] [G loss: 2.628431]\n",
      "313 [D loss: 0.230211, acc.: 93.75%] [G loss: 2.616822]\n",
      "314 [D loss: 0.360317, acc.: 87.50%] [G loss: 2.623740]\n",
      "315 [D loss: 0.297431, acc.: 90.62%] [G loss: 2.720988]\n",
      "316 [D loss: 0.373400, acc.: 82.81%] [G loss: 2.897316]\n",
      "317 [D loss: 0.345737, acc.: 87.50%] [G loss: 2.608763]\n",
      "318 [D loss: 0.433994, acc.: 85.94%] [G loss: 2.550295]\n",
      "319 [D loss: 0.394409, acc.: 87.50%] [G loss: 2.368053]\n",
      "320 [D loss: 0.355138, acc.: 87.50%] [G loss: 2.502629]\n",
      "321 [D loss: 0.343777, acc.: 89.06%] [G loss: 2.479848]\n",
      "322 [D loss: 0.344645, acc.: 87.50%] [G loss: 2.636718]\n",
      "323 [D loss: 0.291838, acc.: 89.06%] [G loss: 2.643595]\n",
      "324 [D loss: 0.242189, acc.: 89.06%] [G loss: 2.374271]\n",
      "325 [D loss: 0.303494, acc.: 92.19%] [G loss: 2.670947]\n",
      "326 [D loss: 0.219052, acc.: 93.75%] [G loss: 2.606507]\n",
      "327 [D loss: 0.207608, acc.: 93.75%] [G loss: 2.503792]\n",
      "328 [D loss: 0.207625, acc.: 93.75%] [G loss: 2.353257]\n",
      "329 [D loss: 0.200240, acc.: 93.75%] [G loss: 2.550158]\n",
      "330 [D loss: 0.153130, acc.: 96.88%] [G loss: 2.826618]\n",
      "331 [D loss: 0.153638, acc.: 96.88%] [G loss: 2.614070]\n",
      "332 [D loss: 0.171281, acc.: 93.75%] [G loss: 2.688408]\n",
      "333 [D loss: 0.222390, acc.: 90.62%] [G loss: 2.980653]\n",
      "334 [D loss: 0.128201, acc.: 96.88%] [G loss: 3.229957]\n",
      "335 [D loss: 0.137293, acc.: 95.31%] [G loss: 3.288290]\n",
      "336 [D loss: 0.130620, acc.: 92.19%] [G loss: 3.321172]\n",
      "337 [D loss: 0.162898, acc.: 92.19%] [G loss: 3.130453]\n",
      "338 [D loss: 0.221237, acc.: 90.62%] [G loss: 3.721496]\n",
      "339 [D loss: 0.169284, acc.: 93.75%] [G loss: 3.217258]\n",
      "340 [D loss: 0.377235, acc.: 84.38%] [G loss: 3.393108]\n",
      "341 [D loss: 0.252521, acc.: 89.06%] [G loss: 3.197618]\n",
      "342 [D loss: 0.283694, acc.: 92.19%] [G loss: 2.435553]\n",
      "343 [D loss: 0.294310, acc.: 93.75%] [G loss: 2.069820]\n",
      "344 [D loss: 0.443254, acc.: 81.25%] [G loss: 2.783569]\n",
      "345 [D loss: 0.305681, acc.: 90.62%] [G loss: 3.103631]\n",
      "346 [D loss: 0.374026, acc.: 85.94%] [G loss: 2.595913]\n",
      "347 [D loss: 0.376005, acc.: 89.06%] [G loss: 2.567395]\n",
      "348 [D loss: 0.280834, acc.: 90.62%] [G loss: 2.361190]\n",
      "349 [D loss: 0.364056, acc.: 89.06%] [G loss: 2.689808]\n",
      "350 [D loss: 0.331809, acc.: 90.62%] [G loss: 2.921322]\n",
      "351 [D loss: 0.335558, acc.: 89.06%] [G loss: 2.738127]\n",
      "352 [D loss: 0.294405, acc.: 90.62%] [G loss: 2.406931]\n",
      "353 [D loss: 0.271157, acc.: 90.62%] [G loss: 2.353124]\n",
      "354 [D loss: 0.301055, acc.: 92.19%] [G loss: 2.359648]\n",
      "355 [D loss: 0.280774, acc.: 93.75%] [G loss: 2.266969]\n",
      "356 [D loss: 0.344682, acc.: 87.50%] [G loss: 2.178703]\n",
      "357 [D loss: 0.370128, acc.: 85.94%] [G loss: 2.582509]\n",
      "358 [D loss: 0.338332, acc.: 89.06%] [G loss: 2.654550]\n",
      "359 [D loss: 0.330496, acc.: 89.06%] [G loss: 2.595173]\n",
      "360 [D loss: 0.348523, acc.: 87.50%] [G loss: 2.626031]\n",
      "361 [D loss: 0.336392, acc.: 85.94%] [G loss: 2.574350]\n",
      "362 [D loss: 0.388019, acc.: 87.50%] [G loss: 2.803032]\n",
      "363 [D loss: 0.321963, acc.: 89.06%] [G loss: 2.798939]\n",
      "364 [D loss: 0.333244, acc.: 87.50%] [G loss: 2.596967]\n",
      "365 [D loss: 0.451419, acc.: 81.25%] [G loss: 3.348077]\n",
      "366 [D loss: 0.259308, acc.: 90.62%] [G loss: 3.480654]\n",
      "367 [D loss: 0.285940, acc.: 90.62%] [G loss: 2.924484]\n",
      "368 [D loss: 0.289579, acc.: 90.62%] [G loss: 2.955528]\n",
      "369 [D loss: 0.250266, acc.: 93.75%] [G loss: 3.175469]\n",
      "370 [D loss: 0.238721, acc.: 89.06%] [G loss: 3.002037]\n",
      "371 [D loss: 0.257691, acc.: 89.06%] [G loss: 3.237182]\n",
      "372 [D loss: 0.230355, acc.: 89.06%] [G loss: 3.168332]\n",
      "373 [D loss: 0.207008, acc.: 93.75%] [G loss: 2.664343]\n",
      "374 [D loss: 0.201419, acc.: 90.62%] [G loss: 3.333213]\n",
      "375 [D loss: 0.123202, acc.: 93.75%] [G loss: 3.522728]\n",
      "376 [D loss: 0.132337, acc.: 93.75%] [G loss: 3.249884]\n",
      "377 [D loss: 0.124236, acc.: 98.44%] [G loss: 2.990034]\n",
      "378 [D loss: 0.200714, acc.: 93.75%] [G loss: 3.331192]\n",
      "379 [D loss: 0.135544, acc.: 96.88%] [G loss: 3.054663]\n",
      "380 [D loss: 0.329983, acc.: 87.50%] [G loss: 3.515864]\n",
      "381 [D loss: 0.194379, acc.: 92.19%] [G loss: 3.331498]\n",
      "382 [D loss: 0.255909, acc.: 93.75%] [G loss: 2.688154]\n",
      "383 [D loss: 0.390369, acc.: 82.81%] [G loss: 2.645901]\n",
      "384 [D loss: 0.332334, acc.: 87.50%] [G loss: 3.242941]\n",
      "385 [D loss: 0.191520, acc.: 95.31%] [G loss: 2.881774]\n",
      "386 [D loss: 0.194086, acc.: 96.88%] [G loss: 2.514505]\n",
      "387 [D loss: 0.189116, acc.: 96.88%] [G loss: 2.510571]\n",
      "388 [D loss: 0.191748, acc.: 93.75%] [G loss: 2.795770]\n",
      "389 [D loss: 0.180308, acc.: 95.31%] [G loss: 3.136643]\n",
      "390 [D loss: 0.205998, acc.: 93.75%] [G loss: 3.108992]\n",
      "391 [D loss: 0.217714, acc.: 93.75%] [G loss: 3.737823]\n",
      "392 [D loss: 0.192399, acc.: 93.75%] [G loss: 3.206678]\n",
      "393 [D loss: 0.227268, acc.: 93.75%] [G loss: 2.541280]\n",
      "394 [D loss: 0.267382, acc.: 93.75%] [G loss: 3.052257]\n",
      "395 [D loss: 0.196884, acc.: 95.31%] [G loss: 3.026485]\n",
      "396 [D loss: 0.193922, acc.: 95.31%] [G loss: 2.733240]\n",
      "397 [D loss: 0.238133, acc.: 95.31%] [G loss: 2.659333]\n",
      "398 [D loss: 0.219287, acc.: 95.31%] [G loss: 2.942622]\n",
      "399 [D loss: 0.250104, acc.: 93.75%] [G loss: 2.767547]\n",
      "400 [D loss: 0.303408, acc.: 92.19%] [G loss: 2.908715]\n",
      "generated_data\n",
      "401 [D loss: 0.291264, acc.: 92.19%] [G loss: 2.637743]\n",
      "402 [D loss: 0.341607, acc.: 92.19%] [G loss: 2.257566]\n",
      "403 [D loss: 0.337958, acc.: 92.19%] [G loss: 2.463559]\n",
      "404 [D loss: 0.307234, acc.: 93.75%] [G loss: 2.166530]\n",
      "405 [D loss: 0.364279, acc.: 90.62%] [G loss: 2.181585]\n",
      "406 [D loss: 0.403091, acc.: 85.94%] [G loss: 1.814204]\n",
      "407 [D loss: 0.555367, acc.: 75.00%] [G loss: 1.688857]\n",
      "408 [D loss: 0.552586, acc.: 67.19%] [G loss: 1.751546]\n",
      "409 [D loss: 0.491150, acc.: 68.75%] [G loss: 2.469094]\n",
      "410 [D loss: 0.360261, acc.: 84.38%] [G loss: 2.524853]\n",
      "411 [D loss: 0.378796, acc.: 85.94%] [G loss: 2.386776]\n",
      "412 [D loss: 0.406337, acc.: 84.38%] [G loss: 2.833315]\n",
      "413 [D loss: 0.283489, acc.: 87.50%] [G loss: 2.746381]\n",
      "414 [D loss: 0.294934, acc.: 87.50%] [G loss: 2.589790]\n",
      "415 [D loss: 0.285010, acc.: 89.06%] [G loss: 2.360018]\n",
      "416 [D loss: 0.275867, acc.: 93.75%] [G loss: 2.522425]\n",
      "417 [D loss: 0.252636, acc.: 90.62%] [G loss: 2.988272]\n",
      "418 [D loss: 0.215981, acc.: 93.75%] [G loss: 3.011406]\n",
      "419 [D loss: 0.227866, acc.: 93.75%] [G loss: 2.962751]\n",
      "420 [D loss: 0.224439, acc.: 92.19%] [G loss: 3.043350]\n",
      "421 [D loss: 0.180061, acc.: 92.19%] [G loss: 2.868448]\n",
      "422 [D loss: 0.162027, acc.: 96.88%] [G loss: 2.625675]\n",
      "423 [D loss: 0.151130, acc.: 96.88%] [G loss: 2.441461]\n",
      "424 [D loss: 0.137943, acc.: 96.88%] [G loss: 2.396981]\n",
      "425 [D loss: 0.180919, acc.: 96.88%] [G loss: 2.364289]\n",
      "426 [D loss: 0.174874, acc.: 96.88%] [G loss: 2.191566]\n",
      "427 [D loss: 0.183824, acc.: 95.31%] [G loss: 2.232650]\n",
      "428 [D loss: 0.219122, acc.: 92.19%] [G loss: 2.362908]\n",
      "429 [D loss: 0.212413, acc.: 93.75%] [G loss: 2.342692]\n",
      "430 [D loss: 0.192382, acc.: 93.75%] [G loss: 2.399681]\n",
      "431 [D loss: 0.234922, acc.: 92.19%] [G loss: 2.748094]\n",
      "432 [D loss: 0.225904, acc.: 93.75%] [G loss: 2.729877]\n",
      "433 [D loss: 0.255462, acc.: 92.19%] [G loss: 2.508807]\n",
      "434 [D loss: 0.217445, acc.: 93.75%] [G loss: 2.644167]\n",
      "435 [D loss: 0.215798, acc.: 93.75%] [G loss: 2.500334]\n",
      "436 [D loss: 0.212568, acc.: 93.75%] [G loss: 2.429267]\n",
      "437 [D loss: 0.181429, acc.: 93.75%] [G loss: 2.550790]\n",
      "438 [D loss: 0.184857, acc.: 93.75%] [G loss: 2.439992]\n",
      "439 [D loss: 0.149434, acc.: 93.75%] [G loss: 2.586734]\n",
      "440 [D loss: 0.131130, acc.: 95.31%] [G loss: 2.529230]\n",
      "441 [D loss: 0.128926, acc.: 93.75%] [G loss: 2.525374]\n",
      "442 [D loss: 0.119632, acc.: 96.88%] [G loss: 2.415128]\n",
      "443 [D loss: 0.111159, acc.: 96.88%] [G loss: 2.470159]\n",
      "444 [D loss: 0.103078, acc.: 100.00%] [G loss: 2.159362]\n",
      "445 [D loss: 0.159831, acc.: 93.75%] [G loss: 2.218290]\n",
      "446 [D loss: 0.215097, acc.: 92.19%] [G loss: 2.500895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447 [D loss: 0.140166, acc.: 98.44%] [G loss: 2.649817]\n",
      "448 [D loss: 0.164227, acc.: 96.88%] [G loss: 2.109673]\n",
      "449 [D loss: 0.305580, acc.: 85.94%] [G loss: 2.102418]\n",
      "450 [D loss: 0.303916, acc.: 90.62%] [G loss: 2.310626]\n",
      "451 [D loss: 0.289936, acc.: 93.75%] [G loss: 2.331345]\n",
      "452 [D loss: 0.289611, acc.: 92.19%] [G loss: 2.537964]\n",
      "453 [D loss: 0.203578, acc.: 93.75%] [G loss: 2.951217]\n",
      "454 [D loss: 0.198956, acc.: 92.19%] [G loss: 2.858400]\n",
      "455 [D loss: 0.202141, acc.: 93.75%] [G loss: 2.904790]\n",
      "456 [D loss: 0.192781, acc.: 95.31%] [G loss: 3.415874]\n",
      "457 [D loss: 0.152835, acc.: 93.75%] [G loss: 3.624343]\n",
      "458 [D loss: 0.169642, acc.: 95.31%] [G loss: 3.095323]\n",
      "459 [D loss: 0.364929, acc.: 84.38%] [G loss: 4.536909]\n",
      "460 [D loss: 0.256362, acc.: 87.50%] [G loss: 4.372355]\n",
      "461 [D loss: 0.237549, acc.: 89.06%] [G loss: 3.217726]\n",
      "462 [D loss: 0.209464, acc.: 92.19%] [G loss: 2.595729]\n",
      "463 [D loss: 0.251189, acc.: 89.06%] [G loss: 4.264249]\n",
      "464 [D loss: 0.177834, acc.: 90.62%] [G loss: 4.200736]\n",
      "465 [D loss: 0.184978, acc.: 90.62%] [G loss: 3.543780]\n",
      "466 [D loss: 0.143777, acc.: 93.75%] [G loss: 3.061417]\n",
      "467 [D loss: 0.128238, acc.: 95.31%] [G loss: 3.049239]\n",
      "468 [D loss: 0.122059, acc.: 96.88%] [G loss: 3.093080]\n",
      "469 [D loss: 0.110401, acc.: 93.75%] [G loss: 3.667372]\n",
      "470 [D loss: 0.097741, acc.: 96.88%] [G loss: 3.849367]\n",
      "471 [D loss: 0.108934, acc.: 95.31%] [G loss: 3.505892]\n",
      "472 [D loss: 0.120837, acc.: 95.31%] [G loss: 3.726049]\n",
      "473 [D loss: 0.109675, acc.: 95.31%] [G loss: 4.328797]\n",
      "474 [D loss: 0.077897, acc.: 96.88%] [G loss: 4.057633]\n",
      "475 [D loss: 0.256249, acc.: 87.50%] [G loss: 5.199337]\n",
      "476 [D loss: 0.098298, acc.: 96.88%] [G loss: 5.190775]\n",
      "477 [D loss: 0.217668, acc.: 90.62%] [G loss: 3.812177]\n",
      "478 [D loss: 0.067013, acc.: 98.44%] [G loss: 3.021783]\n",
      "479 [D loss: 0.069704, acc.: 98.44%] [G loss: 2.783503]\n",
      "480 [D loss: 0.092412, acc.: 98.44%] [G loss: 2.603966]\n",
      "481 [D loss: 0.086863, acc.: 96.88%] [G loss: 2.265289]\n",
      "482 [D loss: 0.174678, acc.: 95.31%] [G loss: 2.398662]\n",
      "483 [D loss: 0.190052, acc.: 95.31%] [G loss: 3.131742]\n",
      "484 [D loss: 0.086360, acc.: 98.44%] [G loss: 3.278591]\n",
      "485 [D loss: 0.131100, acc.: 96.88%] [G loss: 2.628545]\n",
      "486 [D loss: 0.126775, acc.: 96.88%] [G loss: 2.375152]\n",
      "487 [D loss: 0.117660, acc.: 100.00%] [G loss: 2.043881]\n",
      "488 [D loss: 0.167707, acc.: 92.19%] [G loss: 2.069652]\n",
      "489 [D loss: 0.189946, acc.: 95.31%] [G loss: 2.322042]\n",
      "490 [D loss: 0.158578, acc.: 96.88%] [G loss: 2.580271]\n",
      "491 [D loss: 0.218767, acc.: 95.31%] [G loss: 2.834596]\n",
      "492 [D loss: 0.201636, acc.: 95.31%] [G loss: 2.981620]\n",
      "493 [D loss: 0.192056, acc.: 95.31%] [G loss: 3.181326]\n",
      "494 [D loss: 0.201250, acc.: 95.31%] [G loss: 4.111062]\n",
      "495 [D loss: 0.194695, acc.: 95.31%] [G loss: 3.662573]\n",
      "496 [D loss: 0.300985, acc.: 92.19%] [G loss: 4.245833]\n",
      "497 [D loss: 0.220257, acc.: 93.75%] [G loss: 3.743280]\n",
      "498 [D loss: 0.350531, acc.: 92.19%] [G loss: 4.084577]\n",
      "499 [D loss: 0.299009, acc.: 92.19%] [G loss: 3.907063]\n",
      "500 [D loss: 0.314378, acc.: 92.19%] [G loss: 3.336953]\n",
      "generated_data\n",
      "501 [D loss: 0.316440, acc.: 90.62%] [G loss: 2.714753]\n",
      "502 [D loss: 0.299325, acc.: 89.06%] [G loss: 2.866089]\n",
      "503 [D loss: 0.276741, acc.: 90.62%] [G loss: 3.105352]\n",
      "504 [D loss: 0.335894, acc.: 87.50%] [G loss: 3.452019]\n",
      "505 [D loss: 0.239434, acc.: 90.62%] [G loss: 3.558634]\n",
      "506 [D loss: 0.245242, acc.: 93.75%] [G loss: 3.154059]\n",
      "507 [D loss: 0.224100, acc.: 93.75%] [G loss: 2.816541]\n",
      "508 [D loss: 0.221243, acc.: 93.75%] [G loss: 2.947261]\n",
      "509 [D loss: 0.211748, acc.: 92.19%] [G loss: 2.868420]\n",
      "510 [D loss: 0.234120, acc.: 92.19%] [G loss: 2.830910]\n",
      "511 [D loss: 0.431631, acc.: 75.00%] [G loss: 3.573061]\n",
      "512 [D loss: 0.243754, acc.: 92.19%] [G loss: 3.620071]\n",
      "513 [D loss: 0.321963, acc.: 89.06%] [G loss: 3.127106]\n",
      "514 [D loss: 0.340923, acc.: 85.94%] [G loss: 2.991603]\n",
      "515 [D loss: 0.430182, acc.: 81.25%] [G loss: 2.997416]\n",
      "516 [D loss: 0.382122, acc.: 82.81%] [G loss: 3.141401]\n",
      "517 [D loss: 0.399744, acc.: 85.94%] [G loss: 3.765421]\n",
      "518 [D loss: 0.349076, acc.: 89.06%] [G loss: 3.298886]\n",
      "519 [D loss: 0.452083, acc.: 87.50%] [G loss: 3.311992]\n",
      "520 [D loss: 0.301264, acc.: 90.62%] [G loss: 3.380224]\n",
      "521 [D loss: 0.262799, acc.: 93.75%] [G loss: 2.921036]\n",
      "522 [D loss: 0.264020, acc.: 92.19%] [G loss: 2.547508]\n",
      "523 [D loss: 0.249210, acc.: 93.75%] [G loss: 2.350615]\n",
      "524 [D loss: 0.380028, acc.: 89.06%] [G loss: 2.519613]\n",
      "525 [D loss: 0.248548, acc.: 92.19%] [G loss: 2.545401]\n",
      "526 [D loss: 0.208278, acc.: 93.75%] [G loss: 2.427104]\n",
      "527 [D loss: 0.204342, acc.: 93.75%] [G loss: 2.010942]\n",
      "528 [D loss: 0.381170, acc.: 85.94%] [G loss: 2.256298]\n",
      "529 [D loss: 0.194401, acc.: 93.75%] [G loss: 2.078379]\n",
      "530 [D loss: 0.245807, acc.: 90.62%] [G loss: 2.184469]\n",
      "531 [D loss: 0.211539, acc.: 90.62%] [G loss: 1.986848]\n",
      "532 [D loss: 0.205615, acc.: 93.75%] [G loss: 2.099030]\n",
      "533 [D loss: 0.227550, acc.: 92.19%] [G loss: 1.945182]\n",
      "534 [D loss: 0.260181, acc.: 92.19%] [G loss: 2.236316]\n",
      "535 [D loss: 0.269214, acc.: 87.50%] [G loss: 2.135735]\n",
      "536 [D loss: 0.227399, acc.: 90.62%] [G loss: 1.919429]\n",
      "537 [D loss: 0.282698, acc.: 89.06%] [G loss: 2.146007]\n",
      "538 [D loss: 0.214637, acc.: 92.19%] [G loss: 2.200197]\n",
      "539 [D loss: 0.223634, acc.: 90.62%] [G loss: 2.070990]\n",
      "540 [D loss: 0.235333, acc.: 92.19%] [G loss: 1.945713]\n",
      "541 [D loss: 0.208786, acc.: 92.19%] [G loss: 2.242101]\n",
      "542 [D loss: 0.232240, acc.: 92.19%] [G loss: 2.274367]\n",
      "543 [D loss: 0.186916, acc.: 93.75%] [G loss: 2.413701]\n",
      "544 [D loss: 0.184454, acc.: 93.75%] [G loss: 2.457186]\n",
      "545 [D loss: 0.167894, acc.: 95.31%] [G loss: 2.326635]\n",
      "546 [D loss: 0.171546, acc.: 95.31%] [G loss: 2.425277]\n",
      "547 [D loss: 0.141641, acc.: 95.31%] [G loss: 2.464959]\n",
      "548 [D loss: 0.365372, acc.: 87.50%] [G loss: 2.768303]\n",
      "549 [D loss: 0.184172, acc.: 93.75%] [G loss: 2.736406]\n",
      "550 [D loss: 0.235977, acc.: 92.19%] [G loss: 2.530482]\n",
      "551 [D loss: 0.289627, acc.: 90.62%] [G loss: 2.759599]\n",
      "552 [D loss: 0.291674, acc.: 92.19%] [G loss: 3.026073]\n",
      "553 [D loss: 0.282031, acc.: 87.50%] [G loss: 3.088129]\n",
      "554 [D loss: 0.337493, acc.: 85.94%] [G loss: 3.043752]\n",
      "555 [D loss: 0.273936, acc.: 92.19%] [G loss: 2.895873]\n",
      "556 [D loss: 0.311030, acc.: 89.06%] [G loss: 2.630840]\n",
      "557 [D loss: 0.362007, acc.: 89.06%] [G loss: 2.792145]\n",
      "558 [D loss: 0.292887, acc.: 90.62%] [G loss: 2.816200]\n",
      "559 [D loss: 0.382063, acc.: 89.06%] [G loss: 2.378433]\n",
      "560 [D loss: 0.345333, acc.: 89.06%] [G loss: 2.388517]\n",
      "561 [D loss: 0.311744, acc.: 90.62%] [G loss: 2.804827]\n",
      "562 [D loss: 0.302226, acc.: 89.06%] [G loss: 2.875674]\n",
      "563 [D loss: 0.265153, acc.: 90.62%] [G loss: 2.612269]\n",
      "564 [D loss: 0.256460, acc.: 89.06%] [G loss: 2.797782]\n",
      "565 [D loss: 0.212235, acc.: 90.62%] [G loss: 2.875108]\n",
      "566 [D loss: 0.166443, acc.: 90.62%] [G loss: 2.790781]\n",
      "567 [D loss: 0.178041, acc.: 90.62%] [G loss: 2.260566]\n",
      "568 [D loss: 0.258883, acc.: 89.06%] [G loss: 2.522227]\n",
      "569 [D loss: 0.185676, acc.: 92.19%] [G loss: 2.619731]\n",
      "570 [D loss: 0.200553, acc.: 90.62%] [G loss: 2.916999]\n",
      "571 [D loss: 0.154932, acc.: 95.31%] [G loss: 2.650318]\n",
      "572 [D loss: 0.188189, acc.: 92.19%] [G loss: 2.271057]\n",
      "573 [D loss: 0.146300, acc.: 96.88%] [G loss: 2.074339]\n",
      "574 [D loss: 0.357458, acc.: 90.62%] [G loss: 2.133834]\n",
      "575 [D loss: 0.265434, acc.: 90.62%] [G loss: 2.592003]\n",
      "576 [D loss: 0.297649, acc.: 87.50%] [G loss: 2.414802]\n",
      "577 [D loss: 0.295333, acc.: 85.94%] [G loss: 1.988646]\n",
      "578 [D loss: 0.245496, acc.: 89.06%] [G loss: 1.908139]\n",
      "579 [D loss: 0.292258, acc.: 89.06%] [G loss: 2.142521]\n",
      "580 [D loss: 0.204566, acc.: 92.19%] [G loss: 2.253317]\n",
      "581 [D loss: 0.196858, acc.: 90.62%] [G loss: 2.038679]\n",
      "582 [D loss: 0.243528, acc.: 90.62%] [G loss: 2.138977]\n",
      "583 [D loss: 0.201627, acc.: 93.75%] [G loss: 2.577434]\n",
      "584 [D loss: 0.262035, acc.: 89.06%] [G loss: 2.521695]\n",
      "585 [D loss: 0.206424, acc.: 93.75%] [G loss: 2.365453]\n",
      "586 [D loss: 0.253395, acc.: 89.06%] [G loss: 2.677138]\n",
      "587 [D loss: 0.239526, acc.: 89.06%] [G loss: 2.611872]\n",
      "588 [D loss: 0.210957, acc.: 92.19%] [G loss: 2.649402]\n",
      "589 [D loss: 0.244135, acc.: 89.06%] [G loss: 3.045722]\n",
      "590 [D loss: 0.266266, acc.: 92.19%] [G loss: 3.148504]\n",
      "591 [D loss: 0.263987, acc.: 89.06%] [G loss: 2.848342]\n",
      "592 [D loss: 0.290401, acc.: 89.06%] [G loss: 2.838974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593 [D loss: 0.278186, acc.: 89.06%] [G loss: 2.526172]\n",
      "594 [D loss: 0.376461, acc.: 82.81%] [G loss: 2.300360]\n",
      "595 [D loss: 0.305582, acc.: 87.50%] [G loss: 2.434676]\n",
      "596 [D loss: 0.306420, acc.: 90.62%] [G loss: 2.606611]\n",
      "597 [D loss: 0.299811, acc.: 90.62%] [G loss: 2.245359]\n",
      "598 [D loss: 0.331685, acc.: 87.50%] [G loss: 2.327141]\n",
      "599 [D loss: 0.289902, acc.: 90.62%] [G loss: 2.325922]\n",
      "600 [D loss: 0.285872, acc.: 90.62%] [G loss: 2.110035]\n",
      "generated_data\n",
      "601 [D loss: 0.339612, acc.: 87.50%] [G loss: 2.275066]\n",
      "602 [D loss: 0.263370, acc.: 90.62%] [G loss: 2.513970]\n",
      "603 [D loss: 0.260578, acc.: 87.50%] [G loss: 2.548159]\n",
      "604 [D loss: 0.216418, acc.: 90.62%] [G loss: 2.505454]\n",
      "605 [D loss: 0.236768, acc.: 87.50%] [G loss: 2.389684]\n",
      "606 [D loss: 0.276252, acc.: 87.50%] [G loss: 2.660067]\n",
      "607 [D loss: 0.231572, acc.: 90.62%] [G loss: 2.854624]\n",
      "608 [D loss: 0.216580, acc.: 92.19%] [G loss: 2.576172]\n",
      "609 [D loss: 0.216226, acc.: 90.62%] [G loss: 2.416528]\n",
      "610 [D loss: 0.205945, acc.: 92.19%] [G loss: 2.482876]\n",
      "611 [D loss: 0.235303, acc.: 90.62%] [G loss: 2.340089]\n",
      "612 [D loss: 0.264560, acc.: 89.06%] [G loss: 2.555379]\n",
      "613 [D loss: 0.227785, acc.: 90.62%] [G loss: 2.539203]\n",
      "614 [D loss: 0.244022, acc.: 90.62%] [G loss: 2.472075]\n",
      "615 [D loss: 0.257907, acc.: 90.62%] [G loss: 2.252466]\n",
      "616 [D loss: 0.239419, acc.: 92.19%] [G loss: 2.489075]\n",
      "617 [D loss: 0.273852, acc.: 92.19%] [G loss: 2.198148]\n",
      "618 [D loss: 0.264511, acc.: 92.19%] [G loss: 2.417102]\n",
      "619 [D loss: 0.258813, acc.: 92.19%] [G loss: 2.093346]\n",
      "620 [D loss: 0.266787, acc.: 90.62%] [G loss: 2.254200]\n",
      "621 [D loss: 0.252156, acc.: 92.19%] [G loss: 2.440243]\n",
      "622 [D loss: 0.247455, acc.: 92.19%] [G loss: 2.342966]\n",
      "623 [D loss: 0.248255, acc.: 92.19%] [G loss: 2.274883]\n",
      "624 [D loss: 0.226985, acc.: 92.19%] [G loss: 2.168941]\n",
      "625 [D loss: 0.253472, acc.: 92.19%] [G loss: 2.341658]\n",
      "626 [D loss: 0.221984, acc.: 92.19%] [G loss: 2.368167]\n",
      "627 [D loss: 0.233874, acc.: 90.62%] [G loss: 2.320214]\n",
      "628 [D loss: 0.223555, acc.: 92.19%] [G loss: 2.280850]\n",
      "629 [D loss: 0.207036, acc.: 92.19%] [G loss: 2.256680]\n",
      "630 [D loss: 0.199184, acc.: 92.19%] [G loss: 2.290946]\n",
      "631 [D loss: 0.187742, acc.: 92.19%] [G loss: 2.123646]\n",
      "632 [D loss: 0.181762, acc.: 92.19%] [G loss: 2.257965]\n",
      "633 [D loss: 0.174955, acc.: 92.19%] [G loss: 2.177981]\n",
      "634 [D loss: 0.163763, acc.: 92.19%] [G loss: 2.190010]\n",
      "635 [D loss: 0.178967, acc.: 92.19%] [G loss: 2.141068]\n",
      "636 [D loss: 0.158289, acc.: 95.31%] [G loss: 2.029508]\n",
      "637 [D loss: 0.153438, acc.: 95.31%] [G loss: 2.060503]\n",
      "638 [D loss: 0.158358, acc.: 93.75%] [G loss: 1.928583]\n",
      "639 [D loss: 0.162162, acc.: 95.31%] [G loss: 2.119586]\n",
      "640 [D loss: 0.154537, acc.: 95.31%] [G loss: 2.079027]\n",
      "641 [D loss: 0.159704, acc.: 93.75%] [G loss: 2.205348]\n",
      "642 [D loss: 0.169209, acc.: 95.31%] [G loss: 2.214993]\n",
      "643 [D loss: 0.166094, acc.: 95.31%] [G loss: 2.249750]\n",
      "644 [D loss: 0.159826, acc.: 95.31%] [G loss: 2.245747]\n",
      "645 [D loss: 0.157373, acc.: 93.75%] [G loss: 2.161839]\n",
      "646 [D loss: 0.193833, acc.: 95.31%] [G loss: 2.171195]\n",
      "647 [D loss: 0.185478, acc.: 93.75%] [G loss: 2.315032]\n",
      "648 [D loss: 0.183618, acc.: 93.75%] [G loss: 2.136829]\n",
      "649 [D loss: 0.228152, acc.: 92.19%] [G loss: 2.272310]\n",
      "650 [D loss: 0.239770, acc.: 93.75%] [G loss: 2.468998]\n",
      "651 [D loss: 0.218670, acc.: 93.75%] [G loss: 2.736421]\n",
      "652 [D loss: 0.230195, acc.: 92.19%] [G loss: 2.729548]\n",
      "653 [D loss: 0.219679, acc.: 90.62%] [G loss: 2.586024]\n",
      "654 [D loss: 0.242548, acc.: 90.62%] [G loss: 2.583550]\n",
      "655 [D loss: 0.219601, acc.: 90.62%] [G loss: 2.541664]\n",
      "656 [D loss: 0.291569, acc.: 89.06%] [G loss: 2.662823]\n",
      "657 [D loss: 0.237154, acc.: 92.19%] [G loss: 2.670251]\n",
      "658 [D loss: 0.244038, acc.: 92.19%] [G loss: 2.449020]\n",
      "659 [D loss: 0.224646, acc.: 92.19%] [G loss: 2.450990]\n",
      "660 [D loss: 0.243068, acc.: 92.19%] [G loss: 2.259787]\n",
      "661 [D loss: 0.231603, acc.: 92.19%] [G loss: 2.447068]\n",
      "662 [D loss: 0.224291, acc.: 92.19%] [G loss: 2.626194]\n",
      "663 [D loss: 0.271087, acc.: 90.62%] [G loss: 2.448747]\n",
      "664 [D loss: 0.217089, acc.: 92.19%] [G loss: 2.351986]\n",
      "665 [D loss: 0.207203, acc.: 92.19%] [G loss: 2.401120]\n",
      "666 [D loss: 0.207903, acc.: 92.19%] [G loss: 2.345636]\n",
      "667 [D loss: 0.244453, acc.: 90.62%] [G loss: 2.381852]\n",
      "668 [D loss: 0.216570, acc.: 92.19%] [G loss: 2.486259]\n",
      "669 [D loss: 0.217571, acc.: 92.19%] [G loss: 2.252776]\n",
      "670 [D loss: 0.228357, acc.: 90.62%] [G loss: 2.150070]\n",
      "671 [D loss: 0.205250, acc.: 92.19%] [G loss: 2.301508]\n",
      "672 [D loss: 0.222688, acc.: 92.19%] [G loss: 2.383744]\n",
      "673 [D loss: 0.203641, acc.: 92.19%] [G loss: 2.416112]\n",
      "674 [D loss: 0.199899, acc.: 92.19%] [G loss: 2.289424]\n",
      "675 [D loss: 0.186052, acc.: 92.19%] [G loss: 2.241443]\n",
      "676 [D loss: 0.187102, acc.: 92.19%] [G loss: 2.115976]\n",
      "677 [D loss: 0.208081, acc.: 92.19%] [G loss: 2.045032]\n",
      "678 [D loss: 0.196860, acc.: 93.75%] [G loss: 2.137441]\n",
      "679 [D loss: 0.228700, acc.: 89.06%] [G loss: 2.398652]\n",
      "680 [D loss: 0.232100, acc.: 90.62%] [G loss: 2.397246]\n",
      "681 [D loss: 0.250309, acc.: 89.06%] [G loss: 2.266984]\n",
      "682 [D loss: 0.288122, acc.: 87.50%] [G loss: 2.320094]\n",
      "683 [D loss: 0.265034, acc.: 92.19%] [G loss: 2.491258]\n",
      "684 [D loss: 0.271860, acc.: 90.62%] [G loss: 2.188384]\n",
      "685 [D loss: 0.268967, acc.: 89.06%] [G loss: 2.375329]\n",
      "686 [D loss: 0.269147, acc.: 89.06%] [G loss: 2.228284]\n",
      "687 [D loss: 0.265037, acc.: 90.62%] [G loss: 2.305058]\n",
      "688 [D loss: 0.256964, acc.: 89.06%] [G loss: 2.144582]\n",
      "689 [D loss: 0.247384, acc.: 92.19%] [G loss: 2.218874]\n",
      "690 [D loss: 0.268637, acc.: 90.62%] [G loss: 2.320928]\n",
      "691 [D loss: 0.305054, acc.: 89.06%] [G loss: 2.489951]\n",
      "692 [D loss: 0.268624, acc.: 90.62%] [G loss: 2.520562]\n",
      "693 [D loss: 0.293754, acc.: 90.62%] [G loss: 2.235031]\n",
      "694 [D loss: 0.303962, acc.: 89.06%] [G loss: 2.079777]\n",
      "695 [D loss: 0.294128, acc.: 87.50%] [G loss: 2.030032]\n",
      "696 [D loss: 0.272493, acc.: 90.62%] [G loss: 2.072461]\n",
      "697 [D loss: 0.264251, acc.: 90.62%] [G loss: 2.229035]\n",
      "698 [D loss: 0.294396, acc.: 87.50%] [G loss: 2.259378]\n",
      "699 [D loss: 0.258167, acc.: 89.06%] [G loss: 2.336008]\n",
      "700 [D loss: 0.288932, acc.: 85.94%] [G loss: 2.455682]\n",
      "generated_data\n",
      "701 [D loss: 0.253429, acc.: 90.62%] [G loss: 2.784248]\n",
      "702 [D loss: 0.290843, acc.: 90.62%] [G loss: 2.298308]\n",
      "703 [D loss: 0.301567, acc.: 85.94%] [G loss: 2.606534]\n",
      "704 [D loss: 0.247030, acc.: 90.62%] [G loss: 2.762983]\n",
      "705 [D loss: 0.263002, acc.: 90.62%] [G loss: 2.332168]\n",
      "706 [D loss: 0.249738, acc.: 90.62%] [G loss: 2.287357]\n",
      "707 [D loss: 0.261280, acc.: 90.62%] [G loss: 2.002751]\n",
      "708 [D loss: 0.216854, acc.: 92.19%] [G loss: 2.126022]\n",
      "709 [D loss: 0.227143, acc.: 92.19%] [G loss: 1.930211]\n",
      "710 [D loss: 0.295316, acc.: 89.06%] [G loss: 2.046920]\n",
      "711 [D loss: 0.222442, acc.: 92.19%] [G loss: 2.328195]\n",
      "712 [D loss: 0.260793, acc.: 90.62%] [G loss: 2.189540]\n",
      "713 [D loss: 0.258693, acc.: 90.62%] [G loss: 2.112341]\n",
      "714 [D loss: 0.291916, acc.: 90.62%] [G loss: 2.119784]\n",
      "715 [D loss: 0.291944, acc.: 90.62%] [G loss: 2.481719]\n",
      "716 [D loss: 0.260886, acc.: 90.62%] [G loss: 2.536838]\n",
      "717 [D loss: 0.299137, acc.: 90.62%] [G loss: 2.401779]\n",
      "718 [D loss: 0.284014, acc.: 89.06%] [G loss: 2.323403]\n",
      "719 [D loss: 0.267712, acc.: 90.62%] [G loss: 2.415791]\n",
      "720 [D loss: 0.272665, acc.: 90.62%] [G loss: 2.282758]\n",
      "721 [D loss: 0.263607, acc.: 90.62%] [G loss: 2.299683]\n",
      "722 [D loss: 0.280936, acc.: 90.62%] [G loss: 2.191371]\n",
      "723 [D loss: 0.267332, acc.: 89.06%] [G loss: 2.244081]\n",
      "724 [D loss: 0.246534, acc.: 90.62%] [G loss: 2.117433]\n",
      "725 [D loss: 0.272917, acc.: 90.62%] [G loss: 2.159827]\n",
      "726 [D loss: 0.255013, acc.: 90.62%] [G loss: 2.146552]\n",
      "727 [D loss: 0.238116, acc.: 90.62%] [G loss: 1.992369]\n",
      "728 [D loss: 0.248362, acc.: 90.62%] [G loss: 2.158212]\n",
      "729 [D loss: 0.234225, acc.: 90.62%] [G loss: 1.940760]\n",
      "730 [D loss: 0.211675, acc.: 90.62%] [G loss: 2.035960]\n",
      "731 [D loss: 0.262654, acc.: 90.62%] [G loss: 2.119016]\n",
      "732 [D loss: 0.232741, acc.: 90.62%] [G loss: 2.180977]\n",
      "733 [D loss: 0.241627, acc.: 90.62%] [G loss: 2.104363]\n",
      "734 [D loss: 0.236992, acc.: 90.62%] [G loss: 2.070234]\n",
      "735 [D loss: 0.229966, acc.: 92.19%] [G loss: 2.181769]\n",
      "736 [D loss: 0.212172, acc.: 92.19%] [G loss: 2.000671]\n",
      "737 [D loss: 0.235565, acc.: 90.62%] [G loss: 2.036990]\n",
      "738 [D loss: 0.224631, acc.: 93.75%] [G loss: 2.161374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "739 [D loss: 0.213583, acc.: 92.19%] [G loss: 2.162723]\n",
      "740 [D loss: 0.215879, acc.: 92.19%] [G loss: 2.087482]\n",
      "741 [D loss: 0.204104, acc.: 93.75%] [G loss: 2.249715]\n",
      "742 [D loss: 0.207827, acc.: 93.75%] [G loss: 2.039650]\n",
      "743 [D loss: 0.224502, acc.: 92.19%] [G loss: 2.113369]\n",
      "744 [D loss: 0.202417, acc.: 93.75%] [G loss: 2.019002]\n",
      "745 [D loss: 0.197430, acc.: 93.75%] [G loss: 2.098564]\n",
      "746 [D loss: 0.220372, acc.: 92.19%] [G loss: 2.142205]\n",
      "747 [D loss: 0.197344, acc.: 92.19%] [G loss: 2.131257]\n",
      "748 [D loss: 0.236251, acc.: 92.19%] [G loss: 2.126297]\n",
      "749 [D loss: 0.220752, acc.: 92.19%] [G loss: 2.197434]\n",
      "750 [D loss: 0.228134, acc.: 92.19%] [G loss: 2.103996]\n",
      "751 [D loss: 0.209523, acc.: 92.19%] [G loss: 2.028212]\n",
      "752 [D loss: 0.215623, acc.: 92.19%] [G loss: 2.038866]\n",
      "753 [D loss: 0.229050, acc.: 92.19%] [G loss: 2.335670]\n",
      "754 [D loss: 0.254868, acc.: 85.94%] [G loss: 2.383464]\n",
      "755 [D loss: 0.222041, acc.: 92.19%] [G loss: 2.549055]\n",
      "756 [D loss: 0.233289, acc.: 90.62%] [G loss: 2.249949]\n",
      "757 [D loss: 0.239089, acc.: 89.06%] [G loss: 2.186524]\n",
      "758 [D loss: 0.236267, acc.: 89.06%] [G loss: 2.100218]\n",
      "759 [D loss: 0.277236, acc.: 89.06%] [G loss: 2.301406]\n",
      "760 [D loss: 0.267584, acc.: 89.06%] [G loss: 2.413857]\n",
      "761 [D loss: 0.270517, acc.: 90.62%] [G loss: 2.300502]\n",
      "762 [D loss: 0.286236, acc.: 89.06%] [G loss: 2.651307]\n",
      "763 [D loss: 0.229573, acc.: 90.62%] [G loss: 2.615121]\n",
      "764 [D loss: 0.244584, acc.: 90.62%] [G loss: 2.295505]\n",
      "765 [D loss: 0.243112, acc.: 90.62%] [G loss: 2.178423]\n",
      "766 [D loss: 0.199237, acc.: 90.62%] [G loss: 2.227879]\n",
      "767 [D loss: 0.205282, acc.: 92.19%] [G loss: 2.150203]\n",
      "768 [D loss: 0.190182, acc.: 92.19%] [G loss: 2.235545]\n",
      "769 [D loss: 0.201616, acc.: 92.19%] [G loss: 2.250345]\n",
      "770 [D loss: 0.193062, acc.: 92.19%] [G loss: 2.161818]\n",
      "771 [D loss: 0.184139, acc.: 93.75%] [G loss: 2.406133]\n",
      "772 [D loss: 0.223621, acc.: 90.62%] [G loss: 2.203796]\n",
      "773 [D loss: 0.201072, acc.: 92.19%] [G loss: 2.297534]\n",
      "774 [D loss: 0.201232, acc.: 95.31%] [G loss: 2.225881]\n",
      "775 [D loss: 0.225201, acc.: 92.19%] [G loss: 2.575475]\n",
      "776 [D loss: 0.234907, acc.: 92.19%] [G loss: 2.449838]\n",
      "777 [D loss: 0.224473, acc.: 90.62%] [G loss: 2.453943]\n",
      "778 [D loss: 0.256906, acc.: 90.62%] [G loss: 2.548143]\n",
      "779 [D loss: 0.238737, acc.: 92.19%] [G loss: 2.700651]\n",
      "780 [D loss: 0.256485, acc.: 90.62%] [G loss: 2.573357]\n",
      "781 [D loss: 0.268383, acc.: 92.19%] [G loss: 2.373195]\n",
      "782 [D loss: 0.240749, acc.: 90.62%] [G loss: 2.537269]\n",
      "783 [D loss: 0.245945, acc.: 90.62%] [G loss: 2.465979]\n",
      "784 [D loss: 0.268530, acc.: 90.62%] [G loss: 2.345973]\n",
      "785 [D loss: 0.247690, acc.: 90.62%] [G loss: 2.419183]\n",
      "786 [D loss: 0.273101, acc.: 89.06%] [G loss: 2.439349]\n",
      "787 [D loss: 0.263957, acc.: 92.19%] [G loss: 2.340648]\n",
      "788 [D loss: 0.250187, acc.: 90.62%] [G loss: 2.355947]\n",
      "789 [D loss: 0.272052, acc.: 90.62%] [G loss: 2.291584]\n",
      "790 [D loss: 0.233887, acc.: 90.62%] [G loss: 2.322090]\n",
      "791 [D loss: 0.250133, acc.: 90.62%] [G loss: 2.276767]\n",
      "792 [D loss: 0.245813, acc.: 89.06%] [G loss: 2.226317]\n",
      "793 [D loss: 0.245963, acc.: 90.62%] [G loss: 2.024371]\n",
      "794 [D loss: 0.222295, acc.: 90.62%] [G loss: 2.192928]\n",
      "795 [D loss: 0.275154, acc.: 90.62%] [G loss: 2.247244]\n",
      "796 [D loss: 0.240045, acc.: 90.62%] [G loss: 2.277504]\n",
      "797 [D loss: 0.227693, acc.: 90.62%] [G loss: 2.356545]\n",
      "798 [D loss: 0.232305, acc.: 90.62%] [G loss: 2.162050]\n",
      "799 [D loss: 0.226162, acc.: 90.62%] [G loss: 2.130136]\n",
      "800 [D loss: 0.216419, acc.: 90.62%] [G loss: 2.265279]\n",
      "generated_data\n",
      "801 [D loss: 0.215749, acc.: 90.62%] [G loss: 2.138300]\n",
      "802 [D loss: 0.201817, acc.: 92.19%] [G loss: 2.116433]\n",
      "803 [D loss: 0.204608, acc.: 92.19%] [G loss: 2.250413]\n",
      "804 [D loss: 0.211170, acc.: 92.19%] [G loss: 2.198130]\n",
      "805 [D loss: 0.212252, acc.: 92.19%] [G loss: 2.219381]\n",
      "806 [D loss: 0.216503, acc.: 93.75%] [G loss: 2.250463]\n",
      "807 [D loss: 0.215247, acc.: 92.19%] [G loss: 2.141664]\n",
      "808 [D loss: 0.223975, acc.: 93.75%] [G loss: 2.216347]\n",
      "809 [D loss: 0.196506, acc.: 93.75%] [G loss: 2.272003]\n",
      "810 [D loss: 0.225231, acc.: 92.19%] [G loss: 2.349219]\n",
      "811 [D loss: 0.202587, acc.: 92.19%] [G loss: 2.321764]\n",
      "812 [D loss: 0.210814, acc.: 92.19%] [G loss: 2.232279]\n",
      "813 [D loss: 0.204693, acc.: 92.19%] [G loss: 2.185410]\n",
      "814 [D loss: 0.216598, acc.: 93.75%] [G loss: 2.158896]\n",
      "815 [D loss: 0.207639, acc.: 92.19%] [G loss: 2.160021]\n",
      "816 [D loss: 0.184274, acc.: 93.75%] [G loss: 2.306369]\n",
      "817 [D loss: 0.202286, acc.: 92.19%] [G loss: 2.198452]\n",
      "818 [D loss: 0.222173, acc.: 93.75%] [G loss: 2.307678]\n",
      "819 [D loss: 0.210078, acc.: 92.19%] [G loss: 2.180296]\n",
      "820 [D loss: 0.216667, acc.: 93.75%] [G loss: 2.228189]\n",
      "821 [D loss: 0.190928, acc.: 93.75%] [G loss: 2.303584]\n",
      "822 [D loss: 0.212681, acc.: 93.75%] [G loss: 2.294443]\n",
      "823 [D loss: 0.219352, acc.: 92.19%] [G loss: 2.260417]\n",
      "824 [D loss: 0.207045, acc.: 93.75%] [G loss: 2.402574]\n",
      "825 [D loss: 0.232830, acc.: 93.75%] [G loss: 2.023818]\n",
      "826 [D loss: 0.193005, acc.: 93.75%] [G loss: 2.235727]\n",
      "827 [D loss: 0.230980, acc.: 92.19%] [G loss: 2.232604]\n",
      "828 [D loss: 0.258056, acc.: 92.19%] [G loss: 2.499640]\n",
      "829 [D loss: 0.249517, acc.: 90.62%] [G loss: 2.554542]\n",
      "830 [D loss: 0.228506, acc.: 90.62%] [G loss: 2.446208]\n",
      "831 [D loss: 0.247608, acc.: 92.19%] [G loss: 2.352326]\n",
      "832 [D loss: 0.249431, acc.: 92.19%] [G loss: 2.249350]\n",
      "833 [D loss: 0.231540, acc.: 92.19%] [G loss: 2.417036]\n",
      "834 [D loss: 0.228080, acc.: 92.19%] [G loss: 2.230096]\n",
      "835 [D loss: 0.210979, acc.: 90.62%] [G loss: 2.234998]\n",
      "836 [D loss: 0.220187, acc.: 92.19%] [G loss: 2.191008]\n",
      "837 [D loss: 0.224760, acc.: 90.62%] [G loss: 2.323823]\n",
      "838 [D loss: 0.232672, acc.: 90.62%] [G loss: 2.088188]\n",
      "839 [D loss: 0.194603, acc.: 92.19%] [G loss: 2.406638]\n",
      "840 [D loss: 0.205861, acc.: 92.19%] [G loss: 2.398436]\n",
      "841 [D loss: 0.240432, acc.: 90.62%] [G loss: 2.394415]\n",
      "842 [D loss: 0.186889, acc.: 92.19%] [G loss: 2.408641]\n",
      "843 [D loss: 0.213895, acc.: 93.75%] [G loss: 2.452118]\n",
      "844 [D loss: 0.195282, acc.: 92.19%] [G loss: 2.353681]\n",
      "845 [D loss: 0.183082, acc.: 92.19%] [G loss: 2.387933]\n",
      "846 [D loss: 0.203411, acc.: 90.62%] [G loss: 2.308520]\n",
      "847 [D loss: 0.202052, acc.: 92.19%] [G loss: 2.328613]\n",
      "848 [D loss: 0.195573, acc.: 92.19%] [G loss: 2.552749]\n",
      "849 [D loss: 0.191148, acc.: 92.19%] [G loss: 2.445857]\n",
      "850 [D loss: 0.243898, acc.: 90.62%] [G loss: 2.255050]\n",
      "851 [D loss: 0.260729, acc.: 90.62%] [G loss: 2.780955]\n",
      "852 [D loss: 0.207342, acc.: 92.19%] [G loss: 2.695838]\n",
      "853 [D loss: 0.226543, acc.: 92.19%] [G loss: 2.534886]\n",
      "854 [D loss: 0.263623, acc.: 92.19%] [G loss: 2.345594]\n",
      "855 [D loss: 0.210844, acc.: 90.62%] [G loss: 2.366875]\n",
      "856 [D loss: 0.236596, acc.: 92.19%] [G loss: 2.461466]\n",
      "857 [D loss: 0.246928, acc.: 92.19%] [G loss: 2.516774]\n",
      "858 [D loss: 0.219082, acc.: 92.19%] [G loss: 2.456079]\n",
      "859 [D loss: 0.215922, acc.: 90.62%] [G loss: 2.542736]\n",
      "860 [D loss: 0.238863, acc.: 92.19%] [G loss: 2.431538]\n",
      "861 [D loss: 0.219520, acc.: 92.19%] [G loss: 2.400460]\n",
      "862 [D loss: 0.243292, acc.: 90.62%] [G loss: 2.439759]\n",
      "863 [D loss: 0.243905, acc.: 90.62%] [G loss: 2.427940]\n",
      "864 [D loss: 0.229157, acc.: 92.19%] [G loss: 2.187112]\n",
      "865 [D loss: 0.215346, acc.: 90.62%] [G loss: 2.290623]\n",
      "866 [D loss: 0.224738, acc.: 90.62%] [G loss: 2.396065]\n",
      "867 [D loss: 0.238503, acc.: 92.19%] [G loss: 2.315778]\n",
      "868 [D loss: 0.239497, acc.: 90.62%] [G loss: 2.384059]\n",
      "869 [D loss: 0.213018, acc.: 92.19%] [G loss: 2.188871]\n",
      "870 [D loss: 0.231537, acc.: 92.19%] [G loss: 2.160131]\n",
      "871 [D loss: 0.206941, acc.: 92.19%] [G loss: 2.172079]\n",
      "872 [D loss: 0.208683, acc.: 92.19%] [G loss: 2.107573]\n",
      "873 [D loss: 0.203646, acc.: 92.19%] [G loss: 2.039472]\n",
      "874 [D loss: 0.227736, acc.: 92.19%] [G loss: 2.202220]\n",
      "875 [D loss: 0.206645, acc.: 92.19%] [G loss: 2.200627]\n",
      "876 [D loss: 0.215580, acc.: 93.75%] [G loss: 2.283271]\n",
      "877 [D loss: 0.213992, acc.: 92.19%] [G loss: 2.355147]\n",
      "878 [D loss: 0.211168, acc.: 92.19%] [G loss: 2.300888]\n",
      "879 [D loss: 0.202168, acc.: 92.19%] [G loss: 2.374962]\n",
      "880 [D loss: 0.199710, acc.: 92.19%] [G loss: 2.236695]\n",
      "881 [D loss: 0.202816, acc.: 93.75%] [G loss: 2.343503]\n",
      "882 [D loss: 0.196642, acc.: 93.75%] [G loss: 2.260774]\n",
      "883 [D loss: 0.197814, acc.: 93.75%] [G loss: 2.319715]\n",
      "884 [D loss: 0.193053, acc.: 93.75%] [G loss: 2.315694]\n",
      "885 [D loss: 0.203706, acc.: 93.75%] [G loss: 2.341292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "886 [D loss: 0.175320, acc.: 93.75%] [G loss: 2.310656]\n",
      "887 [D loss: 0.165428, acc.: 93.75%] [G loss: 2.277046]\n",
      "888 [D loss: 0.196818, acc.: 93.75%] [G loss: 2.165504]\n",
      "889 [D loss: 0.193248, acc.: 93.75%] [G loss: 2.315818]\n",
      "890 [D loss: 0.188022, acc.: 93.75%] [G loss: 2.422740]\n",
      "891 [D loss: 0.188760, acc.: 93.75%] [G loss: 2.390021]\n",
      "892 [D loss: 0.177039, acc.: 93.75%] [G loss: 2.436870]\n",
      "893 [D loss: 0.176545, acc.: 93.75%] [G loss: 2.357332]\n",
      "894 [D loss: 0.208101, acc.: 93.75%] [G loss: 2.156608]\n",
      "895 [D loss: 0.203865, acc.: 93.75%] [G loss: 2.349258]\n",
      "896 [D loss: 0.177501, acc.: 93.75%] [G loss: 2.352759]\n",
      "897 [D loss: 0.208137, acc.: 93.75%] [G loss: 2.506479]\n",
      "898 [D loss: 0.218363, acc.: 93.75%] [G loss: 2.359844]\n",
      "899 [D loss: 0.218270, acc.: 92.19%] [G loss: 2.524940]\n",
      "900 [D loss: 0.207458, acc.: 93.75%] [G loss: 2.358978]\n",
      "generated_data\n",
      "901 [D loss: 0.200340, acc.: 93.75%] [G loss: 2.503797]\n",
      "902 [D loss: 0.199662, acc.: 93.75%] [G loss: 2.498322]\n",
      "903 [D loss: 0.208241, acc.: 93.75%] [G loss: 2.241127]\n",
      "904 [D loss: 0.196277, acc.: 93.75%] [G loss: 2.424686]\n",
      "905 [D loss: 0.202466, acc.: 93.75%] [G loss: 2.301751]\n",
      "906 [D loss: 0.226555, acc.: 93.75%] [G loss: 2.465258]\n",
      "907 [D loss: 0.212192, acc.: 92.19%] [G loss: 2.230733]\n",
      "908 [D loss: 0.223629, acc.: 92.19%] [G loss: 2.326663]\n",
      "909 [D loss: 0.237178, acc.: 92.19%] [G loss: 2.408459]\n",
      "910 [D loss: 0.302135, acc.: 84.38%] [G loss: 2.605473]\n",
      "911 [D loss: 0.280876, acc.: 89.06%] [G loss: 2.744593]\n",
      "912 [D loss: 0.254833, acc.: 90.62%] [G loss: 2.692379]\n",
      "913 [D loss: 0.246096, acc.: 90.62%] [G loss: 2.567876]\n",
      "914 [D loss: 0.254904, acc.: 90.62%] [G loss: 2.197413]\n",
      "915 [D loss: 0.247678, acc.: 90.62%] [G loss: 2.389842]\n",
      "916 [D loss: 0.233336, acc.: 90.62%] [G loss: 2.313568]\n",
      "917 [D loss: 0.202894, acc.: 90.62%] [G loss: 2.353724]\n",
      "918 [D loss: 0.217174, acc.: 90.62%] [G loss: 2.344180]\n",
      "919 [D loss: 0.236578, acc.: 92.19%] [G loss: 2.252933]\n",
      "920 [D loss: 0.195373, acc.: 90.62%] [G loss: 2.346132]\n",
      "921 [D loss: 0.200950, acc.: 92.19%] [G loss: 2.170650]\n",
      "922 [D loss: 0.202870, acc.: 92.19%] [G loss: 2.357298]\n",
      "923 [D loss: 0.201990, acc.: 93.75%] [G loss: 2.453320]\n",
      "924 [D loss: 0.202679, acc.: 92.19%] [G loss: 2.444542]\n",
      "925 [D loss: 0.207961, acc.: 92.19%] [G loss: 2.415377]\n",
      "926 [D loss: 0.193256, acc.: 90.62%] [G loss: 2.369387]\n",
      "927 [D loss: 0.211500, acc.: 93.75%] [G loss: 2.734005]\n",
      "928 [D loss: 0.204702, acc.: 90.62%] [G loss: 2.352731]\n",
      "929 [D loss: 0.258023, acc.: 90.62%] [G loss: 2.619772]\n",
      "930 [D loss: 0.220488, acc.: 92.19%] [G loss: 2.428730]\n",
      "931 [D loss: 0.215817, acc.: 92.19%] [G loss: 2.532770]\n",
      "932 [D loss: 0.217117, acc.: 90.62%] [G loss: 2.429732]\n",
      "933 [D loss: 0.206568, acc.: 92.19%] [G loss: 2.584729]\n",
      "934 [D loss: 0.211255, acc.: 92.19%] [G loss: 2.502778]\n",
      "935 [D loss: 0.247335, acc.: 92.19%] [G loss: 2.646881]\n",
      "936 [D loss: 0.199945, acc.: 92.19%] [G loss: 2.574916]\n",
      "937 [D loss: 0.260701, acc.: 89.06%] [G loss: 2.636533]\n",
      "938 [D loss: 0.255243, acc.: 90.62%] [G loss: 2.651555]\n",
      "939 [D loss: 0.228963, acc.: 89.06%] [G loss: 2.625295]\n",
      "940 [D loss: 0.236924, acc.: 92.19%] [G loss: 2.458525]\n",
      "941 [D loss: 0.218342, acc.: 90.62%] [G loss: 2.279397]\n",
      "942 [D loss: 0.243174, acc.: 92.19%] [G loss: 2.227763]\n",
      "943 [D loss: 0.215569, acc.: 92.19%] [G loss: 2.236186]\n",
      "944 [D loss: 0.215408, acc.: 92.19%] [G loss: 2.204529]\n",
      "945 [D loss: 0.227846, acc.: 92.19%] [G loss: 2.394018]\n",
      "946 [D loss: 0.214549, acc.: 92.19%] [G loss: 2.348474]\n",
      "947 [D loss: 0.213468, acc.: 92.19%] [G loss: 2.418686]\n",
      "948 [D loss: 0.211587, acc.: 92.19%] [G loss: 2.562572]\n",
      "949 [D loss: 0.216997, acc.: 92.19%] [G loss: 2.249115]\n",
      "950 [D loss: 0.201300, acc.: 92.19%] [G loss: 2.349754]\n",
      "951 [D loss: 0.206360, acc.: 92.19%] [G loss: 2.376104]\n",
      "952 [D loss: 0.206875, acc.: 93.75%] [G loss: 2.333050]\n",
      "953 [D loss: 0.213378, acc.: 93.75%] [G loss: 2.345901]\n",
      "954 [D loss: 0.213401, acc.: 92.19%] [G loss: 2.472815]\n",
      "955 [D loss: 0.208119, acc.: 92.19%] [G loss: 2.303267]\n",
      "956 [D loss: 0.206280, acc.: 93.75%] [G loss: 2.325677]\n",
      "957 [D loss: 0.179037, acc.: 93.75%] [G loss: 2.301491]\n",
      "958 [D loss: 0.189239, acc.: 93.75%] [G loss: 2.261422]\n",
      "959 [D loss: 0.204879, acc.: 92.19%] [G loss: 2.235994]\n",
      "960 [D loss: 0.213043, acc.: 93.75%] [G loss: 2.233386]\n",
      "961 [D loss: 0.200241, acc.: 93.75%] [G loss: 2.238991]\n",
      "962 [D loss: 0.189741, acc.: 92.19%] [G loss: 2.391619]\n",
      "963 [D loss: 0.179616, acc.: 93.75%] [G loss: 2.291924]\n",
      "964 [D loss: 0.192315, acc.: 93.75%] [G loss: 2.263494]\n",
      "965 [D loss: 0.193784, acc.: 93.75%] [G loss: 2.360261]\n",
      "966 [D loss: 0.208263, acc.: 93.75%] [G loss: 2.367923]\n",
      "967 [D loss: 0.198508, acc.: 92.19%] [G loss: 2.438576]\n",
      "968 [D loss: 0.226513, acc.: 90.62%] [G loss: 2.495982]\n",
      "969 [D loss: 0.210380, acc.: 93.75%] [G loss: 2.775337]\n",
      "970 [D loss: 0.212230, acc.: 93.75%] [G loss: 2.638442]\n",
      "971 [D loss: 0.193352, acc.: 92.19%] [G loss: 2.596247]\n",
      "972 [D loss: 0.206959, acc.: 90.62%] [G loss: 2.579807]\n",
      "973 [D loss: 0.198526, acc.: 92.19%] [G loss: 2.630452]\n",
      "974 [D loss: 0.200143, acc.: 92.19%] [G loss: 2.805712]\n",
      "975 [D loss: 0.208361, acc.: 90.62%] [G loss: 2.467067]\n",
      "976 [D loss: 0.202256, acc.: 92.19%] [G loss: 2.459165]\n",
      "977 [D loss: 0.227437, acc.: 90.62%] [G loss: 2.522827]\n",
      "978 [D loss: 0.234062, acc.: 92.19%] [G loss: 2.549857]\n",
      "979 [D loss: 0.239702, acc.: 89.06%] [G loss: 2.275873]\n",
      "980 [D loss: 0.237779, acc.: 92.19%] [G loss: 2.699143]\n",
      "981 [D loss: 0.207635, acc.: 92.19%] [G loss: 2.693620]\n",
      "982 [D loss: 0.180045, acc.: 92.19%] [G loss: 2.584980]\n",
      "983 [D loss: 0.214152, acc.: 92.19%] [G loss: 2.445553]\n",
      "984 [D loss: 0.330928, acc.: 87.50%] [G loss: 2.708031]\n",
      "985 [D loss: 0.255449, acc.: 90.62%] [G loss: 2.735110]\n",
      "986 [D loss: 0.273140, acc.: 93.75%] [G loss: 2.821727]\n",
      "987 [D loss: 0.248274, acc.: 89.06%] [G loss: 2.665285]\n",
      "988 [D loss: 0.240831, acc.: 92.19%] [G loss: 2.408412]\n",
      "989 [D loss: 0.222268, acc.: 92.19%] [G loss: 2.496187]\n",
      "990 [D loss: 0.240674, acc.: 89.06%] [G loss: 2.576956]\n",
      "991 [D loss: 0.275073, acc.: 87.50%] [G loss: 2.491904]\n",
      "992 [D loss: 0.297348, acc.: 89.06%] [G loss: 2.517938]\n",
      "993 [D loss: 0.287516, acc.: 90.62%] [G loss: 2.470967]\n",
      "994 [D loss: 0.248705, acc.: 90.62%] [G loss: 2.857763]\n",
      "995 [D loss: 0.305606, acc.: 90.62%] [G loss: 2.475825]\n",
      "996 [D loss: 0.230434, acc.: 92.19%] [G loss: 2.361596]\n",
      "997 [D loss: 0.298740, acc.: 82.81%] [G loss: 2.478816]\n",
      "998 [D loss: 0.226609, acc.: 90.62%] [G loss: 2.682500]\n",
      "999 [D loss: 0.300840, acc.: 89.06%] [G loss: 2.656751]\n",
      "1000 [D loss: 0.256529, acc.: 89.06%] [G loss: 2.370330]\n",
      "generated_data\n",
      "1001 [D loss: 0.267343, acc.: 90.62%] [G loss: 2.406951]\n",
      "1002 [D loss: 0.309265, acc.: 84.38%] [G loss: 2.257048]\n",
      "1003 [D loss: 0.281875, acc.: 87.50%] [G loss: 2.702149]\n",
      "1004 [D loss: 0.280296, acc.: 89.06%] [G loss: 2.526664]\n",
      "1005 [D loss: 0.250417, acc.: 89.06%] [G loss: 2.801235]\n",
      "1006 [D loss: 0.264519, acc.: 89.06%] [G loss: 2.332815]\n",
      "1007 [D loss: 0.261016, acc.: 85.94%] [G loss: 2.519856]\n",
      "1008 [D loss: 0.289619, acc.: 85.94%] [G loss: 2.600143]\n",
      "1009 [D loss: 0.225804, acc.: 89.06%] [G loss: 2.454119]\n",
      "1010 [D loss: 0.222510, acc.: 89.06%] [G loss: 2.458411]\n",
      "1011 [D loss: 0.263602, acc.: 89.06%] [G loss: 2.652319]\n",
      "1012 [D loss: 0.253028, acc.: 90.62%] [G loss: 3.002118]\n",
      "1013 [D loss: 0.225929, acc.: 92.19%] [G loss: 2.891593]\n",
      "1014 [D loss: 0.203809, acc.: 92.19%] [G loss: 2.617435]\n",
      "1015 [D loss: 0.276449, acc.: 90.62%] [G loss: 2.781133]\n",
      "1016 [D loss: 0.202668, acc.: 90.62%] [G loss: 2.738001]\n",
      "1017 [D loss: 0.200966, acc.: 93.75%] [G loss: 2.459709]\n",
      "1018 [D loss: 0.226767, acc.: 93.75%] [G loss: 2.594139]\n",
      "1019 [D loss: 0.198371, acc.: 93.75%] [G loss: 2.276180]\n",
      "1020 [D loss: 0.200915, acc.: 90.62%] [G loss: 2.537659]\n",
      "1021 [D loss: 0.194526, acc.: 92.19%] [G loss: 2.581752]\n",
      "1022 [D loss: 0.158178, acc.: 93.75%] [G loss: 2.633854]\n",
      "1023 [D loss: 0.171282, acc.: 95.31%] [G loss: 2.617992]\n",
      "1024 [D loss: 0.163775, acc.: 93.75%] [G loss: 2.639173]\n",
      "1025 [D loss: 0.159144, acc.: 95.31%] [G loss: 2.564970]\n",
      "1026 [D loss: 0.169932, acc.: 93.75%] [G loss: 2.555732]\n",
      "1027 [D loss: 0.173027, acc.: 93.75%] [G loss: 2.459450]\n",
      "1028 [D loss: 0.216595, acc.: 93.75%] [G loss: 2.486429]\n",
      "1029 [D loss: 0.212445, acc.: 95.31%] [G loss: 2.513760]\n",
      "1030 [D loss: 0.176931, acc.: 93.75%] [G loss: 2.705474]\n",
      "1031 [D loss: 0.164352, acc.: 95.31%] [G loss: 2.795465]\n",
      "1032 [D loss: 0.183299, acc.: 93.75%] [G loss: 2.744461]\n",
      "1033 [D loss: 0.210993, acc.: 93.75%] [G loss: 2.835141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1034 [D loss: 0.164394, acc.: 93.75%] [G loss: 2.722063]\n",
      "1035 [D loss: 0.179378, acc.: 93.75%] [G loss: 2.690787]\n",
      "1036 [D loss: 0.162944, acc.: 93.75%] [G loss: 2.473814]\n",
      "1037 [D loss: 0.162303, acc.: 93.75%] [G loss: 2.453058]\n",
      "1038 [D loss: 0.157996, acc.: 95.31%] [G loss: 2.488454]\n",
      "1039 [D loss: 0.181111, acc.: 95.31%] [G loss: 2.624316]\n",
      "1040 [D loss: 0.188075, acc.: 93.75%] [G loss: 2.615147]\n",
      "1041 [D loss: 0.171770, acc.: 93.75%] [G loss: 2.913820]\n",
      "1042 [D loss: 0.152221, acc.: 95.31%] [G loss: 2.733790]\n",
      "1043 [D loss: 0.193149, acc.: 93.75%] [G loss: 2.671406]\n",
      "1044 [D loss: 0.170937, acc.: 95.31%] [G loss: 2.618585]\n",
      "1045 [D loss: 0.176352, acc.: 93.75%] [G loss: 2.617159]\n",
      "1046 [D loss: 0.188356, acc.: 95.31%] [G loss: 2.546430]\n",
      "1047 [D loss: 0.171422, acc.: 95.31%] [G loss: 2.470323]\n",
      "1048 [D loss: 0.182068, acc.: 93.75%] [G loss: 2.766337]\n",
      "1049 [D loss: 0.201243, acc.: 93.75%] [G loss: 2.823062]\n",
      "1050 [D loss: 0.162505, acc.: 95.31%] [G loss: 2.923043]\n",
      "1051 [D loss: 0.176826, acc.: 93.75%] [G loss: 2.497815]\n",
      "1052 [D loss: 0.145626, acc.: 93.75%] [G loss: 2.668704]\n",
      "1053 [D loss: 0.165065, acc.: 93.75%] [G loss: 2.567889]\n",
      "1054 [D loss: 0.165052, acc.: 93.75%] [G loss: 2.740304]\n",
      "1055 [D loss: 0.177255, acc.: 93.75%] [G loss: 2.814747]\n",
      "1056 [D loss: 0.150152, acc.: 93.75%] [G loss: 2.706311]\n",
      "1057 [D loss: 0.197075, acc.: 93.75%] [G loss: 2.773548]\n",
      "1058 [D loss: 0.161092, acc.: 93.75%] [G loss: 2.786211]\n",
      "1059 [D loss: 0.175678, acc.: 93.75%] [G loss: 2.751588]\n",
      "1060 [D loss: 0.152614, acc.: 93.75%] [G loss: 2.740121]\n",
      "1061 [D loss: 0.157944, acc.: 93.75%] [G loss: 2.655253]\n",
      "1062 [D loss: 0.158355, acc.: 93.75%] [G loss: 2.770846]\n",
      "1063 [D loss: 0.135303, acc.: 93.75%] [G loss: 3.050021]\n",
      "1064 [D loss: 0.175202, acc.: 93.75%] [G loss: 2.829350]\n",
      "1065 [D loss: 0.193355, acc.: 93.75%] [G loss: 2.784273]\n",
      "1066 [D loss: 0.173815, acc.: 92.19%] [G loss: 2.737915]\n",
      "1067 [D loss: 0.147739, acc.: 93.75%] [G loss: 2.666565]\n",
      "1068 [D loss: 0.172332, acc.: 93.75%] [G loss: 2.785705]\n",
      "1069 [D loss: 0.221094, acc.: 93.75%] [G loss: 2.716885]\n",
      "1070 [D loss: 0.192543, acc.: 93.75%] [G loss: 2.946839]\n",
      "1071 [D loss: 0.180163, acc.: 93.75%] [G loss: 2.748484]\n",
      "1072 [D loss: 0.188826, acc.: 93.75%] [G loss: 2.704183]\n",
      "1073 [D loss: 0.177673, acc.: 93.75%] [G loss: 2.661957]\n",
      "1074 [D loss: 0.181761, acc.: 92.19%] [G loss: 2.813852]\n",
      "1075 [D loss: 0.181422, acc.: 93.75%] [G loss: 2.575187]\n",
      "1076 [D loss: 0.221233, acc.: 92.19%] [G loss: 2.837801]\n",
      "1077 [D loss: 0.187271, acc.: 93.75%] [G loss: 2.752980]\n",
      "1078 [D loss: 0.186189, acc.: 93.75%] [G loss: 2.694425]\n",
      "1079 [D loss: 0.191527, acc.: 93.75%] [G loss: 2.810087]\n",
      "1080 [D loss: 0.176770, acc.: 93.75%] [G loss: 2.666280]\n",
      "1081 [D loss: 0.184965, acc.: 93.75%] [G loss: 2.713216]\n",
      "1082 [D loss: 0.176052, acc.: 92.19%] [G loss: 2.818394]\n",
      "1083 [D loss: 0.223686, acc.: 93.75%] [G loss: 2.751038]\n",
      "1084 [D loss: 0.166387, acc.: 93.75%] [G loss: 2.775124]\n",
      "1085 [D loss: 0.184284, acc.: 93.75%] [G loss: 2.546668]\n",
      "1086 [D loss: 0.193656, acc.: 93.75%] [G loss: 2.723114]\n",
      "1087 [D loss: 0.178501, acc.: 93.75%] [G loss: 2.769814]\n",
      "1088 [D loss: 0.162508, acc.: 93.75%] [G loss: 3.122657]\n",
      "1089 [D loss: 0.162190, acc.: 93.75%] [G loss: 2.983988]\n",
      "1090 [D loss: 0.186384, acc.: 93.75%] [G loss: 2.727144]\n",
      "1091 [D loss: 0.186678, acc.: 93.75%] [G loss: 2.608628]\n",
      "1092 [D loss: 0.201799, acc.: 93.75%] [G loss: 2.983741]\n",
      "1093 [D loss: 0.169074, acc.: 93.75%] [G loss: 2.595737]\n",
      "1094 [D loss: 0.175730, acc.: 93.75%] [G loss: 2.361729]\n",
      "1095 [D loss: 0.150901, acc.: 93.75%] [G loss: 2.783895]\n",
      "1096 [D loss: 0.189879, acc.: 90.62%] [G loss: 3.064218]\n",
      "1097 [D loss: 0.159015, acc.: 93.75%] [G loss: 2.842947]\n",
      "1098 [D loss: 0.159747, acc.: 93.75%] [G loss: 2.569654]\n",
      "1099 [D loss: 0.160151, acc.: 93.75%] [G loss: 2.775338]\n",
      "1100 [D loss: 0.181343, acc.: 93.75%] [G loss: 2.622968]\n",
      "generated_data\n",
      "1101 [D loss: 0.137673, acc.: 93.75%] [G loss: 2.821734]\n",
      "1102 [D loss: 0.168783, acc.: 93.75%] [G loss: 2.803140]\n",
      "1103 [D loss: 0.166731, acc.: 93.75%] [G loss: 2.791383]\n",
      "1104 [D loss: 0.181094, acc.: 93.75%] [G loss: 2.536201]\n",
      "1105 [D loss: 0.166604, acc.: 93.75%] [G loss: 2.859092]\n",
      "1106 [D loss: 0.174954, acc.: 93.75%] [G loss: 2.533554]\n",
      "1107 [D loss: 0.157186, acc.: 93.75%] [G loss: 2.718512]\n",
      "1108 [D loss: 0.171117, acc.: 93.75%] [G loss: 2.919542]\n",
      "1109 [D loss: 0.138893, acc.: 93.75%] [G loss: 2.556096]\n",
      "1110 [D loss: 0.223746, acc.: 92.19%] [G loss: 2.879998]\n",
      "1111 [D loss: 0.144873, acc.: 93.75%] [G loss: 2.845360]\n",
      "1112 [D loss: 0.175327, acc.: 93.75%] [G loss: 2.431271]\n",
      "1113 [D loss: 0.342332, acc.: 87.50%] [G loss: 2.763226]\n",
      "1114 [D loss: 0.274986, acc.: 90.62%] [G loss: 3.172945]\n",
      "1115 [D loss: 0.215727, acc.: 90.62%] [G loss: 2.753621]\n",
      "1116 [D loss: 0.267992, acc.: 89.06%] [G loss: 2.701777]\n",
      "1117 [D loss: 0.229108, acc.: 92.19%] [G loss: 2.533990]\n",
      "1118 [D loss: 0.262237, acc.: 89.06%] [G loss: 2.707725]\n",
      "1119 [D loss: 0.232594, acc.: 92.19%] [G loss: 2.457339]\n",
      "1120 [D loss: 0.283392, acc.: 85.94%] [G loss: 2.720908]\n",
      "1121 [D loss: 0.253275, acc.: 90.62%] [G loss: 2.749735]\n",
      "1122 [D loss: 0.322754, acc.: 85.94%] [G loss: 2.620957]\n",
      "1123 [D loss: 0.260882, acc.: 89.06%] [G loss: 2.678052]\n",
      "1124 [D loss: 0.250689, acc.: 89.06%] [G loss: 2.468992]\n",
      "1125 [D loss: 0.349282, acc.: 84.38%] [G loss: 2.748303]\n",
      "1126 [D loss: 0.272158, acc.: 87.50%] [G loss: 2.981134]\n",
      "1127 [D loss: 0.248318, acc.: 89.06%] [G loss: 2.697900]\n",
      "1128 [D loss: 0.237715, acc.: 90.62%] [G loss: 2.558163]\n",
      "1129 [D loss: 0.270098, acc.: 90.62%] [G loss: 2.661929]\n",
      "1130 [D loss: 0.238315, acc.: 90.62%] [G loss: 2.600596]\n",
      "1131 [D loss: 0.216549, acc.: 92.19%] [G loss: 2.659852]\n",
      "1132 [D loss: 0.208402, acc.: 93.75%] [G loss: 2.842181]\n",
      "1133 [D loss: 0.225014, acc.: 92.19%] [G loss: 2.744244]\n",
      "1134 [D loss: 0.208808, acc.: 90.62%] [G loss: 2.798330]\n",
      "1135 [D loss: 0.211917, acc.: 90.62%] [G loss: 2.872385]\n",
      "1136 [D loss: 0.204611, acc.: 93.75%] [G loss: 2.660511]\n",
      "1137 [D loss: 0.209506, acc.: 90.62%] [G loss: 2.543696]\n",
      "1138 [D loss: 0.239350, acc.: 89.06%] [G loss: 2.505563]\n",
      "1139 [D loss: 0.229839, acc.: 90.62%] [G loss: 2.431906]\n",
      "1140 [D loss: 0.174214, acc.: 92.19%] [G loss: 2.546156]\n",
      "1141 [D loss: 0.219532, acc.: 92.19%] [G loss: 2.490054]\n",
      "1142 [D loss: 0.210883, acc.: 90.62%] [G loss: 2.627228]\n",
      "1143 [D loss: 0.215043, acc.: 90.62%] [G loss: 2.301529]\n",
      "1144 [D loss: 0.213306, acc.: 92.19%] [G loss: 2.151369]\n",
      "1145 [D loss: 0.258151, acc.: 89.06%] [G loss: 2.406302]\n",
      "1146 [D loss: 0.242453, acc.: 85.94%] [G loss: 2.868154]\n",
      "1147 [D loss: 0.204011, acc.: 89.06%] [G loss: 2.602979]\n",
      "1148 [D loss: 0.223512, acc.: 90.62%] [G loss: 2.810239]\n",
      "1149 [D loss: 0.212644, acc.: 90.62%] [G loss: 2.474345]\n",
      "1150 [D loss: 0.172042, acc.: 90.62%] [G loss: 2.256911]\n",
      "1151 [D loss: 0.183442, acc.: 93.75%] [G loss: 2.508928]\n",
      "1152 [D loss: 0.197405, acc.: 92.19%] [G loss: 2.584620]\n",
      "1153 [D loss: 0.192259, acc.: 90.62%] [G loss: 2.635922]\n",
      "1154 [D loss: 0.209916, acc.: 90.62%] [G loss: 2.706297]\n",
      "1155 [D loss: 0.204015, acc.: 90.62%] [G loss: 2.488435]\n",
      "1156 [D loss: 0.205453, acc.: 93.75%] [G loss: 2.741356]\n",
      "1157 [D loss: 0.164039, acc.: 92.19%] [G loss: 2.885755]\n",
      "1158 [D loss: 0.210277, acc.: 93.75%] [G loss: 2.453227]\n",
      "1159 [D loss: 0.203322, acc.: 92.19%] [G loss: 2.621351]\n",
      "1160 [D loss: 0.203529, acc.: 92.19%] [G loss: 2.392270]\n",
      "1161 [D loss: 0.238361, acc.: 90.62%] [G loss: 2.722548]\n",
      "1162 [D loss: 0.214873, acc.: 92.19%] [G loss: 2.909579]\n",
      "1163 [D loss: 0.244309, acc.: 89.06%] [G loss: 2.503380]\n",
      "1164 [D loss: 0.224346, acc.: 87.50%] [G loss: 2.504527]\n",
      "1165 [D loss: 0.211807, acc.: 92.19%] [G loss: 2.866527]\n",
      "1166 [D loss: 0.212573, acc.: 89.06%] [G loss: 2.856492]\n",
      "1167 [D loss: 0.209489, acc.: 92.19%] [G loss: 2.383421]\n",
      "1168 [D loss: 0.247250, acc.: 90.62%] [G loss: 2.460444]\n",
      "1169 [D loss: 0.244172, acc.: 89.06%] [G loss: 2.554024]\n",
      "1170 [D loss: 0.224564, acc.: 90.62%] [G loss: 2.594515]\n",
      "1171 [D loss: 0.221191, acc.: 90.62%] [G loss: 2.420130]\n",
      "1172 [D loss: 0.243058, acc.: 87.50%] [G loss: 2.713069]\n",
      "1173 [D loss: 0.214150, acc.: 92.19%] [G loss: 2.377120]\n",
      "1174 [D loss: 0.249171, acc.: 90.62%] [G loss: 2.735473]\n",
      "1175 [D loss: 0.218054, acc.: 89.06%] [G loss: 2.680176]\n",
      "1176 [D loss: 0.239888, acc.: 93.75%] [G loss: 2.682078]\n",
      "1177 [D loss: 0.230545, acc.: 89.06%] [G loss: 2.676075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1178 [D loss: 0.223865, acc.: 90.62%] [G loss: 2.532567]\n",
      "1179 [D loss: 0.268841, acc.: 87.50%] [G loss: 2.720349]\n",
      "1180 [D loss: 0.228063, acc.: 90.62%] [G loss: 2.597700]\n",
      "1181 [D loss: 0.252014, acc.: 87.50%] [G loss: 2.559086]\n",
      "1182 [D loss: 0.189762, acc.: 92.19%] [G loss: 2.270820]\n",
      "1183 [D loss: 0.196964, acc.: 90.62%] [G loss: 2.128003]\n",
      "1184 [D loss: 0.217476, acc.: 93.75%] [G loss: 2.330824]\n",
      "1185 [D loss: 0.235786, acc.: 90.62%] [G loss: 2.562990]\n",
      "1186 [D loss: 0.218574, acc.: 89.06%] [G loss: 2.382711]\n",
      "1187 [D loss: 0.208428, acc.: 90.62%] [G loss: 2.281678]\n",
      "1188 [D loss: 0.233446, acc.: 89.06%] [G loss: 2.547001]\n",
      "1189 [D loss: 0.214339, acc.: 89.06%] [G loss: 2.415685]\n",
      "1190 [D loss: 0.212629, acc.: 92.19%] [G loss: 2.510419]\n",
      "1191 [D loss: 0.222733, acc.: 89.06%] [G loss: 2.599048]\n",
      "1192 [D loss: 0.209907, acc.: 92.19%] [G loss: 2.683446]\n",
      "1193 [D loss: 0.188252, acc.: 93.75%] [G loss: 2.493608]\n",
      "1194 [D loss: 0.255988, acc.: 87.50%] [G loss: 2.831598]\n",
      "1195 [D loss: 0.240626, acc.: 90.62%] [G loss: 2.457629]\n",
      "1196 [D loss: 0.239259, acc.: 89.06%] [G loss: 2.479388]\n",
      "1197 [D loss: 0.271163, acc.: 85.94%] [G loss: 2.525698]\n",
      "1198 [D loss: 0.223927, acc.: 89.06%] [G loss: 2.565331]\n",
      "1199 [D loss: 0.254952, acc.: 89.06%] [G loss: 2.313294]\n",
      "1200 [D loss: 0.248080, acc.: 89.06%] [G loss: 2.286563]\n",
      "generated_data\n",
      "1201 [D loss: 0.239976, acc.: 89.06%] [G loss: 2.663184]\n",
      "1202 [D loss: 0.232191, acc.: 87.50%] [G loss: 2.665236]\n",
      "1203 [D loss: 0.230613, acc.: 90.62%] [G loss: 2.532545]\n",
      "1204 [D loss: 0.230944, acc.: 89.06%] [G loss: 2.326535]\n",
      "1205 [D loss: 0.183623, acc.: 90.62%] [G loss: 2.538886]\n",
      "1206 [D loss: 0.216842, acc.: 87.50%] [G loss: 2.393803]\n",
      "1207 [D loss: 0.214172, acc.: 92.19%] [G loss: 2.589539]\n",
      "1208 [D loss: 0.180529, acc.: 90.62%] [G loss: 2.621104]\n",
      "1209 [D loss: 0.214192, acc.: 90.62%] [G loss: 2.389276]\n",
      "1210 [D loss: 0.191522, acc.: 89.06%] [G loss: 2.728571]\n",
      "1211 [D loss: 0.243087, acc.: 90.62%] [G loss: 3.051350]\n",
      "1212 [D loss: 0.206130, acc.: 89.06%] [G loss: 2.662675]\n",
      "1213 [D loss: 0.222094, acc.: 89.06%] [G loss: 2.512441]\n",
      "1214 [D loss: 0.237328, acc.: 92.19%] [G loss: 2.575724]\n",
      "1215 [D loss: 0.229970, acc.: 90.62%] [G loss: 2.605134]\n",
      "1216 [D loss: 0.208256, acc.: 90.62%] [G loss: 2.535198]\n",
      "1217 [D loss: 0.241043, acc.: 92.19%] [G loss: 2.310891]\n",
      "1218 [D loss: 0.205617, acc.: 92.19%] [G loss: 2.321840]\n",
      "1219 [D loss: 0.232386, acc.: 89.06%] [G loss: 2.621433]\n",
      "1220 [D loss: 0.193947, acc.: 89.06%] [G loss: 2.733944]\n",
      "1221 [D loss: 0.218586, acc.: 89.06%] [G loss: 2.448810]\n",
      "1222 [D loss: 0.206843, acc.: 92.19%] [G loss: 2.659757]\n",
      "1223 [D loss: 0.236771, acc.: 87.50%] [G loss: 2.837234]\n",
      "1224 [D loss: 0.174579, acc.: 90.62%] [G loss: 2.830629]\n",
      "1225 [D loss: 0.218793, acc.: 89.06%] [G loss: 2.742624]\n",
      "1226 [D loss: 0.248057, acc.: 87.50%] [G loss: 2.460420]\n",
      "1227 [D loss: 0.231834, acc.: 89.06%] [G loss: 2.799909]\n",
      "1228 [D loss: 0.229358, acc.: 89.06%] [G loss: 2.936739]\n",
      "1229 [D loss: 0.180225, acc.: 92.19%] [G loss: 2.764484]\n",
      "1230 [D loss: 0.231385, acc.: 87.50%] [G loss: 2.893202]\n",
      "1231 [D loss: 0.218702, acc.: 89.06%] [G loss: 2.427701]\n",
      "1232 [D loss: 0.187508, acc.: 93.75%] [G loss: 2.705016]\n",
      "1233 [D loss: 0.229972, acc.: 89.06%] [G loss: 2.812106]\n",
      "1234 [D loss: 0.225291, acc.: 92.19%] [G loss: 2.588323]\n",
      "1235 [D loss: 0.192717, acc.: 95.31%] [G loss: 2.744054]\n",
      "1236 [D loss: 0.246147, acc.: 89.06%] [G loss: 2.786742]\n",
      "1237 [D loss: 0.202588, acc.: 90.62%] [G loss: 2.955116]\n",
      "1238 [D loss: 0.217030, acc.: 90.62%] [G loss: 2.279397]\n",
      "1239 [D loss: 0.210495, acc.: 90.62%] [G loss: 2.291717]\n",
      "1240 [D loss: 0.169783, acc.: 93.75%] [G loss: 2.634809]\n",
      "1241 [D loss: 0.184037, acc.: 92.19%] [G loss: 2.760222]\n",
      "1242 [D loss: 0.243487, acc.: 90.62%] [G loss: 2.189892]\n",
      "1243 [D loss: 0.231485, acc.: 89.06%] [G loss: 2.500904]\n",
      "1244 [D loss: 0.192883, acc.: 92.19%] [G loss: 2.443413]\n",
      "1245 [D loss: 0.231251, acc.: 87.50%] [G loss: 2.742327]\n",
      "1246 [D loss: 0.209055, acc.: 89.06%] [G loss: 2.955041]\n",
      "1247 [D loss: 0.187467, acc.: 92.19%] [G loss: 2.431962]\n",
      "1248 [D loss: 0.257596, acc.: 87.50%] [G loss: 2.475041]\n",
      "1249 [D loss: 0.221656, acc.: 90.62%] [G loss: 2.612376]\n",
      "1250 [D loss: 0.244147, acc.: 89.06%] [G loss: 2.508418]\n",
      "1251 [D loss: 0.267818, acc.: 87.50%] [G loss: 2.776376]\n",
      "1252 [D loss: 0.184363, acc.: 92.19%] [G loss: 2.669207]\n",
      "1253 [D loss: 0.248564, acc.: 89.06%] [G loss: 2.628520]\n",
      "1254 [D loss: 0.250659, acc.: 90.62%] [G loss: 2.562700]\n",
      "1255 [D loss: 0.272688, acc.: 84.38%] [G loss: 2.421773]\n",
      "1256 [D loss: 0.195734, acc.: 89.06%] [G loss: 2.580555]\n",
      "1257 [D loss: 0.244466, acc.: 85.94%] [G loss: 2.610545]\n",
      "1258 [D loss: 0.228827, acc.: 89.06%] [G loss: 2.663950]\n",
      "1259 [D loss: 0.215343, acc.: 90.62%] [G loss: 2.369670]\n",
      "1260 [D loss: 0.179049, acc.: 90.62%] [G loss: 2.438805]\n",
      "1261 [D loss: 0.226776, acc.: 87.50%] [G loss: 2.416002]\n",
      "1262 [D loss: 0.172196, acc.: 96.88%] [G loss: 2.456724]\n",
      "1263 [D loss: 0.210417, acc.: 90.62%] [G loss: 2.585930]\n",
      "1264 [D loss: 0.232605, acc.: 90.62%] [G loss: 2.847405]\n",
      "1265 [D loss: 0.175448, acc.: 90.62%] [G loss: 2.627325]\n",
      "1266 [D loss: 0.193702, acc.: 90.62%] [G loss: 2.812953]\n",
      "1267 [D loss: 0.218842, acc.: 89.06%] [G loss: 3.067279]\n",
      "1268 [D loss: 0.216925, acc.: 90.62%] [G loss: 2.796632]\n",
      "1269 [D loss: 0.224114, acc.: 93.75%] [G loss: 2.718014]\n",
      "1270 [D loss: 0.281569, acc.: 89.06%] [G loss: 2.734810]\n",
      "1271 [D loss: 0.191698, acc.: 90.62%] [G loss: 2.493211]\n",
      "1272 [D loss: 0.211791, acc.: 92.19%] [G loss: 2.586963]\n",
      "1273 [D loss: 0.212174, acc.: 90.62%] [G loss: 2.650409]\n",
      "1274 [D loss: 0.206938, acc.: 89.06%] [G loss: 2.442578]\n",
      "1275 [D loss: 0.227935, acc.: 90.62%] [G loss: 2.812177]\n",
      "1276 [D loss: 0.233019, acc.: 90.62%] [G loss: 2.906808]\n",
      "1277 [D loss: 0.224260, acc.: 90.62%] [G loss: 2.649524]\n",
      "1278 [D loss: 0.207258, acc.: 90.62%] [G loss: 2.752137]\n",
      "1279 [D loss: 0.237540, acc.: 87.50%] [G loss: 2.450918]\n",
      "1280 [D loss: 0.199788, acc.: 89.06%] [G loss: 2.915560]\n",
      "1281 [D loss: 0.212888, acc.: 90.62%] [G loss: 2.532635]\n",
      "1282 [D loss: 0.228267, acc.: 92.19%] [G loss: 2.625189]\n",
      "1283 [D loss: 0.286842, acc.: 85.94%] [G loss: 2.859951]\n",
      "1284 [D loss: 0.219026, acc.: 90.62%] [G loss: 2.795234]\n",
      "1285 [D loss: 0.191786, acc.: 89.06%] [G loss: 2.516066]\n",
      "1286 [D loss: 0.206779, acc.: 92.19%] [G loss: 2.638855]\n",
      "1287 [D loss: 0.202318, acc.: 92.19%] [G loss: 2.490360]\n",
      "1288 [D loss: 0.237755, acc.: 89.06%] [G loss: 2.872094]\n",
      "1289 [D loss: 0.231933, acc.: 92.19%] [G loss: 2.764375]\n",
      "1290 [D loss: 0.236575, acc.: 89.06%] [G loss: 2.423172]\n",
      "1291 [D loss: 0.205589, acc.: 89.06%] [G loss: 3.061169]\n",
      "1292 [D loss: 0.221592, acc.: 87.50%] [G loss: 2.773581]\n",
      "1293 [D loss: 0.222721, acc.: 92.19%] [G loss: 2.855469]\n",
      "1294 [D loss: 0.183926, acc.: 92.19%] [G loss: 2.513474]\n",
      "1295 [D loss: 0.227172, acc.: 90.62%] [G loss: 2.584448]\n",
      "1296 [D loss: 0.284751, acc.: 89.06%] [G loss: 2.714524]\n",
      "1297 [D loss: 0.220179, acc.: 89.06%] [G loss: 2.747375]\n",
      "1298 [D loss: 0.181275, acc.: 93.75%] [G loss: 2.564252]\n",
      "1299 [D loss: 0.193944, acc.: 90.62%] [G loss: 2.651956]\n",
      "1300 [D loss: 0.245408, acc.: 90.62%] [G loss: 2.455725]\n",
      "generated_data\n",
      "1301 [D loss: 0.201274, acc.: 90.62%] [G loss: 2.760226]\n",
      "1302 [D loss: 0.222449, acc.: 89.06%] [G loss: 2.511258]\n",
      "1303 [D loss: 0.258234, acc.: 85.94%] [G loss: 2.676700]\n",
      "1304 [D loss: 0.193781, acc.: 89.06%] [G loss: 2.753505]\n",
      "1305 [D loss: 0.269787, acc.: 85.94%] [G loss: 3.096724]\n",
      "1306 [D loss: 0.208930, acc.: 89.06%] [G loss: 3.047079]\n",
      "1307 [D loss: 0.209936, acc.: 90.62%] [G loss: 2.645755]\n",
      "1308 [D loss: 0.207139, acc.: 89.06%] [G loss: 2.664420]\n",
      "1309 [D loss: 0.201033, acc.: 92.19%] [G loss: 2.694550]\n",
      "1310 [D loss: 0.187454, acc.: 92.19%] [G loss: 2.339931]\n",
      "1311 [D loss: 0.176195, acc.: 95.31%] [G loss: 2.404364]\n",
      "1312 [D loss: 0.153059, acc.: 95.31%] [G loss: 2.362525]\n",
      "1313 [D loss: 0.217464, acc.: 89.06%] [G loss: 2.997034]\n",
      "1314 [D loss: 0.240077, acc.: 89.06%] [G loss: 2.940159]\n",
      "1315 [D loss: 0.226419, acc.: 87.50%] [G loss: 3.017436]\n",
      "1316 [D loss: 0.228474, acc.: 90.62%] [G loss: 2.329590]\n",
      "1317 [D loss: 0.237117, acc.: 89.06%] [G loss: 2.370885]\n",
      "1318 [D loss: 0.242740, acc.: 89.06%] [G loss: 3.195765]\n",
      "1319 [D loss: 0.216601, acc.: 92.19%] [G loss: 3.031166]\n",
      "1320 [D loss: 0.226597, acc.: 87.50%] [G loss: 2.556560]\n",
      "1321 [D loss: 0.223538, acc.: 90.62%] [G loss: 2.520175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1322 [D loss: 0.224503, acc.: 90.62%] [G loss: 2.584700]\n",
      "1323 [D loss: 0.222750, acc.: 90.62%] [G loss: 2.532394]\n",
      "1324 [D loss: 0.212143, acc.: 90.62%] [G loss: 2.514611]\n",
      "1325 [D loss: 0.198345, acc.: 93.75%] [G loss: 2.751316]\n",
      "1326 [D loss: 0.262623, acc.: 85.94%] [G loss: 2.836352]\n",
      "1327 [D loss: 0.236169, acc.: 90.62%] [G loss: 2.587216]\n",
      "1328 [D loss: 0.217604, acc.: 87.50%] [G loss: 2.728163]\n",
      "1329 [D loss: 0.259680, acc.: 85.94%] [G loss: 2.689225]\n",
      "1330 [D loss: 0.207435, acc.: 89.06%] [G loss: 2.576393]\n",
      "1331 [D loss: 0.186970, acc.: 92.19%] [G loss: 2.760263]\n",
      "1332 [D loss: 0.204312, acc.: 92.19%] [G loss: 2.562320]\n",
      "1333 [D loss: 0.194293, acc.: 89.06%] [G loss: 2.755937]\n",
      "1334 [D loss: 0.223106, acc.: 87.50%] [G loss: 2.478955]\n",
      "1335 [D loss: 0.229246, acc.: 87.50%] [G loss: 2.512680]\n",
      "1336 [D loss: 0.207852, acc.: 90.62%] [G loss: 2.608844]\n",
      "1337 [D loss: 0.223581, acc.: 87.50%] [G loss: 2.510378]\n",
      "1338 [D loss: 0.246811, acc.: 89.06%] [G loss: 2.844848]\n",
      "1339 [D loss: 0.223186, acc.: 92.19%] [G loss: 2.292187]\n",
      "1340 [D loss: 0.234297, acc.: 85.94%] [G loss: 2.574888]\n",
      "1341 [D loss: 0.251137, acc.: 89.06%] [G loss: 2.491033]\n",
      "1342 [D loss: 0.194459, acc.: 90.62%] [G loss: 2.675706]\n",
      "1343 [D loss: 0.204789, acc.: 92.19%] [G loss: 2.641205]\n",
      "1344 [D loss: 0.183608, acc.: 92.19%] [G loss: 2.609211]\n",
      "1345 [D loss: 0.216623, acc.: 92.19%] [G loss: 2.358864]\n",
      "1346 [D loss: 0.240537, acc.: 85.94%] [G loss: 2.583606]\n",
      "1347 [D loss: 0.223589, acc.: 90.62%] [G loss: 2.693621]\n",
      "1348 [D loss: 0.209274, acc.: 87.50%] [G loss: 2.562115]\n",
      "1349 [D loss: 0.232094, acc.: 90.62%] [G loss: 2.653875]\n",
      "1350 [D loss: 0.170384, acc.: 92.19%] [G loss: 2.794190]\n",
      "1351 [D loss: 0.270851, acc.: 87.50%] [G loss: 2.767391]\n",
      "1352 [D loss: 0.248827, acc.: 89.06%] [G loss: 2.569328]\n",
      "1353 [D loss: 0.252047, acc.: 90.62%] [G loss: 2.460907]\n",
      "1354 [D loss: 0.219023, acc.: 87.50%] [G loss: 2.690346]\n",
      "1355 [D loss: 0.214236, acc.: 90.62%] [G loss: 2.476726]\n",
      "1356 [D loss: 0.215356, acc.: 90.62%] [G loss: 2.794521]\n",
      "1357 [D loss: 0.273444, acc.: 87.50%] [G loss: 2.720432]\n",
      "1358 [D loss: 0.226673, acc.: 89.06%] [G loss: 2.735704]\n",
      "1359 [D loss: 0.221609, acc.: 89.06%] [G loss: 2.699905]\n",
      "1360 [D loss: 0.178289, acc.: 93.75%] [G loss: 2.532309]\n",
      "1361 [D loss: 0.226276, acc.: 90.62%] [G loss: 2.918850]\n",
      "1362 [D loss: 0.208507, acc.: 89.06%] [G loss: 3.057353]\n",
      "1363 [D loss: 0.264075, acc.: 87.50%] [G loss: 2.540563]\n",
      "1364 [D loss: 0.221626, acc.: 89.06%] [G loss: 2.605853]\n",
      "1365 [D loss: 0.218967, acc.: 90.62%] [G loss: 2.678810]\n",
      "1366 [D loss: 0.258582, acc.: 90.62%] [G loss: 2.540108]\n",
      "1367 [D loss: 0.194922, acc.: 90.62%] [G loss: 2.605679]\n",
      "1368 [D loss: 0.230769, acc.: 90.62%] [G loss: 3.114127]\n",
      "1369 [D loss: 0.211444, acc.: 90.62%] [G loss: 2.635874]\n",
      "1370 [D loss: 0.217309, acc.: 90.62%] [G loss: 2.344291]\n",
      "1371 [D loss: 0.240449, acc.: 89.06%] [G loss: 2.765453]\n",
      "1372 [D loss: 0.216139, acc.: 90.62%] [G loss: 2.822840]\n",
      "1373 [D loss: 0.271688, acc.: 85.94%] [G loss: 2.836217]\n",
      "1374 [D loss: 0.242775, acc.: 89.06%] [G loss: 2.634350]\n",
      "1375 [D loss: 0.203237, acc.: 90.62%] [G loss: 2.342835]\n",
      "1376 [D loss: 0.243393, acc.: 87.50%] [G loss: 2.760997]\n",
      "1377 [D loss: 0.210112, acc.: 90.62%] [G loss: 2.350319]\n",
      "1378 [D loss: 0.217133, acc.: 90.62%] [G loss: 2.341963]\n",
      "1379 [D loss: 0.204748, acc.: 89.06%] [G loss: 2.578421]\n",
      "1380 [D loss: 0.197114, acc.: 89.06%] [G loss: 2.605063]\n",
      "1381 [D loss: 0.211941, acc.: 89.06%] [G loss: 2.786570]\n",
      "1382 [D loss: 0.236939, acc.: 89.06%] [G loss: 2.753248]\n",
      "1383 [D loss: 0.206370, acc.: 92.19%] [G loss: 2.685118]\n",
      "1384 [D loss: 0.222834, acc.: 90.62%] [G loss: 2.427025]\n",
      "1385 [D loss: 0.203651, acc.: 87.50%] [G loss: 2.785872]\n",
      "1386 [D loss: 0.214745, acc.: 92.19%] [G loss: 2.475682]\n",
      "1387 [D loss: 0.217366, acc.: 89.06%] [G loss: 2.997458]\n",
      "1388 [D loss: 0.223658, acc.: 89.06%] [G loss: 2.676630]\n",
      "1389 [D loss: 0.237576, acc.: 89.06%] [G loss: 2.477239]\n",
      "1390 [D loss: 0.278827, acc.: 90.62%] [G loss: 3.204672]\n",
      "1391 [D loss: 0.201615, acc.: 90.62%] [G loss: 2.745883]\n",
      "1392 [D loss: 0.226452, acc.: 89.06%] [G loss: 2.586211]\n",
      "1393 [D loss: 0.231113, acc.: 90.62%] [G loss: 2.868971]\n",
      "1394 [D loss: 0.193971, acc.: 92.19%] [G loss: 2.662090]\n",
      "1395 [D loss: 0.274879, acc.: 89.06%] [G loss: 2.825745]\n",
      "1396 [D loss: 0.205963, acc.: 89.06%] [G loss: 2.849451]\n",
      "1397 [D loss: 0.220548, acc.: 90.62%] [G loss: 3.022603]\n",
      "1398 [D loss: 0.177444, acc.: 92.19%] [G loss: 2.837788]\n",
      "1399 [D loss: 0.242074, acc.: 85.94%] [G loss: 2.504032]\n",
      "1400 [D loss: 0.212461, acc.: 92.19%] [G loss: 3.088094]\n",
      "generated_data\n",
      "1401 [D loss: 0.182881, acc.: 92.19%] [G loss: 2.641985]\n",
      "1402 [D loss: 0.195812, acc.: 87.50%] [G loss: 2.806247]\n",
      "1403 [D loss: 0.202256, acc.: 89.06%] [G loss: 2.849289]\n",
      "1404 [D loss: 0.234946, acc.: 87.50%] [G loss: 2.711260]\n",
      "1405 [D loss: 0.230890, acc.: 89.06%] [G loss: 2.771414]\n",
      "1406 [D loss: 0.240187, acc.: 87.50%] [G loss: 2.657655]\n",
      "1407 [D loss: 0.211757, acc.: 90.62%] [G loss: 2.954175]\n",
      "1408 [D loss: 0.212812, acc.: 90.62%] [G loss: 3.195284]\n",
      "1409 [D loss: 0.223194, acc.: 92.19%] [G loss: 2.771400]\n",
      "1410 [D loss: 0.223067, acc.: 89.06%] [G loss: 2.711672]\n",
      "1411 [D loss: 0.210765, acc.: 89.06%] [G loss: 2.411494]\n",
      "1412 [D loss: 0.241436, acc.: 89.06%] [G loss: 2.454562]\n",
      "1413 [D loss: 0.229606, acc.: 90.62%] [G loss: 2.726326]\n",
      "1414 [D loss: 0.217612, acc.: 90.62%] [G loss: 2.976107]\n",
      "1415 [D loss: 0.233530, acc.: 89.06%] [G loss: 2.593607]\n",
      "1416 [D loss: 0.231640, acc.: 89.06%] [G loss: 2.630793]\n",
      "1417 [D loss: 0.187366, acc.: 95.31%] [G loss: 2.601709]\n",
      "1418 [D loss: 0.169710, acc.: 92.19%] [G loss: 2.734972]\n",
      "1419 [D loss: 0.271464, acc.: 85.94%] [G loss: 2.833534]\n",
      "1420 [D loss: 0.182716, acc.: 90.62%] [G loss: 2.805221]\n",
      "1421 [D loss: 0.220186, acc.: 90.62%] [G loss: 2.732129]\n",
      "1422 [D loss: 0.231351, acc.: 89.06%] [G loss: 2.746592]\n",
      "1423 [D loss: 0.215503, acc.: 87.50%] [G loss: 2.965269]\n",
      "1424 [D loss: 0.165767, acc.: 92.19%] [G loss: 2.649864]\n",
      "1425 [D loss: 0.206114, acc.: 90.62%] [G loss: 2.464119]\n",
      "1426 [D loss: 0.168788, acc.: 95.31%] [G loss: 2.523175]\n",
      "1427 [D loss: 0.229225, acc.: 90.62%] [G loss: 2.471265]\n",
      "1428 [D loss: 0.198378, acc.: 92.19%] [G loss: 2.397039]\n",
      "1429 [D loss: 0.235005, acc.: 85.94%] [G loss: 2.781308]\n",
      "1430 [D loss: 0.178928, acc.: 92.19%] [G loss: 2.638909]\n",
      "1431 [D loss: 0.215519, acc.: 92.19%] [G loss: 2.541742]\n",
      "1432 [D loss: 0.187164, acc.: 92.19%] [G loss: 2.791089]\n",
      "1433 [D loss: 0.215833, acc.: 90.62%] [G loss: 2.494865]\n",
      "1434 [D loss: 0.191034, acc.: 92.19%] [G loss: 2.831060]\n",
      "1435 [D loss: 0.197575, acc.: 90.62%] [G loss: 2.381818]\n",
      "1436 [D loss: 0.234283, acc.: 87.50%] [G loss: 2.773213]\n",
      "1437 [D loss: 0.213245, acc.: 90.62%] [G loss: 2.800831]\n",
      "1438 [D loss: 0.197306, acc.: 89.06%] [G loss: 2.822040]\n",
      "1439 [D loss: 0.250687, acc.: 90.62%] [G loss: 2.864258]\n",
      "1440 [D loss: 0.197902, acc.: 92.19%] [G loss: 2.742635]\n",
      "1441 [D loss: 0.236978, acc.: 92.19%] [G loss: 2.962679]\n",
      "1442 [D loss: 0.236879, acc.: 90.62%] [G loss: 2.527999]\n",
      "1443 [D loss: 0.208695, acc.: 89.06%] [G loss: 2.918128]\n",
      "1444 [D loss: 0.233088, acc.: 90.62%] [G loss: 2.401491]\n",
      "1445 [D loss: 0.257717, acc.: 89.06%] [G loss: 2.991529]\n",
      "1446 [D loss: 0.198566, acc.: 92.19%] [G loss: 3.073304]\n",
      "1447 [D loss: 0.197516, acc.: 92.19%] [G loss: 3.036516]\n",
      "1448 [D loss: 0.180528, acc.: 93.75%] [G loss: 2.831825]\n",
      "1449 [D loss: 0.214504, acc.: 90.62%] [G loss: 2.820758]\n",
      "1450 [D loss: 0.227825, acc.: 90.62%] [G loss: 2.926532]\n",
      "1451 [D loss: 0.239706, acc.: 92.19%] [G loss: 2.945513]\n",
      "1452 [D loss: 0.210545, acc.: 89.06%] [G loss: 2.781578]\n",
      "1453 [D loss: 0.203039, acc.: 90.62%] [G loss: 2.776432]\n",
      "1454 [D loss: 0.253510, acc.: 89.06%] [G loss: 2.791269]\n",
      "1455 [D loss: 0.233959, acc.: 85.94%] [G loss: 3.119261]\n",
      "1456 [D loss: 0.248115, acc.: 87.50%] [G loss: 2.890124]\n",
      "1457 [D loss: 0.200372, acc.: 92.19%] [G loss: 3.257112]\n",
      "1458 [D loss: 0.213215, acc.: 89.06%] [G loss: 2.828144]\n",
      "1459 [D loss: 0.208686, acc.: 93.75%] [G loss: 2.658701]\n",
      "1460 [D loss: 0.204849, acc.: 92.19%] [G loss: 2.777764]\n",
      "1461 [D loss: 0.167863, acc.: 92.19%] [G loss: 2.427385]\n",
      "1462 [D loss: 0.244539, acc.: 90.62%] [G loss: 2.881228]\n",
      "1463 [D loss: 0.189632, acc.: 90.62%] [G loss: 2.546534]\n",
      "1464 [D loss: 0.203188, acc.: 89.06%] [G loss: 2.775091]\n",
      "1465 [D loss: 0.212560, acc.: 93.75%] [G loss: 3.056689]\n",
      "1466 [D loss: 0.221493, acc.: 89.06%] [G loss: 2.968117]\n",
      "1467 [D loss: 0.231410, acc.: 90.62%] [G loss: 2.425971]\n",
      "1468 [D loss: 0.225926, acc.: 90.62%] [G loss: 2.516219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1469 [D loss: 0.200935, acc.: 92.19%] [G loss: 2.806294]\n",
      "1470 [D loss: 0.216187, acc.: 92.19%] [G loss: 2.557498]\n",
      "1471 [D loss: 0.212338, acc.: 90.62%] [G loss: 2.864798]\n",
      "1472 [D loss: 0.221169, acc.: 90.62%] [G loss: 2.948061]\n",
      "1473 [D loss: 0.226156, acc.: 89.06%] [G loss: 2.946394]\n",
      "1474 [D loss: 0.275853, acc.: 89.06%] [G loss: 3.382871]\n",
      "1475 [D loss: 0.208750, acc.: 92.19%] [G loss: 2.926632]\n",
      "1476 [D loss: 0.202553, acc.: 92.19%] [G loss: 2.921308]\n",
      "1477 [D loss: 0.218532, acc.: 90.62%] [G loss: 2.707642]\n",
      "1478 [D loss: 0.183103, acc.: 93.75%] [G loss: 3.024158]\n",
      "1479 [D loss: 0.268017, acc.: 85.94%] [G loss: 2.843892]\n",
      "1480 [D loss: 0.213732, acc.: 90.62%] [G loss: 3.208294]\n",
      "1481 [D loss: 0.219162, acc.: 90.62%] [G loss: 2.631534]\n",
      "1482 [D loss: 0.187884, acc.: 92.19%] [G loss: 2.573903]\n",
      "1483 [D loss: 0.154699, acc.: 93.75%] [G loss: 2.720175]\n",
      "1484 [D loss: 0.223983, acc.: 89.06%] [G loss: 2.435440]\n",
      "1485 [D loss: 0.189955, acc.: 92.19%] [G loss: 2.693690]\n",
      "1486 [D loss: 0.231306, acc.: 87.50%] [G loss: 2.596962]\n",
      "1487 [D loss: 0.169499, acc.: 93.75%] [G loss: 2.855511]\n",
      "1488 [D loss: 0.222490, acc.: 89.06%] [G loss: 2.745407]\n",
      "1489 [D loss: 0.233458, acc.: 87.50%] [G loss: 2.699499]\n",
      "1490 [D loss: 0.222008, acc.: 90.62%] [G loss: 2.957053]\n",
      "1491 [D loss: 0.251012, acc.: 89.06%] [G loss: 2.841381]\n",
      "1492 [D loss: 0.260719, acc.: 92.19%] [G loss: 2.790589]\n",
      "1493 [D loss: 0.224003, acc.: 87.50%] [G loss: 2.649533]\n",
      "1494 [D loss: 0.239817, acc.: 90.62%] [G loss: 2.514363]\n",
      "1495 [D loss: 0.182282, acc.: 92.19%] [G loss: 3.010540]\n",
      "1496 [D loss: 0.178672, acc.: 93.75%] [G loss: 2.695930]\n",
      "1497 [D loss: 0.235568, acc.: 90.62%] [G loss: 2.830872]\n",
      "1498 [D loss: 0.222141, acc.: 92.19%] [G loss: 2.659473]\n",
      "1499 [D loss: 0.233937, acc.: 90.62%] [G loss: 3.086800]\n",
      "1500 [D loss: 0.267518, acc.: 87.50%] [G loss: 2.780726]\n",
      "generated_data\n",
      "1501 [D loss: 0.213830, acc.: 90.62%] [G loss: 2.575799]\n",
      "1502 [D loss: 0.239367, acc.: 89.06%] [G loss: 2.808763]\n",
      "1503 [D loss: 0.180377, acc.: 92.19%] [G loss: 2.924395]\n",
      "1504 [D loss: 0.216352, acc.: 90.62%] [G loss: 2.831800]\n",
      "1505 [D loss: 0.184766, acc.: 92.19%] [G loss: 2.478076]\n",
      "1506 [D loss: 0.214150, acc.: 89.06%] [G loss: 2.667232]\n",
      "1507 [D loss: 0.170402, acc.: 92.19%] [G loss: 3.029116]\n",
      "1508 [D loss: 0.218460, acc.: 90.62%] [G loss: 2.385913]\n",
      "1509 [D loss: 0.204082, acc.: 93.75%] [G loss: 2.714780]\n",
      "1510 [D loss: 0.216983, acc.: 92.19%] [G loss: 2.654350]\n",
      "1511 [D loss: 0.260826, acc.: 87.50%] [G loss: 2.792013]\n",
      "1512 [D loss: 0.218416, acc.: 90.62%] [G loss: 2.916611]\n",
      "1513 [D loss: 0.262214, acc.: 89.06%] [G loss: 2.696377]\n",
      "1514 [D loss: 0.237845, acc.: 85.94%] [G loss: 2.863801]\n",
      "1515 [D loss: 0.189264, acc.: 93.75%] [G loss: 2.645750]\n",
      "1516 [D loss: 0.268929, acc.: 85.94%] [G loss: 2.660002]\n",
      "1517 [D loss: 0.272815, acc.: 87.50%] [G loss: 2.831103]\n",
      "1518 [D loss: 0.191474, acc.: 89.06%] [G loss: 3.056598]\n",
      "1519 [D loss: 0.248704, acc.: 87.50%] [G loss: 2.433128]\n",
      "1520 [D loss: 0.216482, acc.: 90.62%] [G loss: 2.615770]\n",
      "1521 [D loss: 0.196331, acc.: 90.62%] [G loss: 2.544044]\n",
      "1522 [D loss: 0.158960, acc.: 95.31%] [G loss: 2.185132]\n",
      "1523 [D loss: 0.204808, acc.: 96.88%] [G loss: 2.841870]\n",
      "1524 [D loss: 0.193830, acc.: 92.19%] [G loss: 2.547493]\n",
      "1525 [D loss: 0.235128, acc.: 89.06%] [G loss: 3.005305]\n",
      "1526 [D loss: 0.243469, acc.: 89.06%] [G loss: 2.736645]\n",
      "1527 [D loss: 0.197961, acc.: 92.19%] [G loss: 2.305086]\n",
      "1528 [D loss: 0.206096, acc.: 92.19%] [G loss: 2.465977]\n",
      "1529 [D loss: 0.215552, acc.: 90.62%] [G loss: 2.610143]\n",
      "1530 [D loss: 0.237576, acc.: 87.50%] [G loss: 3.151696]\n",
      "1531 [D loss: 0.170201, acc.: 93.75%] [G loss: 3.063296]\n",
      "1532 [D loss: 0.228954, acc.: 90.62%] [G loss: 2.790785]\n",
      "1533 [D loss: 0.209136, acc.: 92.19%] [G loss: 2.859729]\n",
      "1534 [D loss: 0.209163, acc.: 90.62%] [G loss: 2.672007]\n",
      "1535 [D loss: 0.251365, acc.: 87.50%] [G loss: 2.354588]\n",
      "1536 [D loss: 0.185927, acc.: 93.75%] [G loss: 2.714456]\n",
      "1537 [D loss: 0.175655, acc.: 90.62%] [G loss: 2.642473]\n",
      "1538 [D loss: 0.216691, acc.: 89.06%] [G loss: 2.969967]\n",
      "1539 [D loss: 0.193098, acc.: 90.62%] [G loss: 2.778244]\n",
      "1540 [D loss: 0.259450, acc.: 87.50%] [G loss: 2.945737]\n",
      "1541 [D loss: 0.195296, acc.: 90.62%] [G loss: 2.673717]\n",
      "1542 [D loss: 0.211641, acc.: 92.19%] [G loss: 2.925653]\n",
      "1543 [D loss: 0.204919, acc.: 93.75%] [G loss: 2.628326]\n",
      "1544 [D loss: 0.200983, acc.: 93.75%] [G loss: 2.931366]\n",
      "1545 [D loss: 0.194264, acc.: 93.75%] [G loss: 2.787886]\n",
      "1546 [D loss: 0.214198, acc.: 92.19%] [G loss: 2.551155]\n",
      "1547 [D loss: 0.234144, acc.: 89.06%] [G loss: 3.213850]\n",
      "1548 [D loss: 0.222104, acc.: 90.62%] [G loss: 2.941845]\n",
      "1549 [D loss: 0.199502, acc.: 92.19%] [G loss: 2.435850]\n",
      "1550 [D loss: 0.257587, acc.: 87.50%] [G loss: 2.780795]\n",
      "1551 [D loss: 0.263002, acc.: 89.06%] [G loss: 2.589167]\n",
      "1552 [D loss: 0.204608, acc.: 93.75%] [G loss: 2.421422]\n",
      "1553 [D loss: 0.183940, acc.: 92.19%] [G loss: 3.067343]\n",
      "1554 [D loss: 0.194804, acc.: 92.19%] [G loss: 2.708752]\n",
      "1555 [D loss: 0.206698, acc.: 92.19%] [G loss: 2.565073]\n",
      "1556 [D loss: 0.209194, acc.: 92.19%] [G loss: 2.820575]\n",
      "1557 [D loss: 0.248519, acc.: 92.19%] [G loss: 3.166553]\n",
      "1558 [D loss: 0.180267, acc.: 92.19%] [G loss: 2.979636]\n",
      "1559 [D loss: 0.208697, acc.: 92.19%] [G loss: 2.545166]\n",
      "1560 [D loss: 0.195191, acc.: 95.31%] [G loss: 2.571636]\n",
      "1561 [D loss: 0.211406, acc.: 92.19%] [G loss: 2.462392]\n",
      "1562 [D loss: 0.194180, acc.: 92.19%] [G loss: 2.820556]\n",
      "1563 [D loss: 0.206973, acc.: 90.62%] [G loss: 2.502920]\n",
      "1564 [D loss: 0.198522, acc.: 93.75%] [G loss: 2.583244]\n",
      "1565 [D loss: 0.201239, acc.: 92.19%] [G loss: 3.007546]\n",
      "1566 [D loss: 0.208091, acc.: 93.75%] [G loss: 3.083869]\n",
      "1567 [D loss: 0.206128, acc.: 89.06%] [G loss: 2.971692]\n",
      "1568 [D loss: 0.237775, acc.: 90.62%] [G loss: 3.016451]\n",
      "1569 [D loss: 0.227329, acc.: 90.62%] [G loss: 3.193618]\n",
      "1570 [D loss: 0.198863, acc.: 90.62%] [G loss: 2.999180]\n",
      "1571 [D loss: 0.223471, acc.: 92.19%] [G loss: 3.034842]\n",
      "1572 [D loss: 0.190543, acc.: 92.19%] [G loss: 2.863508]\n",
      "1573 [D loss: 0.210908, acc.: 90.62%] [G loss: 2.997025]\n",
      "1574 [D loss: 0.196547, acc.: 89.06%] [G loss: 2.613663]\n",
      "1575 [D loss: 0.216862, acc.: 90.62%] [G loss: 2.833321]\n",
      "1576 [D loss: 0.218005, acc.: 89.06%] [G loss: 2.300746]\n",
      "1577 [D loss: 0.177651, acc.: 93.75%] [G loss: 2.879632]\n",
      "1578 [D loss: 0.219088, acc.: 89.06%] [G loss: 2.614345]\n",
      "1579 [D loss: 0.202018, acc.: 90.62%] [G loss: 2.844293]\n",
      "1580 [D loss: 0.204071, acc.: 92.19%] [G loss: 2.693334]\n",
      "1581 [D loss: 0.229292, acc.: 87.50%] [G loss: 2.503364]\n",
      "1582 [D loss: 0.227104, acc.: 89.06%] [G loss: 3.210217]\n",
      "1583 [D loss: 0.225220, acc.: 89.06%] [G loss: 2.989991]\n",
      "1584 [D loss: 0.216637, acc.: 89.06%] [G loss: 2.691032]\n",
      "1585 [D loss: 0.185100, acc.: 92.19%] [G loss: 2.491890]\n",
      "1586 [D loss: 0.226035, acc.: 89.06%] [G loss: 2.625729]\n",
      "1587 [D loss: 0.209677, acc.: 89.06%] [G loss: 3.027438]\n",
      "1588 [D loss: 0.229463, acc.: 90.62%] [G loss: 2.701894]\n",
      "1589 [D loss: 0.224937, acc.: 92.19%] [G loss: 2.787284]\n",
      "1590 [D loss: 0.198322, acc.: 92.19%] [G loss: 2.851358]\n",
      "1591 [D loss: 0.205477, acc.: 92.19%] [G loss: 2.812913]\n",
      "1592 [D loss: 0.241479, acc.: 87.50%] [G loss: 2.958469]\n",
      "1593 [D loss: 0.241857, acc.: 89.06%] [G loss: 2.589100]\n",
      "1594 [D loss: 0.212363, acc.: 93.75%] [G loss: 2.399939]\n",
      "1595 [D loss: 0.278844, acc.: 90.62%] [G loss: 2.856832]\n",
      "1596 [D loss: 0.153200, acc.: 93.75%] [G loss: 2.904963]\n",
      "1597 [D loss: 0.220279, acc.: 89.06%] [G loss: 2.703170]\n",
      "1598 [D loss: 0.211707, acc.: 92.19%] [G loss: 2.936387]\n",
      "1599 [D loss: 0.182875, acc.: 89.06%] [G loss: 2.814601]\n",
      "1600 [D loss: 0.219112, acc.: 92.19%] [G loss: 2.584199]\n",
      "generated_data\n",
      "1601 [D loss: 0.224459, acc.: 92.19%] [G loss: 2.840669]\n",
      "1602 [D loss: 0.213633, acc.: 90.62%] [G loss: 2.797856]\n",
      "1603 [D loss: 0.205184, acc.: 92.19%] [G loss: 2.916462]\n",
      "1604 [D loss: 0.208404, acc.: 92.19%] [G loss: 2.900117]\n",
      "1605 [D loss: 0.199644, acc.: 92.19%] [G loss: 2.738342]\n",
      "1606 [D loss: 0.232163, acc.: 93.75%] [G loss: 3.039771]\n",
      "1607 [D loss: 0.195788, acc.: 89.06%] [G loss: 2.953637]\n",
      "1608 [D loss: 0.215078, acc.: 90.62%] [G loss: 2.327671]\n",
      "1609 [D loss: 0.171399, acc.: 92.19%] [G loss: 2.624357]\n",
      "1610 [D loss: 0.238378, acc.: 87.50%] [G loss: 2.893414]\n",
      "1611 [D loss: 0.144921, acc.: 95.31%] [G loss: 2.908937]\n",
      "1612 [D loss: 0.223233, acc.: 92.19%] [G loss: 2.634145]\n",
      "1613 [D loss: 0.189786, acc.: 93.75%] [G loss: 3.005825]\n",
      "1614 [D loss: 0.196293, acc.: 93.75%] [G loss: 3.001551]\n",
      "1615 [D loss: 0.221538, acc.: 90.62%] [G loss: 2.685253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1616 [D loss: 0.220503, acc.: 84.38%] [G loss: 2.509659]\n",
      "1617 [D loss: 0.212524, acc.: 89.06%] [G loss: 2.946438]\n",
      "1618 [D loss: 0.203260, acc.: 90.62%] [G loss: 2.822297]\n",
      "1619 [D loss: 0.237143, acc.: 92.19%] [G loss: 2.739192]\n",
      "1620 [D loss: 0.229967, acc.: 90.62%] [G loss: 2.958695]\n",
      "1621 [D loss: 0.245553, acc.: 89.06%] [G loss: 3.017958]\n",
      "1622 [D loss: 0.208560, acc.: 90.62%] [G loss: 3.135919]\n",
      "1623 [D loss: 0.185320, acc.: 92.19%] [G loss: 2.557218]\n",
      "1624 [D loss: 0.175332, acc.: 92.19%] [G loss: 2.876842]\n",
      "1625 [D loss: 0.202312, acc.: 95.31%] [G loss: 3.276179]\n",
      "1626 [D loss: 0.199806, acc.: 93.75%] [G loss: 3.098157]\n",
      "1627 [D loss: 0.268567, acc.: 89.06%] [G loss: 3.236370]\n",
      "1628 [D loss: 0.210606, acc.: 89.06%] [G loss: 2.980594]\n",
      "1629 [D loss: 0.173868, acc.: 92.19%] [G loss: 2.717010]\n",
      "1630 [D loss: 0.234564, acc.: 89.06%] [G loss: 3.132800]\n",
      "1631 [D loss: 0.225843, acc.: 92.19%] [G loss: 3.126682]\n",
      "1632 [D loss: 0.228339, acc.: 92.19%] [G loss: 2.998011]\n",
      "1633 [D loss: 0.184839, acc.: 92.19%] [G loss: 2.639638]\n",
      "1634 [D loss: 0.184578, acc.: 90.62%] [G loss: 2.674915]\n",
      "1635 [D loss: 0.205754, acc.: 93.75%] [G loss: 2.593938]\n",
      "1636 [D loss: 0.184923, acc.: 92.19%] [G loss: 2.661919]\n",
      "1637 [D loss: 0.247522, acc.: 90.62%] [G loss: 2.991749]\n",
      "1638 [D loss: 0.267303, acc.: 82.81%] [G loss: 2.258374]\n",
      "1639 [D loss: 0.190583, acc.: 92.19%] [G loss: 2.675943]\n",
      "1640 [D loss: 0.243930, acc.: 89.06%] [G loss: 2.536470]\n",
      "1641 [D loss: 0.208226, acc.: 92.19%] [G loss: 2.556361]\n",
      "1642 [D loss: 0.240896, acc.: 89.06%] [G loss: 2.883675]\n",
      "1643 [D loss: 0.134499, acc.: 95.31%] [G loss: 2.866462]\n",
      "1644 [D loss: 0.247486, acc.: 89.06%] [G loss: 3.005007]\n",
      "1645 [D loss: 0.208087, acc.: 89.06%] [G loss: 2.950827]\n",
      "1646 [D loss: 0.170455, acc.: 93.75%] [G loss: 2.819527]\n",
      "1647 [D loss: 0.194906, acc.: 92.19%] [G loss: 2.573577]\n",
      "1648 [D loss: 0.189893, acc.: 92.19%] [G loss: 2.650118]\n",
      "1649 [D loss: 0.246766, acc.: 90.62%] [G loss: 3.036769]\n",
      "1650 [D loss: 0.214585, acc.: 90.62%] [G loss: 3.179204]\n",
      "1651 [D loss: 0.252389, acc.: 85.94%] [G loss: 3.054156]\n",
      "1652 [D loss: 0.234324, acc.: 90.62%] [G loss: 2.747765]\n",
      "1653 [D loss: 0.195876, acc.: 90.62%] [G loss: 2.750364]\n",
      "1654 [D loss: 0.200810, acc.: 93.75%] [G loss: 2.716756]\n",
      "1655 [D loss: 0.166871, acc.: 93.75%] [G loss: 2.784151]\n",
      "1656 [D loss: 0.173007, acc.: 92.19%] [G loss: 2.639084]\n",
      "1657 [D loss: 0.257677, acc.: 89.06%] [G loss: 2.912394]\n",
      "1658 [D loss: 0.231914, acc.: 90.62%] [G loss: 2.869649]\n",
      "1659 [D loss: 0.184350, acc.: 92.19%] [G loss: 3.003908]\n",
      "1660 [D loss: 0.180146, acc.: 92.19%] [G loss: 2.570057]\n",
      "1661 [D loss: 0.204461, acc.: 93.75%] [G loss: 3.059062]\n",
      "1662 [D loss: 0.217989, acc.: 92.19%] [G loss: 2.899532]\n",
      "1663 [D loss: 0.165477, acc.: 92.19%] [G loss: 2.661240]\n",
      "1664 [D loss: 0.201630, acc.: 93.75%] [G loss: 2.738559]\n",
      "1665 [D loss: 0.178924, acc.: 92.19%] [G loss: 2.929069]\n",
      "1666 [D loss: 0.191198, acc.: 92.19%] [G loss: 2.646891]\n",
      "1667 [D loss: 0.192540, acc.: 90.62%] [G loss: 2.737270]\n",
      "1668 [D loss: 0.258536, acc.: 87.50%] [G loss: 2.809287]\n",
      "1669 [D loss: 0.298826, acc.: 87.50%] [G loss: 3.056657]\n",
      "1670 [D loss: 0.195641, acc.: 90.62%] [G loss: 2.813122]\n",
      "1671 [D loss: 0.243929, acc.: 87.50%] [G loss: 2.702575]\n",
      "1672 [D loss: 0.224299, acc.: 87.50%] [G loss: 2.568396]\n",
      "1673 [D loss: 0.219695, acc.: 87.50%] [G loss: 3.072865]\n",
      "1674 [D loss: 0.205187, acc.: 90.62%] [G loss: 2.996048]\n",
      "1675 [D loss: 0.217697, acc.: 89.06%] [G loss: 3.014054]\n",
      "1676 [D loss: 0.206339, acc.: 90.62%] [G loss: 2.687706]\n",
      "1677 [D loss: 0.189628, acc.: 92.19%] [G loss: 2.706416]\n",
      "1678 [D loss: 0.208334, acc.: 90.62%] [G loss: 2.923649]\n",
      "1679 [D loss: 0.189819, acc.: 90.62%] [G loss: 3.020408]\n",
      "1680 [D loss: 0.220105, acc.: 89.06%] [G loss: 2.672948]\n",
      "1681 [D loss: 0.226704, acc.: 90.62%] [G loss: 2.984579]\n",
      "1682 [D loss: 0.160704, acc.: 92.19%] [G loss: 2.875394]\n",
      "1683 [D loss: 0.238077, acc.: 90.62%] [G loss: 2.530879]\n",
      "1684 [D loss: 0.233856, acc.: 90.62%] [G loss: 2.857225]\n",
      "1685 [D loss: 0.169227, acc.: 93.75%] [G loss: 3.235755]\n",
      "1686 [D loss: 0.211789, acc.: 90.62%] [G loss: 2.565158]\n",
      "1687 [D loss: 0.197510, acc.: 93.75%] [G loss: 2.492938]\n",
      "1688 [D loss: 0.174644, acc.: 92.19%] [G loss: 2.809347]\n",
      "1689 [D loss: 0.225303, acc.: 89.06%] [G loss: 2.879032]\n",
      "1690 [D loss: 0.206306, acc.: 89.06%] [G loss: 2.856452]\n",
      "1691 [D loss: 0.173164, acc.: 93.75%] [G loss: 2.469530]\n",
      "1692 [D loss: 0.204658, acc.: 92.19%] [G loss: 2.897402]\n",
      "1693 [D loss: 0.174261, acc.: 93.75%] [G loss: 2.620360]\n",
      "1694 [D loss: 0.208170, acc.: 95.31%] [G loss: 2.763233]\n",
      "1695 [D loss: 0.155740, acc.: 93.75%] [G loss: 2.749535]\n",
      "1696 [D loss: 0.209671, acc.: 90.62%] [G loss: 2.916064]\n",
      "1697 [D loss: 0.182658, acc.: 92.19%] [G loss: 2.649322]\n",
      "1698 [D loss: 0.256502, acc.: 93.75%] [G loss: 3.151213]\n",
      "1699 [D loss: 0.224271, acc.: 89.06%] [G loss: 2.903244]\n",
      "1700 [D loss: 0.217962, acc.: 93.75%] [G loss: 2.896529]\n",
      "generated_data\n",
      "1701 [D loss: 0.234677, acc.: 90.62%] [G loss: 3.072383]\n",
      "1702 [D loss: 0.167655, acc.: 92.19%] [G loss: 2.905250]\n",
      "1703 [D loss: 0.237769, acc.: 89.06%] [G loss: 2.497590]\n",
      "1704 [D loss: 0.173053, acc.: 93.75%] [G loss: 2.766952]\n",
      "1705 [D loss: 0.207966, acc.: 92.19%] [G loss: 2.661763]\n",
      "1706 [D loss: 0.205659, acc.: 93.75%] [G loss: 3.387201]\n",
      "1707 [D loss: 0.213979, acc.: 90.62%] [G loss: 2.745803]\n",
      "1708 [D loss: 0.253931, acc.: 85.94%] [G loss: 2.467870]\n",
      "1709 [D loss: 0.230918, acc.: 89.06%] [G loss: 3.350908]\n",
      "1710 [D loss: 0.248206, acc.: 89.06%] [G loss: 2.994502]\n",
      "1711 [D loss: 0.194632, acc.: 92.19%] [G loss: 2.942418]\n",
      "1712 [D loss: 0.227715, acc.: 92.19%] [G loss: 2.854927]\n",
      "1713 [D loss: 0.189607, acc.: 93.75%] [G loss: 2.568731]\n",
      "1714 [D loss: 0.272589, acc.: 90.62%] [G loss: 2.729114]\n",
      "1715 [D loss: 0.187640, acc.: 92.19%] [G loss: 3.240802]\n",
      "1716 [D loss: 0.209769, acc.: 93.75%] [G loss: 3.141009]\n",
      "1717 [D loss: 0.193845, acc.: 90.62%] [G loss: 2.890622]\n",
      "1718 [D loss: 0.213598, acc.: 93.75%] [G loss: 3.097731]\n",
      "1719 [D loss: 0.247651, acc.: 89.06%] [G loss: 3.118856]\n",
      "1720 [D loss: 0.178502, acc.: 92.19%] [G loss: 2.994738]\n",
      "1721 [D loss: 0.207403, acc.: 92.19%] [G loss: 2.702624]\n",
      "1722 [D loss: 0.208885, acc.: 93.75%] [G loss: 2.721045]\n",
      "1723 [D loss: 0.180073, acc.: 92.19%] [G loss: 2.925307]\n",
      "1724 [D loss: 0.183433, acc.: 92.19%] [G loss: 2.769379]\n",
      "1725 [D loss: 0.195135, acc.: 92.19%] [G loss: 2.622296]\n",
      "1726 [D loss: 0.228536, acc.: 87.50%] [G loss: 2.897390]\n",
      "1727 [D loss: 0.221278, acc.: 92.19%] [G loss: 2.517736]\n",
      "1728 [D loss: 0.261740, acc.: 90.62%] [G loss: 3.001147]\n",
      "1729 [D loss: 0.235643, acc.: 92.19%] [G loss: 3.013748]\n",
      "1730 [D loss: 0.257603, acc.: 92.19%] [G loss: 2.904735]\n",
      "1731 [D loss: 0.240052, acc.: 87.50%] [G loss: 2.681278]\n",
      "1732 [D loss: 0.212999, acc.: 92.19%] [G loss: 3.172707]\n",
      "1733 [D loss: 0.222166, acc.: 89.06%] [G loss: 3.339290]\n",
      "1734 [D loss: 0.251126, acc.: 90.62%] [G loss: 3.129927]\n",
      "1735 [D loss: 0.212715, acc.: 92.19%] [G loss: 2.526602]\n",
      "1736 [D loss: 0.186877, acc.: 92.19%] [G loss: 2.826715]\n",
      "1737 [D loss: 0.208383, acc.: 92.19%] [G loss: 2.991804]\n",
      "1738 [D loss: 0.209779, acc.: 90.62%] [G loss: 3.071649]\n",
      "1739 [D loss: 0.219131, acc.: 90.62%] [G loss: 2.625680]\n",
      "1740 [D loss: 0.218208, acc.: 92.19%] [G loss: 2.819173]\n",
      "1741 [D loss: 0.214401, acc.: 93.75%] [G loss: 2.802405]\n",
      "1742 [D loss: 0.202501, acc.: 92.19%] [G loss: 2.821644]\n",
      "1743 [D loss: 0.259808, acc.: 87.50%] [G loss: 2.826388]\n",
      "1744 [D loss: 0.198576, acc.: 92.19%] [G loss: 2.747207]\n",
      "1745 [D loss: 0.232356, acc.: 89.06%] [G loss: 2.637873]\n",
      "1746 [D loss: 0.216377, acc.: 90.62%] [G loss: 2.709531]\n",
      "1747 [D loss: 0.253374, acc.: 90.62%] [G loss: 2.806492]\n",
      "1748 [D loss: 0.244545, acc.: 87.50%] [G loss: 2.680967]\n",
      "1749 [D loss: 0.184407, acc.: 95.31%] [G loss: 2.711076]\n",
      "1750 [D loss: 0.276584, acc.: 89.06%] [G loss: 2.961678]\n",
      "1751 [D loss: 0.221576, acc.: 92.19%] [G loss: 2.774313]\n",
      "1752 [D loss: 0.215215, acc.: 92.19%] [G loss: 2.678373]\n",
      "1753 [D loss: 0.217342, acc.: 92.19%] [G loss: 2.477497]\n",
      "1754 [D loss: 0.163645, acc.: 93.75%] [G loss: 2.865321]\n",
      "1755 [D loss: 0.231114, acc.: 89.06%] [G loss: 2.754885]\n",
      "1756 [D loss: 0.178381, acc.: 93.75%] [G loss: 2.549792]\n",
      "1757 [D loss: 0.244772, acc.: 87.50%] [G loss: 2.572010]\n",
      "1758 [D loss: 0.195543, acc.: 90.62%] [G loss: 2.975202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1759 [D loss: 0.240948, acc.: 90.62%] [G loss: 2.532223]\n",
      "1760 [D loss: 0.187782, acc.: 90.62%] [G loss: 2.807539]\n",
      "1761 [D loss: 0.225500, acc.: 92.19%] [G loss: 2.754704]\n",
      "1762 [D loss: 0.251674, acc.: 89.06%] [G loss: 3.453749]\n",
      "1763 [D loss: 0.204783, acc.: 92.19%] [G loss: 2.687107]\n",
      "1764 [D loss: 0.220423, acc.: 89.06%] [G loss: 2.719827]\n",
      "1765 [D loss: 0.178584, acc.: 90.62%] [G loss: 2.583723]\n",
      "1766 [D loss: 0.193166, acc.: 90.62%] [G loss: 2.849227]\n",
      "1767 [D loss: 0.209802, acc.: 92.19%] [G loss: 2.996534]\n",
      "1768 [D loss: 0.189698, acc.: 90.62%] [G loss: 2.870479]\n",
      "1769 [D loss: 0.220670, acc.: 92.19%] [G loss: 2.785709]\n",
      "1770 [D loss: 0.184184, acc.: 93.75%] [G loss: 3.111469]\n",
      "1771 [D loss: 0.195364, acc.: 90.62%] [G loss: 2.757357]\n",
      "1772 [D loss: 0.217047, acc.: 92.19%] [G loss: 2.976834]\n",
      "1773 [D loss: 0.259260, acc.: 90.62%] [G loss: 2.863204]\n",
      "1774 [D loss: 0.195687, acc.: 92.19%] [G loss: 2.539458]\n",
      "1775 [D loss: 0.245864, acc.: 87.50%] [G loss: 2.708429]\n",
      "1776 [D loss: 0.203953, acc.: 92.19%] [G loss: 3.293916]\n",
      "1777 [D loss: 0.228410, acc.: 90.62%] [G loss: 2.832290]\n",
      "1778 [D loss: 0.241051, acc.: 92.19%] [G loss: 2.631054]\n",
      "1779 [D loss: 0.164477, acc.: 93.75%] [G loss: 3.030230]\n",
      "1780 [D loss: 0.237487, acc.: 90.62%] [G loss: 2.636826]\n",
      "1781 [D loss: 0.224948, acc.: 90.62%] [G loss: 3.037757]\n",
      "1782 [D loss: 0.237665, acc.: 89.06%] [G loss: 2.865512]\n",
      "1783 [D loss: 0.174606, acc.: 93.75%] [G loss: 2.567961]\n",
      "1784 [D loss: 0.256869, acc.: 89.06%] [G loss: 2.766116]\n",
      "1785 [D loss: 0.148978, acc.: 95.31%] [G loss: 2.666997]\n",
      "1786 [D loss: 0.195164, acc.: 93.75%] [G loss: 2.911849]\n",
      "1787 [D loss: 0.201604, acc.: 90.62%] [G loss: 2.789918]\n",
      "1788 [D loss: 0.185469, acc.: 93.75%] [G loss: 2.773540]\n",
      "1789 [D loss: 0.193566, acc.: 90.62%] [G loss: 2.851328]\n",
      "1790 [D loss: 0.154688, acc.: 93.75%] [G loss: 2.919078]\n",
      "1791 [D loss: 0.205500, acc.: 93.75%] [G loss: 2.904306]\n",
      "1792 [D loss: 0.178167, acc.: 93.75%] [G loss: 2.410049]\n",
      "1793 [D loss: 0.215754, acc.: 92.19%] [G loss: 2.978175]\n",
      "1794 [D loss: 0.188760, acc.: 95.31%] [G loss: 2.835328]\n",
      "1795 [D loss: 0.201072, acc.: 92.19%] [G loss: 3.101152]\n",
      "1796 [D loss: 0.205266, acc.: 92.19%] [G loss: 3.342369]\n",
      "1797 [D loss: 0.217605, acc.: 87.50%] [G loss: 2.895465]\n",
      "1798 [D loss: 0.225658, acc.: 90.62%] [G loss: 3.298354]\n",
      "1799 [D loss: 0.200079, acc.: 90.62%] [G loss: 2.872991]\n",
      "1800 [D loss: 0.237948, acc.: 90.62%] [G loss: 2.896109]\n",
      "generated_data\n",
      "1801 [D loss: 0.211675, acc.: 92.19%] [G loss: 3.141259]\n",
      "1802 [D loss: 0.223863, acc.: 92.19%] [G loss: 3.178456]\n",
      "1803 [D loss: 0.190092, acc.: 92.19%] [G loss: 2.887201]\n",
      "1804 [D loss: 0.239220, acc.: 89.06%] [G loss: 3.008316]\n",
      "1805 [D loss: 0.186617, acc.: 92.19%] [G loss: 2.715622]\n",
      "1806 [D loss: 0.183105, acc.: 93.75%] [G loss: 3.009079]\n",
      "1807 [D loss: 0.214036, acc.: 90.62%] [G loss: 3.154709]\n",
      "1808 [D loss: 0.221619, acc.: 89.06%] [G loss: 2.916021]\n",
      "1809 [D loss: 0.174286, acc.: 93.75%] [G loss: 2.720219]\n",
      "1810 [D loss: 0.271619, acc.: 84.38%] [G loss: 2.618948]\n",
      "1811 [D loss: 0.182610, acc.: 92.19%] [G loss: 2.862123]\n",
      "1812 [D loss: 0.200330, acc.: 90.62%] [G loss: 2.696153]\n",
      "1813 [D loss: 0.189266, acc.: 90.62%] [G loss: 3.175570]\n",
      "1814 [D loss: 0.170227, acc.: 95.31%] [G loss: 2.554726]\n",
      "1815 [D loss: 0.200121, acc.: 89.06%] [G loss: 2.887672]\n",
      "1816 [D loss: 0.210292, acc.: 90.62%] [G loss: 3.029309]\n",
      "1817 [D loss: 0.204137, acc.: 92.19%] [G loss: 2.610597]\n",
      "1818 [D loss: 0.244852, acc.: 90.62%] [G loss: 2.975008]\n",
      "1819 [D loss: 0.170131, acc.: 92.19%] [G loss: 2.702349]\n",
      "1820 [D loss: 0.225795, acc.: 90.62%] [G loss: 2.843578]\n",
      "1821 [D loss: 0.194070, acc.: 93.75%] [G loss: 2.909280]\n",
      "1822 [D loss: 0.183286, acc.: 92.19%] [G loss: 2.722301]\n",
      "1823 [D loss: 0.188390, acc.: 87.50%] [G loss: 2.719514]\n",
      "1824 [D loss: 0.189235, acc.: 92.19%] [G loss: 2.937831]\n",
      "1825 [D loss: 0.212940, acc.: 90.62%] [G loss: 2.678834]\n",
      "1826 [D loss: 0.195650, acc.: 93.75%] [G loss: 3.336170]\n",
      "1827 [D loss: 0.202574, acc.: 90.62%] [G loss: 2.609641]\n",
      "1828 [D loss: 0.206887, acc.: 92.19%] [G loss: 2.623036]\n",
      "1829 [D loss: 0.230562, acc.: 89.06%] [G loss: 2.377338]\n",
      "1830 [D loss: 0.214068, acc.: 92.19%] [G loss: 2.815053]\n",
      "1831 [D loss: 0.216110, acc.: 89.06%] [G loss: 2.795800]\n",
      "1832 [D loss: 0.195642, acc.: 95.31%] [G loss: 3.055045]\n",
      "1833 [D loss: 0.190982, acc.: 90.62%] [G loss: 2.562377]\n",
      "1834 [D loss: 0.223583, acc.: 89.06%] [G loss: 2.981535]\n",
      "1835 [D loss: 0.283052, acc.: 87.50%] [G loss: 2.751330]\n",
      "1836 [D loss: 0.232301, acc.: 89.06%] [G loss: 2.704767]\n",
      "1837 [D loss: 0.235781, acc.: 92.19%] [G loss: 3.011878]\n",
      "1838 [D loss: 0.268414, acc.: 87.50%] [G loss: 2.951125]\n",
      "1839 [D loss: 0.229398, acc.: 93.75%] [G loss: 2.681275]\n",
      "1840 [D loss: 0.231441, acc.: 92.19%] [G loss: 3.084844]\n",
      "1841 [D loss: 0.187168, acc.: 92.19%] [G loss: 2.857092]\n",
      "1842 [D loss: 0.184695, acc.: 93.75%] [G loss: 2.196455]\n",
      "1843 [D loss: 0.202768, acc.: 93.75%] [G loss: 2.621325]\n",
      "1844 [D loss: 0.193200, acc.: 93.75%] [G loss: 2.519727]\n",
      "1845 [D loss: 0.198455, acc.: 89.06%] [G loss: 3.182954]\n",
      "1846 [D loss: 0.236127, acc.: 87.50%] [G loss: 2.828396]\n",
      "1847 [D loss: 0.199333, acc.: 92.19%] [G loss: 2.776450]\n",
      "1848 [D loss: 0.224694, acc.: 90.62%] [G loss: 3.126882]\n",
      "1849 [D loss: 0.224628, acc.: 87.50%] [G loss: 2.722714]\n",
      "1850 [D loss: 0.238612, acc.: 89.06%] [G loss: 2.900706]\n",
      "1851 [D loss: 0.235253, acc.: 92.19%] [G loss: 2.857601]\n",
      "1852 [D loss: 0.169913, acc.: 95.31%] [G loss: 2.561081]\n",
      "1853 [D loss: 0.192574, acc.: 92.19%] [G loss: 2.881439]\n",
      "1854 [D loss: 0.226963, acc.: 87.50%] [G loss: 2.537197]\n",
      "1855 [D loss: 0.182067, acc.: 92.19%] [G loss: 2.759109]\n",
      "1856 [D loss: 0.266044, acc.: 85.94%] [G loss: 2.773472]\n",
      "1857 [D loss: 0.206105, acc.: 92.19%] [G loss: 3.000214]\n",
      "1858 [D loss: 0.215718, acc.: 89.06%] [G loss: 2.673870]\n",
      "1859 [D loss: 0.188730, acc.: 92.19%] [G loss: 2.768155]\n",
      "1860 [D loss: 0.178882, acc.: 93.75%] [G loss: 2.544571]\n",
      "1861 [D loss: 0.239488, acc.: 92.19%] [G loss: 2.666839]\n",
      "1862 [D loss: 0.195967, acc.: 92.19%] [G loss: 2.944702]\n",
      "1863 [D loss: 0.203366, acc.: 92.19%] [G loss: 2.819512]\n",
      "1864 [D loss: 0.184252, acc.: 92.19%] [G loss: 2.484873]\n",
      "1865 [D loss: 0.194414, acc.: 93.75%] [G loss: 3.080804]\n",
      "1866 [D loss: 0.220432, acc.: 90.62%] [G loss: 3.169806]\n",
      "1867 [D loss: 0.203688, acc.: 92.19%] [G loss: 2.936732]\n",
      "1868 [D loss: 0.206362, acc.: 93.75%] [G loss: 2.967701]\n",
      "1869 [D loss: 0.225605, acc.: 92.19%] [G loss: 2.716481]\n",
      "1870 [D loss: 0.234395, acc.: 89.06%] [G loss: 2.760215]\n",
      "1871 [D loss: 0.233383, acc.: 92.19%] [G loss: 2.950575]\n",
      "1872 [D loss: 0.250270, acc.: 89.06%] [G loss: 3.123410]\n",
      "1873 [D loss: 0.166359, acc.: 95.31%] [G loss: 2.828254]\n",
      "1874 [D loss: 0.193168, acc.: 90.62%] [G loss: 2.675142]\n",
      "1875 [D loss: 0.198415, acc.: 90.62%] [G loss: 2.989432]\n",
      "1876 [D loss: 0.208188, acc.: 95.31%] [G loss: 3.299502]\n",
      "1877 [D loss: 0.175614, acc.: 93.75%] [G loss: 2.804890]\n",
      "1878 [D loss: 0.163769, acc.: 95.31%] [G loss: 2.574493]\n",
      "1879 [D loss: 0.210459, acc.: 92.19%] [G loss: 3.045359]\n",
      "1880 [D loss: 0.202229, acc.: 92.19%] [G loss: 2.713789]\n",
      "1881 [D loss: 0.170686, acc.: 90.62%] [G loss: 2.219910]\n",
      "1882 [D loss: 0.187471, acc.: 93.75%] [G loss: 2.678573]\n",
      "1883 [D loss: 0.236874, acc.: 90.62%] [G loss: 2.959234]\n",
      "1884 [D loss: 0.195892, acc.: 90.62%] [G loss: 3.199810]\n",
      "1885 [D loss: 0.244518, acc.: 92.19%] [G loss: 2.917329]\n",
      "1886 [D loss: 0.168856, acc.: 93.75%] [G loss: 2.974379]\n",
      "1887 [D loss: 0.219405, acc.: 92.19%] [G loss: 2.874888]\n",
      "1888 [D loss: 0.172626, acc.: 95.31%] [G loss: 2.831515]\n",
      "1889 [D loss: 0.207741, acc.: 93.75%] [G loss: 3.290389]\n",
      "1890 [D loss: 0.217687, acc.: 90.62%] [G loss: 3.090759]\n",
      "1891 [D loss: 0.246317, acc.: 89.06%] [G loss: 3.017701]\n",
      "1892 [D loss: 0.205174, acc.: 92.19%] [G loss: 3.028692]\n",
      "1893 [D loss: 0.186860, acc.: 93.75%] [G loss: 2.856260]\n",
      "1894 [D loss: 0.204824, acc.: 92.19%] [G loss: 2.420190]\n",
      "1895 [D loss: 0.172261, acc.: 93.75%] [G loss: 2.893136]\n",
      "1896 [D loss: 0.166839, acc.: 95.31%] [G loss: 2.385932]\n",
      "1897 [D loss: 0.200334, acc.: 90.62%] [G loss: 2.890529]\n",
      "1898 [D loss: 0.173966, acc.: 93.75%] [G loss: 2.997351]\n",
      "1899 [D loss: 0.188634, acc.: 90.62%] [G loss: 3.053092]\n",
      "1900 [D loss: 0.213714, acc.: 89.06%] [G loss: 3.159541]\n",
      "generated_data\n",
      "1901 [D loss: 0.264033, acc.: 89.06%] [G loss: 2.995018]\n",
      "1902 [D loss: 0.229413, acc.: 90.62%] [G loss: 2.687775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1903 [D loss: 0.204752, acc.: 93.75%] [G loss: 3.012107]\n",
      "1904 [D loss: 0.193989, acc.: 92.19%] [G loss: 2.615480]\n",
      "1905 [D loss: 0.171523, acc.: 93.75%] [G loss: 3.023839]\n",
      "1906 [D loss: 0.221899, acc.: 89.06%] [G loss: 2.615951]\n",
      "1907 [D loss: 0.180635, acc.: 92.19%] [G loss: 3.097538]\n",
      "1908 [D loss: 0.227690, acc.: 90.62%] [G loss: 2.739310]\n",
      "1909 [D loss: 0.219282, acc.: 89.06%] [G loss: 3.030734]\n",
      "1910 [D loss: 0.213320, acc.: 90.62%] [G loss: 3.054606]\n",
      "1911 [D loss: 0.204320, acc.: 90.62%] [G loss: 3.020321]\n",
      "1912 [D loss: 0.258107, acc.: 90.62%] [G loss: 2.625292]\n",
      "1913 [D loss: 0.255430, acc.: 90.62%] [G loss: 2.775203]\n",
      "1914 [D loss: 0.191012, acc.: 93.75%] [G loss: 2.888201]\n",
      "1915 [D loss: 0.202528, acc.: 90.62%] [G loss: 2.880820]\n",
      "1916 [D loss: 0.214264, acc.: 89.06%] [G loss: 2.975913]\n",
      "1917 [D loss: 0.223123, acc.: 90.62%] [G loss: 2.952111]\n",
      "1918 [D loss: 0.189878, acc.: 90.62%] [G loss: 2.902881]\n",
      "1919 [D loss: 0.221330, acc.: 93.75%] [G loss: 3.106540]\n",
      "1920 [D loss: 0.146929, acc.: 95.31%] [G loss: 3.093404]\n",
      "1921 [D loss: 0.202706, acc.: 92.19%] [G loss: 3.013166]\n",
      "1922 [D loss: 0.190000, acc.: 92.19%] [G loss: 2.866329]\n",
      "1923 [D loss: 0.185298, acc.: 95.31%] [G loss: 3.178319]\n",
      "1924 [D loss: 0.176158, acc.: 90.62%] [G loss: 2.958109]\n",
      "1925 [D loss: 0.193555, acc.: 93.75%] [G loss: 2.898163]\n",
      "1926 [D loss: 0.189939, acc.: 92.19%] [G loss: 2.702515]\n",
      "1927 [D loss: 0.175523, acc.: 93.75%] [G loss: 2.575321]\n",
      "1928 [D loss: 0.172404, acc.: 93.75%] [G loss: 2.577672]\n",
      "1929 [D loss: 0.199824, acc.: 93.75%] [G loss: 2.901386]\n",
      "1930 [D loss: 0.212866, acc.: 92.19%] [G loss: 2.902292]\n",
      "1931 [D loss: 0.209687, acc.: 92.19%] [G loss: 2.885810]\n",
      "1932 [D loss: 0.261019, acc.: 92.19%] [G loss: 2.942723]\n",
      "1933 [D loss: 0.208652, acc.: 90.62%] [G loss: 2.807054]\n",
      "1934 [D loss: 0.239084, acc.: 90.62%] [G loss: 3.067239]\n",
      "1935 [D loss: 0.220472, acc.: 85.94%] [G loss: 3.136382]\n",
      "1936 [D loss: 0.197023, acc.: 90.62%] [G loss: 2.778705]\n",
      "1937 [D loss: 0.177750, acc.: 95.31%] [G loss: 2.934433]\n",
      "1938 [D loss: 0.232695, acc.: 90.62%] [G loss: 2.817167]\n",
      "1939 [D loss: 0.203455, acc.: 92.19%] [G loss: 2.746401]\n",
      "1940 [D loss: 0.204393, acc.: 92.19%] [G loss: 2.834124]\n",
      "1941 [D loss: 0.177042, acc.: 95.31%] [G loss: 2.734387]\n",
      "1942 [D loss: 0.186513, acc.: 90.62%] [G loss: 2.820566]\n",
      "1943 [D loss: 0.194742, acc.: 90.62%] [G loss: 2.525084]\n",
      "1944 [D loss: 0.195941, acc.: 92.19%] [G loss: 3.392989]\n",
      "1945 [D loss: 0.183817, acc.: 90.62%] [G loss: 2.690019]\n",
      "1946 [D loss: 0.269704, acc.: 87.50%] [G loss: 3.028883]\n",
      "1947 [D loss: 0.198821, acc.: 90.62%] [G loss: 3.154477]\n",
      "1948 [D loss: 0.263433, acc.: 92.19%] [G loss: 3.136587]\n",
      "1949 [D loss: 0.198396, acc.: 90.62%] [G loss: 2.268005]\n",
      "1950 [D loss: 0.215972, acc.: 92.19%] [G loss: 2.789193]\n",
      "1951 [D loss: 0.160814, acc.: 93.75%] [G loss: 2.743010]\n",
      "1952 [D loss: 0.175045, acc.: 93.75%] [G loss: 2.601452]\n",
      "1953 [D loss: 0.173152, acc.: 93.75%] [G loss: 2.523755]\n",
      "1954 [D loss: 0.176036, acc.: 90.62%] [G loss: 2.605517]\n",
      "1955 [D loss: 0.222347, acc.: 89.06%] [G loss: 3.207133]\n",
      "1956 [D loss: 0.191689, acc.: 95.31%] [G loss: 3.123292]\n",
      "1957 [D loss: 0.194244, acc.: 93.75%] [G loss: 3.134218]\n",
      "1958 [D loss: 0.199814, acc.: 92.19%] [G loss: 2.954777]\n",
      "1959 [D loss: 0.217771, acc.: 92.19%] [G loss: 2.805344]\n",
      "1960 [D loss: 0.204939, acc.: 92.19%] [G loss: 3.051045]\n",
      "1961 [D loss: 0.174472, acc.: 95.31%] [G loss: 3.575902]\n",
      "1962 [D loss: 0.198953, acc.: 95.31%] [G loss: 2.890032]\n",
      "1963 [D loss: 0.201475, acc.: 92.19%] [G loss: 3.511065]\n",
      "1964 [D loss: 0.165453, acc.: 95.31%] [G loss: 2.575022]\n",
      "1965 [D loss: 0.234712, acc.: 89.06%] [G loss: 3.408428]\n",
      "1966 [D loss: 0.170947, acc.: 93.75%] [G loss: 2.891131]\n",
      "1967 [D loss: 0.173989, acc.: 95.31%] [G loss: 2.544066]\n",
      "1968 [D loss: 0.239742, acc.: 90.62%] [G loss: 2.766246]\n",
      "1969 [D loss: 0.197101, acc.: 90.62%] [G loss: 2.814962]\n",
      "1970 [D loss: 0.187036, acc.: 90.62%] [G loss: 2.971354]\n",
      "1971 [D loss: 0.196423, acc.: 92.19%] [G loss: 2.578505]\n",
      "1972 [D loss: 0.190917, acc.: 93.75%] [G loss: 2.965627]\n",
      "1973 [D loss: 0.202338, acc.: 89.06%] [G loss: 2.981848]\n",
      "1974 [D loss: 0.215942, acc.: 93.75%] [G loss: 2.990694]\n",
      "1975 [D loss: 0.253798, acc.: 87.50%] [G loss: 2.764291]\n",
      "1976 [D loss: 0.222969, acc.: 93.75%] [G loss: 2.810014]\n",
      "1977 [D loss: 0.199947, acc.: 92.19%] [G loss: 2.808902]\n",
      "1978 [D loss: 0.196704, acc.: 95.31%] [G loss: 3.140280]\n",
      "1979 [D loss: 0.224597, acc.: 85.94%] [G loss: 3.003445]\n",
      "1980 [D loss: 0.223652, acc.: 89.06%] [G loss: 2.859945]\n",
      "1981 [D loss: 0.167411, acc.: 95.31%] [G loss: 2.546740]\n",
      "1982 [D loss: 0.191985, acc.: 93.75%] [G loss: 2.931754]\n",
      "1983 [D loss: 0.225291, acc.: 90.62%] [G loss: 2.947695]\n",
      "1984 [D loss: 0.184821, acc.: 93.75%] [G loss: 2.867738]\n",
      "1985 [D loss: 0.203979, acc.: 92.19%] [G loss: 3.125369]\n",
      "1986 [D loss: 0.156223, acc.: 95.31%] [G loss: 2.737972]\n",
      "1987 [D loss: 0.226406, acc.: 92.19%] [G loss: 3.236031]\n",
      "1988 [D loss: 0.240859, acc.: 92.19%] [G loss: 2.729073]\n",
      "1989 [D loss: 0.201186, acc.: 93.75%] [G loss: 2.975208]\n",
      "1990 [D loss: 0.210178, acc.: 92.19%] [G loss: 3.319664]\n",
      "1991 [D loss: 0.174323, acc.: 92.19%] [G loss: 2.971061]\n",
      "1992 [D loss: 0.185298, acc.: 92.19%] [G loss: 2.917597]\n",
      "1993 [D loss: 0.198642, acc.: 93.75%] [G loss: 2.803066]\n",
      "1994 [D loss: 0.161478, acc.: 93.75%] [G loss: 3.175766]\n",
      "1995 [D loss: 0.154570, acc.: 95.31%] [G loss: 2.985547]\n",
      "1996 [D loss: 0.178673, acc.: 95.31%] [G loss: 2.934596]\n",
      "1997 [D loss: 0.169706, acc.: 93.75%] [G loss: 3.117877]\n",
      "1998 [D loss: 0.180469, acc.: 92.19%] [G loss: 2.971559]\n",
      "1999 [D loss: 0.221878, acc.: 90.62%] [G loss: 2.861553]\n",
      "2000 [D loss: 0.170787, acc.: 93.75%] [G loss: 3.032109]\n",
      "generated_data\n",
      "2001 [D loss: 0.216502, acc.: 89.06%] [G loss: 2.650626]\n",
      "2002 [D loss: 0.173904, acc.: 92.19%] [G loss: 2.856553]\n",
      "2003 [D loss: 0.239791, acc.: 90.62%] [G loss: 3.102098]\n",
      "2004 [D loss: 0.172034, acc.: 92.19%] [G loss: 2.903465]\n",
      "2005 [D loss: 0.202009, acc.: 90.62%] [G loss: 2.790136]\n",
      "2006 [D loss: 0.215164, acc.: 92.19%] [G loss: 2.620509]\n",
      "2007 [D loss: 0.177149, acc.: 92.19%] [G loss: 2.445896]\n",
      "2008 [D loss: 0.147458, acc.: 93.75%] [G loss: 2.536996]\n",
      "2009 [D loss: 0.201039, acc.: 93.75%] [G loss: 3.395735]\n",
      "2010 [D loss: 0.211270, acc.: 90.62%] [G loss: 2.775467]\n",
      "2011 [D loss: 0.173757, acc.: 93.75%] [G loss: 2.625649]\n",
      "2012 [D loss: 0.213143, acc.: 93.75%] [G loss: 2.899508]\n",
      "2013 [D loss: 0.200481, acc.: 93.75%] [G loss: 2.770335]\n",
      "2014 [D loss: 0.186546, acc.: 95.31%] [G loss: 3.038558]\n",
      "2015 [D loss: 0.179035, acc.: 92.19%] [G loss: 3.102203]\n",
      "2016 [D loss: 0.169683, acc.: 93.75%] [G loss: 2.698470]\n",
      "2017 [D loss: 0.217762, acc.: 93.75%] [G loss: 2.714830]\n",
      "2018 [D loss: 0.192701, acc.: 93.75%] [G loss: 3.282329]\n",
      "2019 [D loss: 0.226758, acc.: 90.62%] [G loss: 3.216230]\n",
      "2020 [D loss: 0.232094, acc.: 89.06%] [G loss: 3.049570]\n",
      "2021 [D loss: 0.151797, acc.: 95.31%] [G loss: 2.514297]\n",
      "2022 [D loss: 0.166640, acc.: 93.75%] [G loss: 2.569414]\n",
      "2023 [D loss: 0.222353, acc.: 92.19%] [G loss: 3.005812]\n",
      "2024 [D loss: 0.170398, acc.: 93.75%] [G loss: 2.803176]\n",
      "2025 [D loss: 0.225563, acc.: 93.75%] [G loss: 3.520088]\n",
      "2026 [D loss: 0.221295, acc.: 92.19%] [G loss: 2.980879]\n",
      "2027 [D loss: 0.149018, acc.: 92.19%] [G loss: 3.006972]\n",
      "2028 [D loss: 0.205139, acc.: 93.75%] [G loss: 2.832181]\n",
      "2029 [D loss: 0.150802, acc.: 95.31%] [G loss: 2.811816]\n",
      "2030 [D loss: 0.194228, acc.: 92.19%] [G loss: 2.897279]\n",
      "2031 [D loss: 0.169180, acc.: 95.31%] [G loss: 2.929941]\n",
      "2032 [D loss: 0.188231, acc.: 93.75%] [G loss: 2.758964]\n",
      "2033 [D loss: 0.190896, acc.: 90.62%] [G loss: 2.955248]\n",
      "2034 [D loss: 0.190472, acc.: 93.75%] [G loss: 2.603363]\n",
      "2035 [D loss: 0.178335, acc.: 93.75%] [G loss: 3.199645]\n",
      "2036 [D loss: 0.168265, acc.: 95.31%] [G loss: 3.202215]\n",
      "2037 [D loss: 0.194101, acc.: 93.75%] [G loss: 2.578933]\n",
      "2038 [D loss: 0.168769, acc.: 95.31%] [G loss: 2.812252]\n",
      "2039 [D loss: 0.209385, acc.: 92.19%] [G loss: 2.696397]\n",
      "2040 [D loss: 0.224078, acc.: 92.19%] [G loss: 3.052350]\n",
      "2041 [D loss: 0.161704, acc.: 95.31%] [G loss: 2.823253]\n",
      "2042 [D loss: 0.191952, acc.: 90.62%] [G loss: 3.107691]\n",
      "2043 [D loss: 0.203601, acc.: 92.19%] [G loss: 2.857835]\n",
      "2044 [D loss: 0.204274, acc.: 92.19%] [G loss: 3.261578]\n",
      "2045 [D loss: 0.217629, acc.: 89.06%] [G loss: 3.010516]\n",
      "2046 [D loss: 0.188624, acc.: 92.19%] [G loss: 3.341419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2047 [D loss: 0.191975, acc.: 93.75%] [G loss: 2.922546]\n",
      "2048 [D loss: 0.185475, acc.: 93.75%] [G loss: 2.944846]\n",
      "2049 [D loss: 0.228728, acc.: 92.19%] [G loss: 3.070703]\n",
      "2050 [D loss: 0.198076, acc.: 93.75%] [G loss: 3.183213]\n",
      "2051 [D loss: 0.210555, acc.: 93.75%] [G loss: 3.086614]\n",
      "2052 [D loss: 0.202171, acc.: 93.75%] [G loss: 2.906657]\n",
      "2053 [D loss: 0.202208, acc.: 93.75%] [G loss: 3.220734]\n",
      "2054 [D loss: 0.173248, acc.: 93.75%] [G loss: 3.264753]\n",
      "2055 [D loss: 0.230993, acc.: 90.62%] [G loss: 2.879034]\n",
      "2056 [D loss: 0.222349, acc.: 92.19%] [G loss: 2.530909]\n",
      "2057 [D loss: 0.186460, acc.: 92.19%] [G loss: 2.850811]\n",
      "2058 [D loss: 0.175017, acc.: 87.50%] [G loss: 2.909884]\n",
      "2059 [D loss: 0.166055, acc.: 95.31%] [G loss: 3.160247]\n",
      "2060 [D loss: 0.249226, acc.: 92.19%] [G loss: 2.846183]\n",
      "2061 [D loss: 0.216168, acc.: 90.62%] [G loss: 2.888421]\n",
      "2062 [D loss: 0.180085, acc.: 93.75%] [G loss: 2.687363]\n",
      "2063 [D loss: 0.197173, acc.: 90.62%] [G loss: 2.653302]\n",
      "2064 [D loss: 0.169885, acc.: 90.62%] [G loss: 2.853013]\n",
      "2065 [D loss: 0.244107, acc.: 89.06%] [G loss: 2.623734]\n",
      "2066 [D loss: 0.169453, acc.: 93.75%] [G loss: 2.946344]\n",
      "2067 [D loss: 0.210204, acc.: 87.50%] [G loss: 3.205565]\n",
      "2068 [D loss: 0.157539, acc.: 92.19%] [G loss: 2.900632]\n",
      "2069 [D loss: 0.250503, acc.: 90.62%] [G loss: 2.684885]\n",
      "2070 [D loss: 0.155448, acc.: 95.31%] [G loss: 3.003159]\n",
      "2071 [D loss: 0.182571, acc.: 92.19%] [G loss: 2.854959]\n",
      "2072 [D loss: 0.182758, acc.: 92.19%] [G loss: 3.347964]\n",
      "2073 [D loss: 0.185596, acc.: 90.62%] [G loss: 2.776296]\n",
      "2074 [D loss: 0.214547, acc.: 90.62%] [G loss: 3.109887]\n",
      "2075 [D loss: 0.198562, acc.: 89.06%] [G loss: 3.058418]\n",
      "2076 [D loss: 0.202378, acc.: 92.19%] [G loss: 2.778494]\n",
      "2077 [D loss: 0.204718, acc.: 90.62%] [G loss: 2.988494]\n",
      "2078 [D loss: 0.243847, acc.: 90.62%] [G loss: 3.055075]\n",
      "2079 [D loss: 0.199907, acc.: 92.19%] [G loss: 2.646356]\n",
      "2080 [D loss: 0.199277, acc.: 93.75%] [G loss: 2.549017]\n",
      "2081 [D loss: 0.202685, acc.: 93.75%] [G loss: 2.955939]\n",
      "2082 [D loss: 0.193046, acc.: 90.62%] [G loss: 3.379439]\n",
      "2083 [D loss: 0.222551, acc.: 93.75%] [G loss: 2.902499]\n",
      "2084 [D loss: 0.220013, acc.: 92.19%] [G loss: 2.648957]\n",
      "2085 [D loss: 0.184008, acc.: 92.19%] [G loss: 2.950341]\n",
      "2086 [D loss: 0.156697, acc.: 93.75%] [G loss: 2.353557]\n",
      "2087 [D loss: 0.231322, acc.: 90.62%] [G loss: 2.796389]\n",
      "2088 [D loss: 0.195390, acc.: 92.19%] [G loss: 3.229545]\n",
      "2089 [D loss: 0.214198, acc.: 90.62%] [G loss: 2.942150]\n",
      "2090 [D loss: 0.176508, acc.: 93.75%] [G loss: 2.641333]\n",
      "2091 [D loss: 0.209299, acc.: 93.75%] [G loss: 2.992923]\n",
      "2092 [D loss: 0.222713, acc.: 93.75%] [G loss: 2.900279]\n",
      "2093 [D loss: 0.210679, acc.: 92.19%] [G loss: 3.167771]\n",
      "2094 [D loss: 0.176747, acc.: 89.06%] [G loss: 2.903919]\n",
      "2095 [D loss: 0.195567, acc.: 95.31%] [G loss: 2.606883]\n",
      "2096 [D loss: 0.240252, acc.: 90.62%] [G loss: 2.899273]\n",
      "2097 [D loss: 0.210208, acc.: 89.06%] [G loss: 2.894076]\n",
      "2098 [D loss: 0.197122, acc.: 93.75%] [G loss: 2.826600]\n",
      "2099 [D loss: 0.241756, acc.: 89.06%] [G loss: 2.961232]\n",
      "2100 [D loss: 0.200766, acc.: 93.75%] [G loss: 2.756178]\n",
      "generated_data\n",
      "2101 [D loss: 0.215566, acc.: 92.19%] [G loss: 2.871215]\n",
      "2102 [D loss: 0.163690, acc.: 95.31%] [G loss: 2.800996]\n",
      "2103 [D loss: 0.205954, acc.: 92.19%] [G loss: 3.424574]\n",
      "2104 [D loss: 0.237059, acc.: 90.62%] [G loss: 3.236552]\n",
      "2105 [D loss: 0.165529, acc.: 92.19%] [G loss: 3.219277]\n",
      "2106 [D loss: 0.204608, acc.: 92.19%] [G loss: 2.842188]\n",
      "2107 [D loss: 0.220110, acc.: 93.75%] [G loss: 3.080834]\n",
      "2108 [D loss: 0.210212, acc.: 90.62%] [G loss: 3.189044]\n",
      "2109 [D loss: 0.217405, acc.: 93.75%] [G loss: 3.061590]\n",
      "2110 [D loss: 0.208395, acc.: 93.75%] [G loss: 3.135691]\n",
      "2111 [D loss: 0.265531, acc.: 92.19%] [G loss: 2.925756]\n",
      "2112 [D loss: 0.213243, acc.: 92.19%] [G loss: 2.628471]\n",
      "2113 [D loss: 0.186283, acc.: 93.75%] [G loss: 2.928489]\n",
      "2114 [D loss: 0.219731, acc.: 92.19%] [G loss: 2.686828]\n",
      "2115 [D loss: 0.166524, acc.: 93.75%] [G loss: 2.974700]\n",
      "2116 [D loss: 0.215043, acc.: 92.19%] [G loss: 2.568221]\n",
      "2117 [D loss: 0.260262, acc.: 87.50%] [G loss: 3.268359]\n",
      "2118 [D loss: 0.233756, acc.: 90.62%] [G loss: 3.118604]\n",
      "2119 [D loss: 0.164960, acc.: 95.31%] [G loss: 2.776532]\n",
      "2120 [D loss: 0.214212, acc.: 93.75%] [G loss: 2.975798]\n",
      "2121 [D loss: 0.173373, acc.: 93.75%] [G loss: 2.996901]\n",
      "2122 [D loss: 0.217331, acc.: 90.62%] [G loss: 2.454296]\n",
      "2123 [D loss: 0.204370, acc.: 93.75%] [G loss: 2.783096]\n",
      "2124 [D loss: 0.194069, acc.: 92.19%] [G loss: 3.115728]\n",
      "2125 [D loss: 0.191022, acc.: 93.75%] [G loss: 3.293231]\n",
      "2126 [D loss: 0.253401, acc.: 87.50%] [G loss: 3.198200]\n",
      "2127 [D loss: 0.191833, acc.: 92.19%] [G loss: 2.997494]\n",
      "2128 [D loss: 0.257490, acc.: 87.50%] [G loss: 3.240127]\n",
      "2129 [D loss: 0.220365, acc.: 89.06%] [G loss: 3.046525]\n",
      "2130 [D loss: 0.184642, acc.: 92.19%] [G loss: 3.067310]\n",
      "2131 [D loss: 0.193677, acc.: 93.75%] [G loss: 2.975422]\n",
      "2132 [D loss: 0.158987, acc.: 95.31%] [G loss: 2.726635]\n",
      "2133 [D loss: 0.176681, acc.: 95.31%] [G loss: 2.750727]\n",
      "2134 [D loss: 0.218547, acc.: 92.19%] [G loss: 3.258263]\n",
      "2135 [D loss: 0.191426, acc.: 93.75%] [G loss: 2.587359]\n",
      "2136 [D loss: 0.181123, acc.: 92.19%] [G loss: 3.126779]\n",
      "2137 [D loss: 0.195749, acc.: 93.75%] [G loss: 2.745728]\n",
      "2138 [D loss: 0.163792, acc.: 93.75%] [G loss: 3.092902]\n",
      "2139 [D loss: 0.186218, acc.: 92.19%] [G loss: 2.886054]\n",
      "2140 [D loss: 0.197453, acc.: 92.19%] [G loss: 3.059898]\n",
      "2141 [D loss: 0.200444, acc.: 93.75%] [G loss: 3.118115]\n",
      "2142 [D loss: 0.168128, acc.: 95.31%] [G loss: 2.940406]\n",
      "2143 [D loss: 0.219133, acc.: 90.62%] [G loss: 2.729239]\n",
      "2144 [D loss: 0.190068, acc.: 93.75%] [G loss: 2.747631]\n",
      "2145 [D loss: 0.221356, acc.: 90.62%] [G loss: 3.026453]\n",
      "2146 [D loss: 0.175774, acc.: 93.75%] [G loss: 2.960320]\n",
      "2147 [D loss: 0.252530, acc.: 90.62%] [G loss: 3.023631]\n",
      "2148 [D loss: 0.217222, acc.: 92.19%] [G loss: 3.095121]\n",
      "2149 [D loss: 0.171876, acc.: 93.75%] [G loss: 3.158665]\n",
      "2150 [D loss: 0.181642, acc.: 92.19%] [G loss: 2.767160]\n",
      "2151 [D loss: 0.177454, acc.: 93.75%] [G loss: 3.168716]\n",
      "2152 [D loss: 0.141649, acc.: 95.31%] [G loss: 2.848763]\n",
      "2153 [D loss: 0.210162, acc.: 93.75%] [G loss: 2.876572]\n",
      "2154 [D loss: 0.223861, acc.: 92.19%] [G loss: 2.766268]\n",
      "2155 [D loss: 0.224711, acc.: 89.06%] [G loss: 2.958660]\n",
      "2156 [D loss: 0.210342, acc.: 92.19%] [G loss: 2.712409]\n",
      "2157 [D loss: 0.206188, acc.: 92.19%] [G loss: 2.854046]\n",
      "2158 [D loss: 0.218365, acc.: 89.06%] [G loss: 3.111128]\n",
      "2159 [D loss: 0.175120, acc.: 95.31%] [G loss: 2.645236]\n",
      "2160 [D loss: 0.183879, acc.: 93.75%] [G loss: 2.654713]\n",
      "2161 [D loss: 0.175638, acc.: 95.31%] [G loss: 3.087726]\n",
      "2162 [D loss: 0.235502, acc.: 90.62%] [G loss: 2.955685]\n",
      "2163 [D loss: 0.177638, acc.: 95.31%] [G loss: 2.859547]\n",
      "2164 [D loss: 0.205813, acc.: 95.31%] [G loss: 3.140675]\n",
      "2165 [D loss: 0.202255, acc.: 93.75%] [G loss: 2.883528]\n",
      "2166 [D loss: 0.169935, acc.: 95.31%] [G loss: 3.089236]\n",
      "2167 [D loss: 0.216627, acc.: 93.75%] [G loss: 2.923703]\n",
      "2168 [D loss: 0.227235, acc.: 90.62%] [G loss: 3.071590]\n",
      "2169 [D loss: 0.175363, acc.: 93.75%] [G loss: 3.141294]\n",
      "2170 [D loss: 0.171003, acc.: 93.75%] [G loss: 3.029494]\n",
      "2171 [D loss: 0.175424, acc.: 95.31%] [G loss: 2.970460]\n",
      "2172 [D loss: 0.144575, acc.: 95.31%] [G loss: 2.950672]\n",
      "2173 [D loss: 0.265748, acc.: 89.06%] [G loss: 3.015745]\n",
      "2174 [D loss: 0.169270, acc.: 95.31%] [G loss: 3.387480]\n",
      "2175 [D loss: 0.201362, acc.: 92.19%] [G loss: 3.085254]\n",
      "2176 [D loss: 0.176234, acc.: 93.75%] [G loss: 3.070230]\n",
      "2177 [D loss: 0.187331, acc.: 93.75%] [G loss: 3.114668]\n",
      "2178 [D loss: 0.177731, acc.: 95.31%] [G loss: 3.008475]\n",
      "2179 [D loss: 0.158425, acc.: 95.31%] [G loss: 3.008256]\n",
      "2180 [D loss: 0.199422, acc.: 93.75%] [G loss: 2.913083]\n",
      "2181 [D loss: 0.225063, acc.: 92.19%] [G loss: 2.892153]\n",
      "2182 [D loss: 0.179499, acc.: 93.75%] [G loss: 2.777865]\n",
      "2183 [D loss: 0.217829, acc.: 90.62%] [G loss: 2.866881]\n",
      "2184 [D loss: 0.180054, acc.: 93.75%] [G loss: 3.097268]\n",
      "2185 [D loss: 0.243820, acc.: 92.19%] [G loss: 2.802278]\n",
      "2186 [D loss: 0.205565, acc.: 87.50%] [G loss: 2.790361]\n",
      "2187 [D loss: 0.172113, acc.: 93.75%] [G loss: 2.658690]\n",
      "2188 [D loss: 0.187598, acc.: 92.19%] [G loss: 2.769702]\n",
      "2189 [D loss: 0.155946, acc.: 95.31%] [G loss: 3.035287]\n",
      "2190 [D loss: 0.170161, acc.: 95.31%] [G loss: 3.068753]\n",
      "2191 [D loss: 0.224861, acc.: 92.19%] [G loss: 2.752573]\n",
      "2192 [D loss: 0.194581, acc.: 95.31%] [G loss: 3.009849]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2193 [D loss: 0.181582, acc.: 92.19%] [G loss: 2.599467]\n",
      "2194 [D loss: 0.241038, acc.: 89.06%] [G loss: 3.059426]\n",
      "2195 [D loss: 0.180261, acc.: 93.75%] [G loss: 3.310329]\n",
      "2196 [D loss: 0.183068, acc.: 93.75%] [G loss: 2.960070]\n",
      "2197 [D loss: 0.211839, acc.: 93.75%] [G loss: 2.649631]\n",
      "2198 [D loss: 0.156066, acc.: 93.75%] [G loss: 3.102090]\n",
      "2199 [D loss: 0.191280, acc.: 92.19%] [G loss: 2.710766]\n",
      "2200 [D loss: 0.192957, acc.: 90.62%] [G loss: 3.209710]\n",
      "generated_data\n",
      "2201 [D loss: 0.178893, acc.: 92.19%] [G loss: 2.967055]\n",
      "2202 [D loss: 0.168641, acc.: 93.75%] [G loss: 2.907547]\n",
      "2203 [D loss: 0.191214, acc.: 93.75%] [G loss: 2.905464]\n",
      "2204 [D loss: 0.206790, acc.: 93.75%] [G loss: 2.977443]\n",
      "2205 [D loss: 0.211962, acc.: 93.75%] [G loss: 2.915644]\n",
      "2206 [D loss: 0.206156, acc.: 89.06%] [G loss: 2.877639]\n",
      "2207 [D loss: 0.186661, acc.: 93.75%] [G loss: 3.029986]\n",
      "2208 [D loss: 0.148452, acc.: 93.75%] [G loss: 2.899581]\n",
      "2209 [D loss: 0.244840, acc.: 92.19%] [G loss: 3.045990]\n",
      "2210 [D loss: 0.172622, acc.: 95.31%] [G loss: 2.886234]\n",
      "2211 [D loss: 0.205436, acc.: 95.31%] [G loss: 2.904506]\n",
      "2212 [D loss: 0.201324, acc.: 93.75%] [G loss: 2.970398]\n",
      "2213 [D loss: 0.162861, acc.: 95.31%] [G loss: 3.102133]\n",
      "2214 [D loss: 0.266206, acc.: 90.62%] [G loss: 2.915141]\n",
      "2215 [D loss: 0.154573, acc.: 95.31%] [G loss: 2.597098]\n",
      "2216 [D loss: 0.197622, acc.: 93.75%] [G loss: 2.594117]\n",
      "2217 [D loss: 0.203954, acc.: 92.19%] [G loss: 2.630890]\n",
      "2218 [D loss: 0.231733, acc.: 89.06%] [G loss: 2.758790]\n",
      "2219 [D loss: 0.198257, acc.: 92.19%] [G loss: 2.756825]\n",
      "2220 [D loss: 0.216209, acc.: 93.75%] [G loss: 3.063031]\n",
      "2221 [D loss: 0.233807, acc.: 89.06%] [G loss: 3.129693]\n",
      "2222 [D loss: 0.201410, acc.: 92.19%] [G loss: 2.791632]\n",
      "2223 [D loss: 0.177870, acc.: 93.75%] [G loss: 2.994494]\n",
      "2224 [D loss: 0.211549, acc.: 90.62%] [G loss: 3.197181]\n",
      "2225 [D loss: 0.174644, acc.: 93.75%] [G loss: 3.118690]\n",
      "2226 [D loss: 0.145030, acc.: 93.75%] [G loss: 2.937967]\n",
      "2227 [D loss: 0.176958, acc.: 95.31%] [G loss: 3.004386]\n",
      "2228 [D loss: 0.244157, acc.: 89.06%] [G loss: 2.670107]\n",
      "2229 [D loss: 0.201653, acc.: 93.75%] [G loss: 3.220859]\n",
      "2230 [D loss: 0.188591, acc.: 92.19%] [G loss: 3.597251]\n",
      "2231 [D loss: 0.195230, acc.: 93.75%] [G loss: 3.029529]\n",
      "2232 [D loss: 0.150145, acc.: 95.31%] [G loss: 3.039741]\n",
      "2233 [D loss: 0.157835, acc.: 95.31%] [G loss: 2.831804]\n",
      "2234 [D loss: 0.196860, acc.: 92.19%] [G loss: 2.693622]\n",
      "2235 [D loss: 0.210167, acc.: 92.19%] [G loss: 3.307255]\n",
      "2236 [D loss: 0.194685, acc.: 95.31%] [G loss: 2.695340]\n",
      "2237 [D loss: 0.183708, acc.: 93.75%] [G loss: 3.081913]\n",
      "2238 [D loss: 0.185811, acc.: 93.75%] [G loss: 2.913369]\n",
      "2239 [D loss: 0.163780, acc.: 95.31%] [G loss: 2.578681]\n",
      "2240 [D loss: 0.172415, acc.: 92.19%] [G loss: 3.180889]\n",
      "2241 [D loss: 0.191821, acc.: 95.31%] [G loss: 3.186496]\n",
      "2242 [D loss: 0.145879, acc.: 95.31%] [G loss: 3.070962]\n",
      "2243 [D loss: 0.209320, acc.: 93.75%] [G loss: 2.707551]\n",
      "2244 [D loss: 0.185820, acc.: 92.19%] [G loss: 2.825020]\n",
      "2245 [D loss: 0.180503, acc.: 93.75%] [G loss: 3.043857]\n",
      "2246 [D loss: 0.180098, acc.: 93.75%] [G loss: 2.790534]\n",
      "2247 [D loss: 0.227256, acc.: 90.62%] [G loss: 2.837984]\n",
      "2248 [D loss: 0.205127, acc.: 93.75%] [G loss: 3.237894]\n",
      "2249 [D loss: 0.242244, acc.: 90.62%] [G loss: 2.987360]\n",
      "2250 [D loss: 0.191815, acc.: 92.19%] [G loss: 2.809271]\n",
      "2251 [D loss: 0.167042, acc.: 95.31%] [G loss: 2.946741]\n",
      "2252 [D loss: 0.168206, acc.: 93.75%] [G loss: 2.704921]\n",
      "2253 [D loss: 0.209976, acc.: 92.19%] [G loss: 3.063147]\n",
      "2254 [D loss: 0.213160, acc.: 90.62%] [G loss: 3.136207]\n",
      "2255 [D loss: 0.192792, acc.: 92.19%] [G loss: 3.037659]\n",
      "2256 [D loss: 0.202179, acc.: 93.75%] [G loss: 2.761241]\n",
      "2257 [D loss: 0.171619, acc.: 95.31%] [G loss: 2.901148]\n",
      "2258 [D loss: 0.177407, acc.: 92.19%] [G loss: 3.059659]\n",
      "2259 [D loss: 0.224161, acc.: 92.19%] [G loss: 2.981497]\n",
      "2260 [D loss: 0.154319, acc.: 93.75%] [G loss: 3.273699]\n",
      "2261 [D loss: 0.188185, acc.: 92.19%] [G loss: 2.681690]\n",
      "2262 [D loss: 0.241539, acc.: 90.62%] [G loss: 3.238863]\n",
      "2263 [D loss: 0.245573, acc.: 92.19%] [G loss: 3.379563]\n",
      "2264 [D loss: 0.234387, acc.: 90.62%] [G loss: 2.877045]\n",
      "2265 [D loss: 0.195213, acc.: 92.19%] [G loss: 2.520490]\n",
      "2266 [D loss: 0.173658, acc.: 93.75%] [G loss: 2.558875]\n",
      "2267 [D loss: 0.152141, acc.: 95.31%] [G loss: 2.856238]\n",
      "2268 [D loss: 0.146161, acc.: 93.75%] [G loss: 3.087250]\n",
      "2269 [D loss: 0.144574, acc.: 95.31%] [G loss: 2.889619]\n",
      "2270 [D loss: 0.173354, acc.: 95.31%] [G loss: 2.753096]\n",
      "2271 [D loss: 0.184699, acc.: 93.75%] [G loss: 2.570392]\n",
      "2272 [D loss: 0.171921, acc.: 95.31%] [G loss: 2.844161]\n",
      "2273 [D loss: 0.255441, acc.: 87.50%] [G loss: 2.956903]\n",
      "2274 [D loss: 0.167114, acc.: 95.31%] [G loss: 2.924562]\n",
      "2275 [D loss: 0.191481, acc.: 93.75%] [G loss: 2.907965]\n",
      "2276 [D loss: 0.218774, acc.: 93.75%] [G loss: 3.099429]\n",
      "2277 [D loss: 0.158247, acc.: 95.31%] [G loss: 2.758176]\n",
      "2278 [D loss: 0.188105, acc.: 95.31%] [G loss: 3.019602]\n",
      "2279 [D loss: 0.220038, acc.: 92.19%] [G loss: 2.727870]\n",
      "2280 [D loss: 0.223396, acc.: 90.62%] [G loss: 2.727733]\n",
      "2281 [D loss: 0.208646, acc.: 93.75%] [G loss: 2.937310]\n",
      "2282 [D loss: 0.227332, acc.: 90.62%] [G loss: 3.010429]\n",
      "2283 [D loss: 0.162890, acc.: 93.75%] [G loss: 3.004955]\n",
      "2284 [D loss: 0.179189, acc.: 93.75%] [G loss: 2.976808]\n",
      "2285 [D loss: 0.200868, acc.: 93.75%] [G loss: 2.780073]\n",
      "2286 [D loss: 0.218600, acc.: 90.62%] [G loss: 2.838912]\n",
      "2287 [D loss: 0.204989, acc.: 93.75%] [G loss: 2.935116]\n",
      "2288 [D loss: 0.192927, acc.: 92.19%] [G loss: 2.963921]\n",
      "2289 [D loss: 0.273647, acc.: 85.94%] [G loss: 2.772127]\n",
      "2290 [D loss: 0.229516, acc.: 90.62%] [G loss: 2.872655]\n",
      "2291 [D loss: 0.189977, acc.: 93.75%] [G loss: 2.747163]\n",
      "2292 [D loss: 0.256817, acc.: 89.06%] [G loss: 2.805732]\n",
      "2293 [D loss: 0.190117, acc.: 95.31%] [G loss: 3.112391]\n",
      "2294 [D loss: 0.221845, acc.: 89.06%] [G loss: 2.866635]\n",
      "2295 [D loss: 0.186431, acc.: 93.75%] [G loss: 2.968624]\n",
      "2296 [D loss: 0.184351, acc.: 93.75%] [G loss: 3.053148]\n",
      "2297 [D loss: 0.206776, acc.: 92.19%] [G loss: 3.006534]\n",
      "2298 [D loss: 0.169046, acc.: 95.31%] [G loss: 3.018809]\n",
      "2299 [D loss: 0.182372, acc.: 93.75%] [G loss: 2.628938]\n",
      "2300 [D loss: 0.174439, acc.: 95.31%] [G loss: 2.807928]\n",
      "generated_data\n",
      "2301 [D loss: 0.216371, acc.: 95.31%] [G loss: 2.624918]\n",
      "2302 [D loss: 0.180344, acc.: 93.75%] [G loss: 3.050284]\n",
      "2303 [D loss: 0.200797, acc.: 92.19%] [G loss: 3.065067]\n",
      "2304 [D loss: 0.201657, acc.: 93.75%] [G loss: 2.991932]\n",
      "2305 [D loss: 0.173220, acc.: 93.75%] [G loss: 3.101470]\n",
      "2306 [D loss: 0.159312, acc.: 93.75%] [G loss: 3.024488]\n",
      "2307 [D loss: 0.198996, acc.: 92.19%] [G loss: 2.500845]\n",
      "2308 [D loss: 0.211427, acc.: 92.19%] [G loss: 2.915082]\n",
      "2309 [D loss: 0.163910, acc.: 93.75%] [G loss: 2.901240]\n",
      "2310 [D loss: 0.174986, acc.: 93.75%] [G loss: 2.900372]\n",
      "2311 [D loss: 0.164987, acc.: 93.75%] [G loss: 2.673452]\n",
      "2312 [D loss: 0.172110, acc.: 95.31%] [G loss: 2.817636]\n",
      "2313 [D loss: 0.158527, acc.: 95.31%] [G loss: 2.964190]\n",
      "2314 [D loss: 0.181932, acc.: 92.19%] [G loss: 3.009507]\n",
      "2315 [D loss: 0.238036, acc.: 90.62%] [G loss: 2.894610]\n",
      "2316 [D loss: 0.204069, acc.: 92.19%] [G loss: 3.017305]\n",
      "2317 [D loss: 0.201991, acc.: 92.19%] [G loss: 3.135172]\n",
      "2318 [D loss: 0.153655, acc.: 95.31%] [G loss: 2.860894]\n",
      "2319 [D loss: 0.157773, acc.: 95.31%] [G loss: 2.684989]\n",
      "2320 [D loss: 0.259584, acc.: 89.06%] [G loss: 2.969381]\n",
      "2321 [D loss: 0.209791, acc.: 92.19%] [G loss: 3.009763]\n",
      "2322 [D loss: 0.210261, acc.: 90.62%] [G loss: 2.636821]\n",
      "2323 [D loss: 0.172429, acc.: 95.31%] [G loss: 2.796953]\n",
      "2324 [D loss: 0.226167, acc.: 87.50%] [G loss: 3.385543]\n",
      "2325 [D loss: 0.224125, acc.: 90.62%] [G loss: 3.212882]\n",
      "2326 [D loss: 0.228080, acc.: 90.62%] [G loss: 3.398886]\n",
      "2327 [D loss: 0.221313, acc.: 93.75%] [G loss: 2.808467]\n",
      "2328 [D loss: 0.185760, acc.: 92.19%] [G loss: 3.106922]\n",
      "2329 [D loss: 0.174479, acc.: 95.31%] [G loss: 2.980793]\n",
      "2330 [D loss: 0.167395, acc.: 93.75%] [G loss: 2.578109]\n",
      "2331 [D loss: 0.214584, acc.: 93.75%] [G loss: 2.921516]\n",
      "2332 [D loss: 0.186010, acc.: 95.31%] [G loss: 2.802849]\n",
      "2333 [D loss: 0.190956, acc.: 95.31%] [G loss: 3.061532]\n",
      "2334 [D loss: 0.181406, acc.: 93.75%] [G loss: 2.459301]\n",
      "2335 [D loss: 0.179303, acc.: 92.19%] [G loss: 2.946255]\n",
      "2336 [D loss: 0.203121, acc.: 90.62%] [G loss: 3.040506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2337 [D loss: 0.157800, acc.: 93.75%] [G loss: 3.555608]\n",
      "2338 [D loss: 0.167195, acc.: 95.31%] [G loss: 2.986961]\n",
      "2339 [D loss: 0.253926, acc.: 89.06%] [G loss: 3.071901]\n",
      "2340 [D loss: 0.177936, acc.: 93.75%] [G loss: 3.269940]\n",
      "2341 [D loss: 0.170640, acc.: 95.31%] [G loss: 2.972674]\n",
      "2342 [D loss: 0.155226, acc.: 95.31%] [G loss: 2.682166]\n",
      "2343 [D loss: 0.191618, acc.: 92.19%] [G loss: 2.919527]\n",
      "2344 [D loss: 0.239195, acc.: 89.06%] [G loss: 3.512115]\n",
      "2345 [D loss: 0.219636, acc.: 90.62%] [G loss: 3.409125]\n",
      "2346 [D loss: 0.205600, acc.: 90.62%] [G loss: 2.653157]\n",
      "2347 [D loss: 0.203718, acc.: 90.62%] [G loss: 2.667425]\n",
      "2348 [D loss: 0.207377, acc.: 92.19%] [G loss: 2.767521]\n",
      "2349 [D loss: 0.198786, acc.: 93.75%] [G loss: 3.172176]\n",
      "2350 [D loss: 0.179602, acc.: 92.19%] [G loss: 2.870659]\n",
      "2351 [D loss: 0.232500, acc.: 89.06%] [G loss: 3.040327]\n",
      "2352 [D loss: 0.184793, acc.: 89.06%] [G loss: 3.386891]\n",
      "2353 [D loss: 0.281050, acc.: 87.50%] [G loss: 3.062752]\n",
      "2354 [D loss: 0.219781, acc.: 92.19%] [G loss: 2.955777]\n",
      "2355 [D loss: 0.231566, acc.: 90.62%] [G loss: 3.004263]\n",
      "2356 [D loss: 0.147578, acc.: 95.31%] [G loss: 2.984046]\n",
      "2357 [D loss: 0.187451, acc.: 93.75%] [G loss: 3.021850]\n",
      "2358 [D loss: 0.199891, acc.: 90.62%] [G loss: 2.964957]\n",
      "2359 [D loss: 0.154429, acc.: 95.31%] [G loss: 3.108059]\n",
      "2360 [D loss: 0.195515, acc.: 93.75%] [G loss: 2.636781]\n",
      "2361 [D loss: 0.173374, acc.: 95.31%] [G loss: 2.667393]\n",
      "2362 [D loss: 0.166951, acc.: 95.31%] [G loss: 2.711438]\n",
      "2363 [D loss: 0.181458, acc.: 93.75%] [G loss: 2.893341]\n",
      "2364 [D loss: 0.166287, acc.: 95.31%] [G loss: 2.914703]\n",
      "2365 [D loss: 0.206747, acc.: 92.19%] [G loss: 3.082198]\n",
      "2366 [D loss: 0.180994, acc.: 93.75%] [G loss: 2.828815]\n",
      "2367 [D loss: 0.190015, acc.: 92.19%] [G loss: 2.965067]\n",
      "2368 [D loss: 0.161818, acc.: 95.31%] [G loss: 3.020332]\n",
      "2369 [D loss: 0.159868, acc.: 93.75%] [G loss: 3.015870]\n",
      "2370 [D loss: 0.184811, acc.: 92.19%] [G loss: 2.571178]\n",
      "2371 [D loss: 0.203973, acc.: 95.31%] [G loss: 2.813193]\n",
      "2372 [D loss: 0.163104, acc.: 95.31%] [G loss: 3.213068]\n",
      "2373 [D loss: 0.156990, acc.: 95.31%] [G loss: 2.947442]\n",
      "2374 [D loss: 0.213131, acc.: 92.19%] [G loss: 2.798771]\n",
      "2375 [D loss: 0.189271, acc.: 95.31%] [G loss: 3.406836]\n",
      "2376 [D loss: 0.176527, acc.: 93.75%] [G loss: 2.630828]\n",
      "2377 [D loss: 0.171709, acc.: 93.75%] [G loss: 2.918118]\n",
      "2378 [D loss: 0.173506, acc.: 93.75%] [G loss: 3.193414]\n",
      "2379 [D loss: 0.200782, acc.: 93.75%] [G loss: 3.165157]\n",
      "2380 [D loss: 0.174840, acc.: 93.75%] [G loss: 2.990284]\n",
      "2381 [D loss: 0.190480, acc.: 92.19%] [G loss: 2.942837]\n",
      "2382 [D loss: 0.190070, acc.: 95.31%] [G loss: 3.502690]\n",
      "2383 [D loss: 0.176696, acc.: 92.19%] [G loss: 3.086279]\n",
      "2384 [D loss: 0.195947, acc.: 93.75%] [G loss: 3.348549]\n",
      "2385 [D loss: 0.177793, acc.: 93.75%] [G loss: 2.953780]\n",
      "2386 [D loss: 0.200683, acc.: 93.75%] [G loss: 3.188435]\n",
      "2387 [D loss: 0.204870, acc.: 93.75%] [G loss: 3.071374]\n",
      "2388 [D loss: 0.224628, acc.: 92.19%] [G loss: 3.494801]\n",
      "2389 [D loss: 0.171481, acc.: 93.75%] [G loss: 3.286990]\n",
      "2390 [D loss: 0.162474, acc.: 95.31%] [G loss: 3.273031]\n",
      "2391 [D loss: 0.181694, acc.: 95.31%] [G loss: 3.160662]\n",
      "2392 [D loss: 0.177674, acc.: 93.75%] [G loss: 3.026527]\n",
      "2393 [D loss: 0.180779, acc.: 93.75%] [G loss: 3.089999]\n",
      "2394 [D loss: 0.199545, acc.: 92.19%] [G loss: 3.182202]\n",
      "2395 [D loss: 0.194474, acc.: 93.75%] [G loss: 3.056659]\n",
      "2396 [D loss: 0.187704, acc.: 95.31%] [G loss: 3.256932]\n",
      "2397 [D loss: 0.188400, acc.: 95.31%] [G loss: 2.811747]\n",
      "2398 [D loss: 0.180171, acc.: 92.19%] [G loss: 3.073216]\n",
      "2399 [D loss: 0.189942, acc.: 93.75%] [G loss: 2.907094]\n",
      "2400 [D loss: 0.189265, acc.: 93.75%] [G loss: 3.340670]\n",
      "generated_data\n",
      "2401 [D loss: 0.155777, acc.: 92.19%] [G loss: 3.639083]\n",
      "2402 [D loss: 0.182730, acc.: 93.75%] [G loss: 2.525553]\n",
      "2403 [D loss: 0.168332, acc.: 95.31%] [G loss: 2.785728]\n",
      "2404 [D loss: 0.136521, acc.: 95.31%] [G loss: 3.588412]\n",
      "2405 [D loss: 0.219110, acc.: 93.75%] [G loss: 2.601196]\n",
      "2406 [D loss: 0.184904, acc.: 92.19%] [G loss: 3.986102]\n",
      "2407 [D loss: 0.170866, acc.: 93.75%] [G loss: 2.972502]\n",
      "2408 [D loss: 0.198701, acc.: 92.19%] [G loss: 3.052381]\n",
      "2409 [D loss: 0.166657, acc.: 93.75%] [G loss: 3.075424]\n",
      "2410 [D loss: 0.181993, acc.: 92.19%] [G loss: 3.246037]\n",
      "2411 [D loss: 0.181153, acc.: 92.19%] [G loss: 2.851690]\n",
      "2412 [D loss: 0.230516, acc.: 92.19%] [G loss: 2.908388]\n",
      "2413 [D loss: 0.168864, acc.: 92.19%] [G loss: 3.082093]\n",
      "2414 [D loss: 0.162610, acc.: 93.75%] [G loss: 2.866437]\n",
      "2415 [D loss: 0.182167, acc.: 95.31%] [G loss: 2.497839]\n",
      "2416 [D loss: 0.161065, acc.: 95.31%] [G loss: 3.084787]\n",
      "2417 [D loss: 0.174349, acc.: 92.19%] [G loss: 3.082343]\n",
      "2418 [D loss: 0.185477, acc.: 93.75%] [G loss: 2.744210]\n",
      "2419 [D loss: 0.173664, acc.: 95.31%] [G loss: 2.852744]\n",
      "2420 [D loss: 0.197066, acc.: 93.75%] [G loss: 2.932951]\n",
      "2421 [D loss: 0.167202, acc.: 95.31%] [G loss: 2.694484]\n",
      "2422 [D loss: 0.156751, acc.: 95.31%] [G loss: 3.267304]\n",
      "2423 [D loss: 0.184675, acc.: 95.31%] [G loss: 2.793510]\n",
      "2424 [D loss: 0.140106, acc.: 93.75%] [G loss: 3.148320]\n",
      "2425 [D loss: 0.197665, acc.: 92.19%] [G loss: 2.962078]\n",
      "2426 [D loss: 0.187010, acc.: 95.31%] [G loss: 3.082136]\n",
      "2427 [D loss: 0.242201, acc.: 89.06%] [G loss: 3.059927]\n",
      "2428 [D loss: 0.188917, acc.: 92.19%] [G loss: 2.790369]\n",
      "2429 [D loss: 0.226624, acc.: 92.19%] [G loss: 2.666195]\n",
      "2430 [D loss: 0.199175, acc.: 93.75%] [G loss: 3.214995]\n",
      "2431 [D loss: 0.194503, acc.: 93.75%] [G loss: 3.342187]\n",
      "2432 [D loss: 0.167449, acc.: 93.75%] [G loss: 2.978381]\n",
      "2433 [D loss: 0.177079, acc.: 95.31%] [G loss: 3.080633]\n",
      "2434 [D loss: 0.235156, acc.: 92.19%] [G loss: 3.199381]\n",
      "2435 [D loss: 0.186064, acc.: 93.75%] [G loss: 3.195078]\n",
      "2436 [D loss: 0.203801, acc.: 93.75%] [G loss: 3.108207]\n",
      "2437 [D loss: 0.207695, acc.: 93.75%] [G loss: 2.946066]\n",
      "2438 [D loss: 0.173368, acc.: 95.31%] [G loss: 2.812460]\n",
      "2439 [D loss: 0.204875, acc.: 92.19%] [G loss: 2.814614]\n",
      "2440 [D loss: 0.184768, acc.: 93.75%] [G loss: 2.979207]\n",
      "2441 [D loss: 0.248875, acc.: 92.19%] [G loss: 3.274061]\n",
      "2442 [D loss: 0.236617, acc.: 89.06%] [G loss: 2.925508]\n",
      "2443 [D loss: 0.200876, acc.: 92.19%] [G loss: 3.738859]\n",
      "2444 [D loss: 0.179052, acc.: 93.75%] [G loss: 3.094937]\n",
      "2445 [D loss: 0.158164, acc.: 95.31%] [G loss: 2.877458]\n",
      "2446 [D loss: 0.227834, acc.: 89.06%] [G loss: 3.154248]\n",
      "2447 [D loss: 0.244799, acc.: 93.75%] [G loss: 3.312547]\n",
      "2448 [D loss: 0.244179, acc.: 90.62%] [G loss: 2.508316]\n",
      "2449 [D loss: 0.170799, acc.: 93.75%] [G loss: 2.817184]\n",
      "2450 [D loss: 0.156400, acc.: 95.31%] [G loss: 2.991418]\n",
      "2451 [D loss: 0.192809, acc.: 92.19%] [G loss: 2.759622]\n",
      "2452 [D loss: 0.152575, acc.: 95.31%] [G loss: 2.575992]\n",
      "2453 [D loss: 0.191105, acc.: 93.75%] [G loss: 2.374561]\n",
      "2454 [D loss: 0.208284, acc.: 93.75%] [G loss: 2.682128]\n",
      "2455 [D loss: 0.173437, acc.: 95.31%] [G loss: 2.940063]\n",
      "2456 [D loss: 0.172096, acc.: 93.75%] [G loss: 2.998272]\n",
      "2457 [D loss: 0.147415, acc.: 95.31%] [G loss: 2.943142]\n",
      "2458 [D loss: 0.202545, acc.: 93.75%] [G loss: 2.948179]\n",
      "2459 [D loss: 0.195540, acc.: 92.19%] [G loss: 2.987412]\n",
      "2460 [D loss: 0.189139, acc.: 93.75%] [G loss: 3.392604]\n",
      "2461 [D loss: 0.185094, acc.: 93.75%] [G loss: 2.766773]\n",
      "2462 [D loss: 0.175547, acc.: 95.31%] [G loss: 2.858276]\n",
      "2463 [D loss: 0.221844, acc.: 93.75%] [G loss: 3.080921]\n",
      "2464 [D loss: 0.174695, acc.: 95.31%] [G loss: 3.301505]\n",
      "2465 [D loss: 0.202337, acc.: 92.19%] [G loss: 3.203139]\n",
      "2466 [D loss: 0.160622, acc.: 95.31%] [G loss: 2.830581]\n",
      "2467 [D loss: 0.203077, acc.: 92.19%] [G loss: 3.178502]\n",
      "2468 [D loss: 0.172293, acc.: 93.75%] [G loss: 3.009844]\n",
      "2469 [D loss: 0.213862, acc.: 92.19%] [G loss: 3.048532]\n",
      "2470 [D loss: 0.189909, acc.: 93.75%] [G loss: 2.967719]\n",
      "2471 [D loss: 0.188355, acc.: 93.75%] [G loss: 2.793511]\n",
      "2472 [D loss: 0.170238, acc.: 92.19%] [G loss: 3.003754]\n",
      "2473 [D loss: 0.160766, acc.: 95.31%] [G loss: 3.045294]\n",
      "2474 [D loss: 0.230539, acc.: 90.62%] [G loss: 3.188477]\n",
      "2475 [D loss: 0.208217, acc.: 92.19%] [G loss: 2.893014]\n",
      "2476 [D loss: 0.252966, acc.: 92.19%] [G loss: 3.008987]\n",
      "2477 [D loss: 0.206525, acc.: 93.75%] [G loss: 3.021445]\n",
      "2478 [D loss: 0.189486, acc.: 93.75%] [G loss: 3.015578]\n",
      "2479 [D loss: 0.174318, acc.: 93.75%] [G loss: 2.912965]\n",
      "2480 [D loss: 0.241988, acc.: 90.62%] [G loss: 3.337114]\n",
      "2481 [D loss: 0.186636, acc.: 95.31%] [G loss: 3.433594]\n",
      "2482 [D loss: 0.209821, acc.: 92.19%] [G loss: 3.118885]\n",
      "2483 [D loss: 0.153657, acc.: 95.31%] [G loss: 2.969703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2484 [D loss: 0.177162, acc.: 95.31%] [G loss: 2.823704]\n",
      "2485 [D loss: 0.221628, acc.: 92.19%] [G loss: 2.816759]\n",
      "2486 [D loss: 0.157754, acc.: 93.75%] [G loss: 3.373863]\n",
      "2487 [D loss: 0.206427, acc.: 92.19%] [G loss: 2.518339]\n",
      "2488 [D loss: 0.166543, acc.: 95.31%] [G loss: 2.914578]\n",
      "2489 [D loss: 0.171396, acc.: 95.31%] [G loss: 3.053786]\n",
      "2490 [D loss: 0.183822, acc.: 92.19%] [G loss: 2.785276]\n",
      "2491 [D loss: 0.156109, acc.: 95.31%] [G loss: 2.826704]\n",
      "2492 [D loss: 0.187608, acc.: 92.19%] [G loss: 2.953102]\n",
      "2493 [D loss: 0.191404, acc.: 92.19%] [G loss: 2.628802]\n",
      "2494 [D loss: 0.157005, acc.: 92.19%] [G loss: 2.887132]\n",
      "2495 [D loss: 0.179269, acc.: 95.31%] [G loss: 2.728932]\n",
      "2496 [D loss: 0.206005, acc.: 90.62%] [G loss: 3.349644]\n",
      "2497 [D loss: 0.215997, acc.: 90.62%] [G loss: 2.792641]\n",
      "2498 [D loss: 0.202292, acc.: 93.75%] [G loss: 3.029006]\n",
      "2499 [D loss: 0.184770, acc.: 93.75%] [G loss: 3.092315]\n",
      "2500 [D loss: 0.176751, acc.: 93.75%] [G loss: 3.227364]\n",
      "generated_data\n",
      "2501 [D loss: 0.170251, acc.: 95.31%] [G loss: 3.061378]\n",
      "2502 [D loss: 0.165048, acc.: 95.31%] [G loss: 2.923629]\n",
      "2503 [D loss: 0.175623, acc.: 93.75%] [G loss: 3.034609]\n",
      "2504 [D loss: 0.180113, acc.: 93.75%] [G loss: 3.029267]\n",
      "2505 [D loss: 0.144714, acc.: 95.31%] [G loss: 2.774131]\n",
      "2506 [D loss: 0.181973, acc.: 93.75%] [G loss: 2.601991]\n",
      "2507 [D loss: 0.165209, acc.: 95.31%] [G loss: 3.379325]\n",
      "2508 [D loss: 0.169427, acc.: 95.31%] [G loss: 2.999645]\n",
      "2509 [D loss: 0.192920, acc.: 93.75%] [G loss: 2.956696]\n",
      "2510 [D loss: 0.205911, acc.: 90.62%] [G loss: 2.997484]\n",
      "2511 [D loss: 0.173940, acc.: 93.75%] [G loss: 2.882150]\n",
      "2512 [D loss: 0.164446, acc.: 95.31%] [G loss: 3.114503]\n",
      "2513 [D loss: 0.133525, acc.: 95.31%] [G loss: 2.708647]\n",
      "2514 [D loss: 0.177579, acc.: 93.75%] [G loss: 2.885485]\n",
      "2515 [D loss: 0.215323, acc.: 93.75%] [G loss: 3.067062]\n",
      "2516 [D loss: 0.167528, acc.: 93.75%] [G loss: 2.752465]\n",
      "2517 [D loss: 0.222438, acc.: 92.19%] [G loss: 2.988284]\n",
      "2518 [D loss: 0.216882, acc.: 93.75%] [G loss: 2.946847]\n",
      "2519 [D loss: 0.159291, acc.: 95.31%] [G loss: 2.861650]\n",
      "2520 [D loss: 0.211646, acc.: 92.19%] [G loss: 3.024416]\n",
      "2521 [D loss: 0.196680, acc.: 92.19%] [G loss: 3.145147]\n",
      "2522 [D loss: 0.197274, acc.: 92.19%] [G loss: 2.620198]\n",
      "2523 [D loss: 0.216399, acc.: 95.31%] [G loss: 2.945971]\n",
      "2524 [D loss: 0.175554, acc.: 93.75%] [G loss: 3.114792]\n",
      "2525 [D loss: 0.175357, acc.: 93.75%] [G loss: 3.193336]\n",
      "2526 [D loss: 0.180348, acc.: 95.31%] [G loss: 3.094923]\n",
      "2527 [D loss: 0.217471, acc.: 90.62%] [G loss: 3.156889]\n",
      "2528 [D loss: 0.173247, acc.: 93.75%] [G loss: 2.942913]\n",
      "2529 [D loss: 0.150938, acc.: 93.75%] [G loss: 3.220073]\n",
      "2530 [D loss: 0.187055, acc.: 93.75%] [G loss: 3.206084]\n",
      "2531 [D loss: 0.169106, acc.: 95.31%] [G loss: 2.869029]\n",
      "2532 [D loss: 0.181479, acc.: 92.19%] [G loss: 3.627428]\n",
      "2533 [D loss: 0.248116, acc.: 90.62%] [G loss: 2.714345]\n",
      "2534 [D loss: 0.185184, acc.: 92.19%] [G loss: 3.488059]\n",
      "2535 [D loss: 0.193670, acc.: 92.19%] [G loss: 2.858840]\n",
      "2536 [D loss: 0.166350, acc.: 93.75%] [G loss: 3.066508]\n",
      "2537 [D loss: 0.191244, acc.: 93.75%] [G loss: 3.277107]\n",
      "2538 [D loss: 0.217604, acc.: 92.19%] [G loss: 3.312937]\n",
      "2539 [D loss: 0.208764, acc.: 90.62%] [G loss: 3.184564]\n",
      "2540 [D loss: 0.176844, acc.: 93.75%] [G loss: 2.972980]\n",
      "2541 [D loss: 0.216679, acc.: 92.19%] [G loss: 3.198991]\n",
      "2542 [D loss: 0.184895, acc.: 93.75%] [G loss: 2.641164]\n",
      "2543 [D loss: 0.187594, acc.: 95.31%] [G loss: 3.062288]\n",
      "2544 [D loss: 0.253303, acc.: 89.06%] [G loss: 3.256264]\n",
      "2545 [D loss: 0.215975, acc.: 92.19%] [G loss: 3.167468]\n",
      "2546 [D loss: 0.234603, acc.: 92.19%] [G loss: 2.870933]\n",
      "2547 [D loss: 0.162360, acc.: 95.31%] [G loss: 2.956295]\n",
      "2548 [D loss: 0.167855, acc.: 92.19%] [G loss: 2.864125]\n",
      "2549 [D loss: 0.190719, acc.: 90.62%] [G loss: 3.110664]\n",
      "2550 [D loss: 0.162494, acc.: 93.75%] [G loss: 2.956673]\n",
      "2551 [D loss: 0.214487, acc.: 92.19%] [G loss: 3.471294]\n",
      "2552 [D loss: 0.180887, acc.: 92.19%] [G loss: 3.502977]\n",
      "2553 [D loss: 0.186637, acc.: 92.19%] [G loss: 3.228024]\n",
      "2554 [D loss: 0.203142, acc.: 93.75%] [G loss: 2.653382]\n",
      "2555 [D loss: 0.205969, acc.: 92.19%] [G loss: 3.260924]\n",
      "2556 [D loss: 0.210547, acc.: 93.75%] [G loss: 3.109798]\n",
      "2557 [D loss: 0.192195, acc.: 92.19%] [G loss: 2.934050]\n",
      "2558 [D loss: 0.216545, acc.: 93.75%] [G loss: 2.639407]\n",
      "2559 [D loss: 0.200486, acc.: 92.19%] [G loss: 2.946274]\n",
      "2560 [D loss: 0.198004, acc.: 92.19%] [G loss: 3.386128]\n",
      "2561 [D loss: 0.220671, acc.: 89.06%] [G loss: 3.049900]\n",
      "2562 [D loss: 0.197864, acc.: 92.19%] [G loss: 3.011242]\n",
      "2563 [D loss: 0.195146, acc.: 92.19%] [G loss: 2.818671]\n",
      "2564 [D loss: 0.200700, acc.: 93.75%] [G loss: 2.997609]\n",
      "2565 [D loss: 0.181128, acc.: 92.19%] [G loss: 3.065060]\n",
      "2566 [D loss: 0.214253, acc.: 92.19%] [G loss: 2.575415]\n",
      "2567 [D loss: 0.220077, acc.: 92.19%] [G loss: 3.116946]\n",
      "2568 [D loss: 0.168429, acc.: 95.31%] [G loss: 3.107328]\n",
      "2569 [D loss: 0.206774, acc.: 93.75%] [G loss: 2.634799]\n",
      "2570 [D loss: 0.173366, acc.: 92.19%] [G loss: 2.959088]\n",
      "2571 [D loss: 0.190121, acc.: 92.19%] [G loss: 3.007222]\n",
      "2572 [D loss: 0.213372, acc.: 89.06%] [G loss: 3.209153]\n",
      "2573 [D loss: 0.168918, acc.: 95.31%] [G loss: 3.076468]\n",
      "2574 [D loss: 0.214734, acc.: 92.19%] [G loss: 2.768778]\n",
      "2575 [D loss: 0.182546, acc.: 93.75%] [G loss: 3.123939]\n",
      "2576 [D loss: 0.177779, acc.: 95.31%] [G loss: 2.638848]\n",
      "2577 [D loss: 0.211359, acc.: 92.19%] [G loss: 2.561670]\n",
      "2578 [D loss: 0.208485, acc.: 93.75%] [G loss: 2.918884]\n",
      "2579 [D loss: 0.158596, acc.: 95.31%] [G loss: 3.013213]\n",
      "2580 [D loss: 0.204591, acc.: 93.75%] [G loss: 2.554372]\n",
      "2581 [D loss: 0.208169, acc.: 93.75%] [G loss: 3.087765]\n",
      "2582 [D loss: 0.171314, acc.: 92.19%] [G loss: 2.929433]\n",
      "2583 [D loss: 0.203072, acc.: 92.19%] [G loss: 3.207250]\n",
      "2584 [D loss: 0.185087, acc.: 92.19%] [G loss: 2.636318]\n",
      "2585 [D loss: 0.214751, acc.: 92.19%] [G loss: 2.962176]\n",
      "2586 [D loss: 0.188370, acc.: 93.75%] [G loss: 3.040105]\n",
      "2587 [D loss: 0.181589, acc.: 93.75%] [G loss: 2.861765]\n",
      "2588 [D loss: 0.190801, acc.: 92.19%] [G loss: 3.032815]\n",
      "2589 [D loss: 0.180151, acc.: 90.62%] [G loss: 2.837226]\n",
      "2590 [D loss: 0.186952, acc.: 92.19%] [G loss: 3.520952]\n",
      "2591 [D loss: 0.187458, acc.: 95.31%] [G loss: 3.258997]\n",
      "2592 [D loss: 0.178691, acc.: 92.19%] [G loss: 3.023384]\n",
      "2593 [D loss: 0.216721, acc.: 90.62%] [G loss: 3.366886]\n",
      "2594 [D loss: 0.217432, acc.: 89.06%] [G loss: 2.928555]\n",
      "2595 [D loss: 0.152422, acc.: 95.31%] [G loss: 3.090842]\n",
      "2596 [D loss: 0.225956, acc.: 92.19%] [G loss: 2.926173]\n",
      "2597 [D loss: 0.153430, acc.: 95.31%] [G loss: 3.141185]\n",
      "2598 [D loss: 0.186727, acc.: 95.31%] [G loss: 3.070169]\n",
      "2599 [D loss: 0.160084, acc.: 95.31%] [G loss: 3.151341]\n",
      "2600 [D loss: 0.209287, acc.: 92.19%] [G loss: 3.226424]\n",
      "generated_data\n",
      "2601 [D loss: 0.201855, acc.: 93.75%] [G loss: 3.191414]\n",
      "2602 [D loss: 0.219543, acc.: 92.19%] [G loss: 3.407086]\n",
      "2603 [D loss: 0.194011, acc.: 92.19%] [G loss: 3.040210]\n",
      "2604 [D loss: 0.195813, acc.: 95.31%] [G loss: 2.730560]\n",
      "2605 [D loss: 0.164069, acc.: 95.31%] [G loss: 2.984906]\n",
      "2606 [D loss: 0.178749, acc.: 93.75%] [G loss: 3.292170]\n",
      "2607 [D loss: 0.206192, acc.: 92.19%] [G loss: 2.901471]\n",
      "2608 [D loss: 0.205372, acc.: 92.19%] [G loss: 3.588102]\n",
      "2609 [D loss: 0.178787, acc.: 95.31%] [G loss: 2.870865]\n",
      "2610 [D loss: 0.168822, acc.: 95.31%] [G loss: 2.750057]\n",
      "2611 [D loss: 0.210632, acc.: 92.19%] [G loss: 2.657502]\n",
      "2612 [D loss: 0.199390, acc.: 93.75%] [G loss: 2.975070]\n",
      "2613 [D loss: 0.159998, acc.: 95.31%] [G loss: 3.102817]\n",
      "2614 [D loss: 0.173541, acc.: 93.75%] [G loss: 2.878587]\n",
      "2615 [D loss: 0.152067, acc.: 95.31%] [G loss: 2.589201]\n",
      "2616 [D loss: 0.195887, acc.: 93.75%] [G loss: 2.616266]\n",
      "2617 [D loss: 0.204745, acc.: 92.19%] [G loss: 2.611175]\n",
      "2618 [D loss: 0.162313, acc.: 95.31%] [G loss: 3.073414]\n",
      "2619 [D loss: 0.204152, acc.: 90.62%] [G loss: 3.346719]\n",
      "2620 [D loss: 0.234766, acc.: 90.62%] [G loss: 3.201697]\n",
      "2621 [D loss: 0.197076, acc.: 93.75%] [G loss: 3.154049]\n",
      "2622 [D loss: 0.211022, acc.: 93.75%] [G loss: 2.716186]\n",
      "2623 [D loss: 0.189405, acc.: 95.31%] [G loss: 2.885262]\n",
      "2624 [D loss: 0.141443, acc.: 95.31%] [G loss: 2.515084]\n",
      "2625 [D loss: 0.287188, acc.: 90.62%] [G loss: 2.993515]\n",
      "2626 [D loss: 0.161443, acc.: 93.75%] [G loss: 3.433609]\n",
      "2627 [D loss: 0.235422, acc.: 87.50%] [G loss: 3.213419]\n",
      "2628 [D loss: 0.156625, acc.: 93.75%] [G loss: 2.835325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2629 [D loss: 0.152812, acc.: 95.31%] [G loss: 3.410864]\n",
      "2630 [D loss: 0.179906, acc.: 93.75%] [G loss: 2.965764]\n",
      "2631 [D loss: 0.207058, acc.: 93.75%] [G loss: 2.959008]\n",
      "2632 [D loss: 0.180991, acc.: 93.75%] [G loss: 2.891646]\n",
      "2633 [D loss: 0.190431, acc.: 93.75%] [G loss: 2.787603]\n",
      "2634 [D loss: 0.209120, acc.: 90.62%] [G loss: 2.881807]\n",
      "2635 [D loss: 0.170327, acc.: 95.31%] [G loss: 2.728409]\n",
      "2636 [D loss: 0.177898, acc.: 92.19%] [G loss: 2.614364]\n",
      "2637 [D loss: 0.216383, acc.: 87.50%] [G loss: 3.194046]\n",
      "2638 [D loss: 0.189173, acc.: 95.31%] [G loss: 2.957227]\n",
      "2639 [D loss: 0.200932, acc.: 92.19%] [G loss: 2.680606]\n",
      "2640 [D loss: 0.153845, acc.: 95.31%] [G loss: 2.452678]\n",
      "2641 [D loss: 0.137598, acc.: 95.31%] [G loss: 2.791056]\n",
      "2642 [D loss: 0.187360, acc.: 95.31%] [G loss: 3.007771]\n",
      "2643 [D loss: 0.166435, acc.: 93.75%] [G loss: 3.323748]\n",
      "2644 [D loss: 0.164737, acc.: 92.19%] [G loss: 3.509980]\n",
      "2645 [D loss: 0.208121, acc.: 93.75%] [G loss: 3.056004]\n",
      "2646 [D loss: 0.154199, acc.: 93.75%] [G loss: 3.671438]\n",
      "2647 [D loss: 0.161662, acc.: 95.31%] [G loss: 2.435927]\n",
      "2648 [D loss: 0.157841, acc.: 95.31%] [G loss: 3.473516]\n",
      "2649 [D loss: 0.144572, acc.: 95.31%] [G loss: 2.710503]\n",
      "2650 [D loss: 0.124075, acc.: 95.31%] [G loss: 3.403943]\n",
      "2651 [D loss: 0.156954, acc.: 95.31%] [G loss: 2.645613]\n",
      "2652 [D loss: 0.154584, acc.: 95.31%] [G loss: 3.159606]\n",
      "2653 [D loss: 0.174963, acc.: 92.19%] [G loss: 3.679745]\n",
      "2654 [D loss: 0.192208, acc.: 95.31%] [G loss: 3.149420]\n",
      "2655 [D loss: 0.198245, acc.: 93.75%] [G loss: 2.720924]\n",
      "2656 [D loss: 0.199555, acc.: 93.75%] [G loss: 2.832984]\n",
      "2657 [D loss: 0.188897, acc.: 92.19%] [G loss: 3.163603]\n",
      "2658 [D loss: 0.151354, acc.: 95.31%] [G loss: 2.731730]\n",
      "2659 [D loss: 0.197805, acc.: 92.19%] [G loss: 2.816282]\n",
      "2660 [D loss: 0.168020, acc.: 95.31%] [G loss: 3.070061]\n",
      "2661 [D loss: 0.253068, acc.: 89.06%] [G loss: 3.595822]\n",
      "2662 [D loss: 0.162492, acc.: 93.75%] [G loss: 2.836695]\n",
      "2663 [D loss: 0.178285, acc.: 95.31%] [G loss: 3.205531]\n",
      "2664 [D loss: 0.179077, acc.: 95.31%] [G loss: 3.126831]\n",
      "2665 [D loss: 0.157473, acc.: 95.31%] [G loss: 2.912359]\n",
      "2666 [D loss: 0.196364, acc.: 93.75%] [G loss: 3.047958]\n",
      "2667 [D loss: 0.172232, acc.: 95.31%] [G loss: 3.255538]\n",
      "2668 [D loss: 0.208666, acc.: 93.75%] [G loss: 3.101635]\n",
      "2669 [D loss: 0.139223, acc.: 95.31%] [G loss: 3.199902]\n",
      "2670 [D loss: 0.191302, acc.: 92.19%] [G loss: 2.536508]\n",
      "2671 [D loss: 0.200966, acc.: 93.75%] [G loss: 2.805330]\n",
      "2672 [D loss: 0.164189, acc.: 95.31%] [G loss: 2.970233]\n",
      "2673 [D loss: 0.137777, acc.: 95.31%] [G loss: 3.238327]\n",
      "2674 [D loss: 0.151270, acc.: 93.75%] [G loss: 2.594425]\n",
      "2675 [D loss: 0.156038, acc.: 95.31%] [G loss: 3.107031]\n",
      "2676 [D loss: 0.167219, acc.: 93.75%] [G loss: 2.975739]\n",
      "2677 [D loss: 0.216576, acc.: 92.19%] [G loss: 3.089996]\n",
      "2678 [D loss: 0.180116, acc.: 93.75%] [G loss: 2.685369]\n",
      "2679 [D loss: 0.183819, acc.: 89.06%] [G loss: 2.918843]\n",
      "2680 [D loss: 0.184299, acc.: 92.19%] [G loss: 2.813056]\n",
      "2681 [D loss: 0.160365, acc.: 95.31%] [G loss: 2.798704]\n",
      "2682 [D loss: 0.189770, acc.: 95.31%] [G loss: 2.923843]\n",
      "2683 [D loss: 0.163358, acc.: 95.31%] [G loss: 3.131057]\n",
      "2684 [D loss: 0.174920, acc.: 93.75%] [G loss: 3.027139]\n",
      "2685 [D loss: 0.172949, acc.: 93.75%] [G loss: 3.026023]\n",
      "2686 [D loss: 0.144888, acc.: 95.31%] [G loss: 3.029576]\n",
      "2687 [D loss: 0.198977, acc.: 92.19%] [G loss: 2.986130]\n",
      "2688 [D loss: 0.241307, acc.: 90.62%] [G loss: 3.213135]\n",
      "2689 [D loss: 0.227042, acc.: 89.06%] [G loss: 3.480519]\n",
      "2690 [D loss: 0.249103, acc.: 89.06%] [G loss: 2.566837]\n",
      "2691 [D loss: 0.153591, acc.: 95.31%] [G loss: 2.928816]\n",
      "2692 [D loss: 0.221800, acc.: 95.31%] [G loss: 2.694106]\n",
      "2693 [D loss: 0.182236, acc.: 93.75%] [G loss: 3.454221]\n",
      "2694 [D loss: 0.184243, acc.: 93.75%] [G loss: 3.218340]\n",
      "2695 [D loss: 0.182770, acc.: 95.31%] [G loss: 3.064253]\n",
      "2696 [D loss: 0.242761, acc.: 92.19%] [G loss: 3.589234]\n",
      "2697 [D loss: 0.195091, acc.: 92.19%] [G loss: 2.991876]\n",
      "2698 [D loss: 0.225329, acc.: 90.62%] [G loss: 2.555361]\n",
      "2699 [D loss: 0.182047, acc.: 95.31%] [G loss: 3.353474]\n",
      "2700 [D loss: 0.225798, acc.: 90.62%] [G loss: 3.163462]\n",
      "generated_data\n",
      "2701 [D loss: 0.145765, acc.: 95.31%] [G loss: 3.145864]\n",
      "2702 [D loss: 0.189144, acc.: 95.31%] [G loss: 2.458496]\n",
      "2703 [D loss: 0.154878, acc.: 93.75%] [G loss: 3.004527]\n",
      "2704 [D loss: 0.175648, acc.: 95.31%] [G loss: 2.856836]\n",
      "2705 [D loss: 0.145475, acc.: 95.31%] [G loss: 3.388635]\n",
      "2706 [D loss: 0.150097, acc.: 95.31%] [G loss: 2.856930]\n",
      "2707 [D loss: 0.156233, acc.: 95.31%] [G loss: 3.188329]\n",
      "2708 [D loss: 0.182583, acc.: 93.75%] [G loss: 3.116154]\n",
      "2709 [D loss: 0.139278, acc.: 95.31%] [G loss: 3.260401]\n",
      "2710 [D loss: 0.149624, acc.: 95.31%] [G loss: 2.805785]\n",
      "2711 [D loss: 0.141572, acc.: 95.31%] [G loss: 3.338721]\n",
      "2712 [D loss: 0.175263, acc.: 93.75%] [G loss: 2.773804]\n",
      "2713 [D loss: 0.160489, acc.: 93.75%] [G loss: 2.994818]\n",
      "2714 [D loss: 0.217567, acc.: 93.75%] [G loss: 3.106568]\n",
      "2715 [D loss: 0.201859, acc.: 93.75%] [G loss: 3.096035]\n",
      "2716 [D loss: 0.157160, acc.: 93.75%] [G loss: 2.836027]\n",
      "2717 [D loss: 0.168793, acc.: 93.75%] [G loss: 2.900285]\n",
      "2718 [D loss: 0.214175, acc.: 93.75%] [G loss: 3.343962]\n",
      "2719 [D loss: 0.143543, acc.: 95.31%] [G loss: 2.969585]\n",
      "2720 [D loss: 0.212355, acc.: 92.19%] [G loss: 3.381972]\n",
      "2721 [D loss: 0.174130, acc.: 95.31%] [G loss: 3.119667]\n",
      "2722 [D loss: 0.182103, acc.: 93.75%] [G loss: 2.757896]\n",
      "2723 [D loss: 0.179659, acc.: 93.75%] [G loss: 3.204398]\n",
      "2724 [D loss: 0.157855, acc.: 93.75%] [G loss: 2.482506]\n",
      "2725 [D loss: 0.166982, acc.: 95.31%] [G loss: 2.963265]\n",
      "2726 [D loss: 0.167438, acc.: 95.31%] [G loss: 3.304147]\n",
      "2727 [D loss: 0.231803, acc.: 92.19%] [G loss: 2.882352]\n",
      "2728 [D loss: 0.177461, acc.: 95.31%] [G loss: 2.850234]\n",
      "2729 [D loss: 0.196972, acc.: 93.75%] [G loss: 3.232732]\n",
      "2730 [D loss: 0.169123, acc.: 95.31%] [G loss: 3.441289]\n",
      "2731 [D loss: 0.158583, acc.: 95.31%] [G loss: 2.589844]\n",
      "2732 [D loss: 0.214838, acc.: 92.19%] [G loss: 3.027155]\n",
      "2733 [D loss: 0.192073, acc.: 93.75%] [G loss: 3.259735]\n",
      "2734 [D loss: 0.229883, acc.: 92.19%] [G loss: 2.934707]\n",
      "2735 [D loss: 0.180833, acc.: 95.31%] [G loss: 2.984290]\n",
      "2736 [D loss: 0.200429, acc.: 93.75%] [G loss: 2.756068]\n",
      "2737 [D loss: 0.194246, acc.: 93.75%] [G loss: 2.907202]\n",
      "2738 [D loss: 0.177951, acc.: 95.31%] [G loss: 2.787302]\n",
      "2739 [D loss: 0.150120, acc.: 95.31%] [G loss: 3.118149]\n",
      "2740 [D loss: 0.170095, acc.: 95.31%] [G loss: 3.071710]\n",
      "2741 [D loss: 0.172294, acc.: 95.31%] [G loss: 2.887515]\n",
      "2742 [D loss: 0.214216, acc.: 93.75%] [G loss: 3.208717]\n",
      "2743 [D loss: 0.179138, acc.: 95.31%] [G loss: 3.143138]\n",
      "2744 [D loss: 0.155327, acc.: 95.31%] [G loss: 2.694205]\n",
      "2745 [D loss: 0.243816, acc.: 90.62%] [G loss: 2.394344]\n",
      "2746 [D loss: 0.169270, acc.: 93.75%] [G loss: 3.026729]\n",
      "2747 [D loss: 0.216003, acc.: 93.75%] [G loss: 2.963025]\n",
      "2748 [D loss: 0.145231, acc.: 93.75%] [G loss: 2.983128]\n",
      "2749 [D loss: 0.175091, acc.: 93.75%] [G loss: 3.114963]\n",
      "2750 [D loss: 0.163836, acc.: 95.31%] [G loss: 2.393518]\n",
      "2751 [D loss: 0.166382, acc.: 95.31%] [G loss: 2.690396]\n",
      "2752 [D loss: 0.213603, acc.: 92.19%] [G loss: 2.481194]\n",
      "2753 [D loss: 0.159637, acc.: 95.31%] [G loss: 3.128837]\n",
      "2754 [D loss: 0.167932, acc.: 95.31%] [G loss: 2.766905]\n",
      "2755 [D loss: 0.144069, acc.: 95.31%] [G loss: 2.706059]\n",
      "2756 [D loss: 0.166037, acc.: 92.19%] [G loss: 3.058657]\n",
      "2757 [D loss: 0.223171, acc.: 93.75%] [G loss: 3.230088]\n",
      "2758 [D loss: 0.198492, acc.: 90.62%] [G loss: 2.507925]\n",
      "2759 [D loss: 0.193241, acc.: 92.19%] [G loss: 3.042216]\n",
      "2760 [D loss: 0.166258, acc.: 95.31%] [G loss: 2.608038]\n",
      "2761 [D loss: 0.240476, acc.: 92.19%] [G loss: 2.994452]\n",
      "2762 [D loss: 0.162888, acc.: 95.31%] [G loss: 3.291968]\n",
      "2763 [D loss: 0.165530, acc.: 93.75%] [G loss: 3.231522]\n",
      "2764 [D loss: 0.179147, acc.: 93.75%] [G loss: 2.763874]\n",
      "2765 [D loss: 0.166959, acc.: 95.31%] [G loss: 2.668370]\n",
      "2766 [D loss: 0.144462, acc.: 95.31%] [G loss: 2.902280]\n",
      "2767 [D loss: 0.232852, acc.: 89.06%] [G loss: 3.078516]\n",
      "2768 [D loss: 0.192147, acc.: 93.75%] [G loss: 2.908890]\n",
      "2769 [D loss: 0.195286, acc.: 92.19%] [G loss: 2.647295]\n",
      "2770 [D loss: 0.189489, acc.: 93.75%] [G loss: 3.163011]\n",
      "2771 [D loss: 0.160918, acc.: 95.31%] [G loss: 2.454792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2772 [D loss: 0.168705, acc.: 95.31%] [G loss: 2.885105]\n",
      "2773 [D loss: 0.291659, acc.: 90.62%] [G loss: 2.951814]\n",
      "2774 [D loss: 0.176465, acc.: 93.75%] [G loss: 2.977088]\n",
      "2775 [D loss: 0.237335, acc.: 90.62%] [G loss: 3.072278]\n",
      "2776 [D loss: 0.228653, acc.: 90.62%] [G loss: 2.964403]\n",
      "2777 [D loss: 0.157319, acc.: 95.31%] [G loss: 2.607695]\n",
      "2778 [D loss: 0.195258, acc.: 93.75%] [G loss: 2.543152]\n",
      "2779 [D loss: 0.221139, acc.: 90.62%] [G loss: 2.916550]\n",
      "2780 [D loss: 0.183988, acc.: 95.31%] [G loss: 3.027460]\n",
      "2781 [D loss: 0.303285, acc.: 85.94%] [G loss: 3.090205]\n",
      "2782 [D loss: 0.229266, acc.: 92.19%] [G loss: 3.073098]\n",
      "2783 [D loss: 0.244914, acc.: 90.62%] [G loss: 2.770400]\n",
      "2784 [D loss: 0.194949, acc.: 92.19%] [G loss: 2.778437]\n",
      "2785 [D loss: 0.175198, acc.: 93.75%] [G loss: 2.983017]\n",
      "2786 [D loss: 0.165803, acc.: 93.75%] [G loss: 2.735092]\n",
      "2787 [D loss: 0.159997, acc.: 95.31%] [G loss: 2.751481]\n",
      "2788 [D loss: 0.198214, acc.: 93.75%] [G loss: 2.691382]\n",
      "2789 [D loss: 0.160697, acc.: 93.75%] [G loss: 3.155989]\n",
      "2790 [D loss: 0.148704, acc.: 95.31%] [G loss: 3.106748]\n",
      "2791 [D loss: 0.177803, acc.: 93.75%] [G loss: 2.868256]\n",
      "2792 [D loss: 0.189802, acc.: 93.75%] [G loss: 3.119513]\n",
      "2793 [D loss: 0.160611, acc.: 95.31%] [G loss: 3.326016]\n",
      "2794 [D loss: 0.243400, acc.: 93.75%] [G loss: 2.776875]\n",
      "2795 [D loss: 0.190062, acc.: 93.75%] [G loss: 2.591011]\n",
      "2796 [D loss: 0.144304, acc.: 95.31%] [G loss: 2.790314]\n",
      "2797 [D loss: 0.139908, acc.: 95.31%] [G loss: 3.402823]\n",
      "2798 [D loss: 0.177024, acc.: 95.31%] [G loss: 2.831521]\n",
      "2799 [D loss: 0.158362, acc.: 95.31%] [G loss: 2.941391]\n",
      "2800 [D loss: 0.213464, acc.: 92.19%] [G loss: 3.185319]\n",
      "generated_data\n",
      "2801 [D loss: 0.156745, acc.: 93.75%] [G loss: 3.387123]\n",
      "2802 [D loss: 0.153281, acc.: 95.31%] [G loss: 2.668262]\n",
      "2803 [D loss: 0.122277, acc.: 95.31%] [G loss: 3.456397]\n",
      "2804 [D loss: 0.171369, acc.: 95.31%] [G loss: 2.630013]\n",
      "2805 [D loss: 0.152309, acc.: 95.31%] [G loss: 3.161340]\n",
      "2806 [D loss: 0.176773, acc.: 93.75%] [G loss: 3.228947]\n",
      "2807 [D loss: 0.168511, acc.: 93.75%] [G loss: 3.559934]\n",
      "2808 [D loss: 0.194253, acc.: 93.75%] [G loss: 3.280704]\n",
      "2809 [D loss: 0.194860, acc.: 90.62%] [G loss: 3.223957]\n",
      "2810 [D loss: 0.201647, acc.: 92.19%] [G loss: 3.498455]\n",
      "2811 [D loss: 0.157070, acc.: 93.75%] [G loss: 3.085999]\n",
      "2812 [D loss: 0.177088, acc.: 93.75%] [G loss: 2.596245]\n",
      "2813 [D loss: 0.166298, acc.: 93.75%] [G loss: 2.622444]\n",
      "2814 [D loss: 0.148524, acc.: 95.31%] [G loss: 2.872383]\n",
      "2815 [D loss: 0.195037, acc.: 93.75%] [G loss: 3.039672]\n",
      "2816 [D loss: 0.181002, acc.: 93.75%] [G loss: 2.854008]\n",
      "2817 [D loss: 0.212353, acc.: 92.19%] [G loss: 3.279989]\n",
      "2818 [D loss: 0.161446, acc.: 95.31%] [G loss: 3.060668]\n",
      "2819 [D loss: 0.169443, acc.: 95.31%] [G loss: 3.275304]\n",
      "2820 [D loss: 0.167469, acc.: 95.31%] [G loss: 2.918060]\n",
      "2821 [D loss: 0.163205, acc.: 95.31%] [G loss: 3.005856]\n",
      "2822 [D loss: 0.168889, acc.: 95.31%] [G loss: 3.072920]\n",
      "2823 [D loss: 0.157598, acc.: 95.31%] [G loss: 2.833190]\n",
      "2824 [D loss: 0.175799, acc.: 95.31%] [G loss: 2.964191]\n",
      "2825 [D loss: 0.134652, acc.: 95.31%] [G loss: 3.279677]\n",
      "2826 [D loss: 0.169185, acc.: 95.31%] [G loss: 2.946962]\n",
      "2827 [D loss: 0.183193, acc.: 93.75%] [G loss: 3.365420]\n",
      "2828 [D loss: 0.165363, acc.: 95.31%] [G loss: 3.011876]\n",
      "2829 [D loss: 0.180455, acc.: 95.31%] [G loss: 2.763363]\n",
      "2830 [D loss: 0.165166, acc.: 93.75%] [G loss: 3.580433]\n",
      "2831 [D loss: 0.171680, acc.: 95.31%] [G loss: 3.178513]\n",
      "2832 [D loss: 0.184726, acc.: 93.75%] [G loss: 3.004529]\n",
      "2833 [D loss: 0.176617, acc.: 93.75%] [G loss: 3.225741]\n",
      "2834 [D loss: 0.176200, acc.: 93.75%] [G loss: 3.428235]\n",
      "2835 [D loss: 0.184576, acc.: 93.75%] [G loss: 2.780617]\n",
      "2836 [D loss: 0.174182, acc.: 93.75%] [G loss: 3.314697]\n",
      "2837 [D loss: 0.164629, acc.: 95.31%] [G loss: 3.617996]\n",
      "2838 [D loss: 0.201996, acc.: 95.31%] [G loss: 2.851015]\n",
      "2839 [D loss: 0.146446, acc.: 95.31%] [G loss: 3.425025]\n",
      "2840 [D loss: 0.176398, acc.: 95.31%] [G loss: 2.745526]\n",
      "2841 [D loss: 0.228831, acc.: 93.75%] [G loss: 3.011657]\n",
      "2842 [D loss: 0.185163, acc.: 95.31%] [G loss: 2.928581]\n",
      "2843 [D loss: 0.197354, acc.: 93.75%] [G loss: 2.887483]\n",
      "2844 [D loss: 0.150463, acc.: 95.31%] [G loss: 3.146924]\n",
      "2845 [D loss: 0.153873, acc.: 95.31%] [G loss: 2.775003]\n",
      "2846 [D loss: 0.171690, acc.: 95.31%] [G loss: 2.607071]\n",
      "2847 [D loss: 0.165174, acc.: 95.31%] [G loss: 2.737788]\n",
      "2848 [D loss: 0.222948, acc.: 93.75%] [G loss: 2.901282]\n",
      "2849 [D loss: 0.150855, acc.: 95.31%] [G loss: 3.025821]\n",
      "2850 [D loss: 0.147953, acc.: 95.31%] [G loss: 3.148793]\n",
      "2851 [D loss: 0.187170, acc.: 93.75%] [G loss: 2.875340]\n",
      "2852 [D loss: 0.174297, acc.: 95.31%] [G loss: 2.674530]\n",
      "2853 [D loss: 0.147467, acc.: 95.31%] [G loss: 3.147904]\n",
      "2854 [D loss: 0.166637, acc.: 95.31%] [G loss: 2.989702]\n",
      "2855 [D loss: 0.157546, acc.: 95.31%] [G loss: 3.040941]\n",
      "2856 [D loss: 0.160194, acc.: 95.31%] [G loss: 2.854154]\n",
      "2857 [D loss: 0.151449, acc.: 93.75%] [G loss: 2.921471]\n",
      "2858 [D loss: 0.176765, acc.: 95.31%] [G loss: 3.018460]\n",
      "2859 [D loss: 0.161853, acc.: 95.31%] [G loss: 3.612245]\n",
      "2860 [D loss: 0.171205, acc.: 95.31%] [G loss: 2.772966]\n",
      "2861 [D loss: 0.211789, acc.: 92.19%] [G loss: 3.210713]\n",
      "2862 [D loss: 0.150050, acc.: 95.31%] [G loss: 2.783378]\n",
      "2863 [D loss: 0.164980, acc.: 95.31%] [G loss: 3.587074]\n",
      "2864 [D loss: 0.145069, acc.: 95.31%] [G loss: 3.173078]\n",
      "2865 [D loss: 0.161463, acc.: 93.75%] [G loss: 2.506160]\n",
      "2866 [D loss: 0.180161, acc.: 93.75%] [G loss: 3.124387]\n",
      "2867 [D loss: 0.161320, acc.: 95.31%] [G loss: 2.915932]\n",
      "2868 [D loss: 0.180136, acc.: 95.31%] [G loss: 2.850155]\n",
      "2869 [D loss: 0.188118, acc.: 93.75%] [G loss: 2.999469]\n",
      "2870 [D loss: 0.188955, acc.: 93.75%] [G loss: 3.438741]\n",
      "2871 [D loss: 0.166685, acc.: 93.75%] [G loss: 2.736209]\n",
      "2872 [D loss: 0.196889, acc.: 93.75%] [G loss: 2.714980]\n",
      "2873 [D loss: 0.176857, acc.: 95.31%] [G loss: 3.076617]\n",
      "2874 [D loss: 0.157168, acc.: 95.31%] [G loss: 2.628773]\n",
      "2875 [D loss: 0.146625, acc.: 95.31%] [G loss: 2.761487]\n",
      "2876 [D loss: 0.188636, acc.: 95.31%] [G loss: 3.179784]\n",
      "2877 [D loss: 0.167896, acc.: 95.31%] [G loss: 3.201384]\n",
      "2878 [D loss: 0.164980, acc.: 95.31%] [G loss: 3.093674]\n",
      "2879 [D loss: 0.170686, acc.: 95.31%] [G loss: 3.240774]\n",
      "2880 [D loss: 0.153534, acc.: 95.31%] [G loss: 2.984681]\n",
      "2881 [D loss: 0.192974, acc.: 93.75%] [G loss: 3.181601]\n",
      "2882 [D loss: 0.149950, acc.: 95.31%] [G loss: 2.983259]\n",
      "2883 [D loss: 0.165198, acc.: 95.31%] [G loss: 2.880612]\n",
      "2884 [D loss: 0.172422, acc.: 95.31%] [G loss: 2.890154]\n",
      "2885 [D loss: 0.161159, acc.: 95.31%] [G loss: 3.046169]\n",
      "2886 [D loss: 0.129002, acc.: 95.31%] [G loss: 3.282062]\n",
      "2887 [D loss: 0.167415, acc.: 95.31%] [G loss: 3.083954]\n",
      "2888 [D loss: 0.143444, acc.: 95.31%] [G loss: 3.253624]\n",
      "2889 [D loss: 0.146151, acc.: 95.31%] [G loss: 3.332878]\n",
      "2890 [D loss: 0.168064, acc.: 95.31%] [G loss: 2.445145]\n",
      "2891 [D loss: 0.145095, acc.: 93.75%] [G loss: 3.077978]\n",
      "2892 [D loss: 0.152200, acc.: 95.31%] [G loss: 3.068265]\n",
      "2893 [D loss: 0.184516, acc.: 93.75%] [G loss: 2.840969]\n",
      "2894 [D loss: 0.162057, acc.: 95.31%] [G loss: 3.512787]\n",
      "2895 [D loss: 0.152152, acc.: 95.31%] [G loss: 3.331633]\n",
      "2896 [D loss: 0.177209, acc.: 92.19%] [G loss: 3.120535]\n",
      "2897 [D loss: 0.154467, acc.: 95.31%] [G loss: 3.470069]\n",
      "2898 [D loss: 0.178804, acc.: 95.31%] [G loss: 3.107708]\n",
      "2899 [D loss: 0.186375, acc.: 93.75%] [G loss: 2.952400]\n",
      "2900 [D loss: 0.168349, acc.: 95.31%] [G loss: 3.304386]\n",
      "generated_data\n",
      "2901 [D loss: 0.148216, acc.: 95.31%] [G loss: 3.296999]\n",
      "2902 [D loss: 0.162926, acc.: 95.31%] [G loss: 3.049900]\n",
      "2903 [D loss: 0.137979, acc.: 95.31%] [G loss: 3.031533]\n",
      "2904 [D loss: 0.229093, acc.: 92.19%] [G loss: 2.737533]\n",
      "2905 [D loss: 0.159460, acc.: 93.75%] [G loss: 3.267953]\n",
      "2906 [D loss: 0.148415, acc.: 95.31%] [G loss: 3.066865]\n",
      "2907 [D loss: 0.155810, acc.: 95.31%] [G loss: 3.252202]\n",
      "2908 [D loss: 0.138192, acc.: 95.31%] [G loss: 2.746735]\n",
      "2909 [D loss: 0.178410, acc.: 92.19%] [G loss: 3.478524]\n",
      "2910 [D loss: 0.204391, acc.: 93.75%] [G loss: 3.103236]\n",
      "2911 [D loss: 0.188317, acc.: 93.75%] [G loss: 3.361838]\n",
      "2912 [D loss: 0.196192, acc.: 93.75%] [G loss: 2.797521]\n",
      "2913 [D loss: 0.138295, acc.: 95.31%] [G loss: 3.257320]\n",
      "2914 [D loss: 0.174303, acc.: 93.75%] [G loss: 3.448140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2915 [D loss: 0.201739, acc.: 92.19%] [G loss: 3.019060]\n",
      "2916 [D loss: 0.191020, acc.: 93.75%] [G loss: 2.816886]\n",
      "2917 [D loss: 0.179921, acc.: 93.75%] [G loss: 3.125059]\n",
      "2918 [D loss: 0.218717, acc.: 92.19%] [G loss: 2.781520]\n",
      "2919 [D loss: 0.174957, acc.: 93.75%] [G loss: 2.674954]\n",
      "2920 [D loss: 0.212676, acc.: 95.31%] [G loss: 2.945921]\n",
      "2921 [D loss: 0.175101, acc.: 95.31%] [G loss: 3.005756]\n",
      "2922 [D loss: 0.158223, acc.: 95.31%] [G loss: 3.047641]\n",
      "2923 [D loss: 0.175478, acc.: 93.75%] [G loss: 2.747452]\n",
      "2924 [D loss: 0.141189, acc.: 95.31%] [G loss: 2.881771]\n",
      "2925 [D loss: 0.190715, acc.: 93.75%] [G loss: 2.548242]\n",
      "2926 [D loss: 0.148054, acc.: 95.31%] [G loss: 2.756680]\n",
      "2927 [D loss: 0.138732, acc.: 93.75%] [G loss: 3.043324]\n",
      "2928 [D loss: 0.183292, acc.: 95.31%] [G loss: 2.813293]\n",
      "2929 [D loss: 0.177965, acc.: 95.31%] [G loss: 2.738007]\n",
      "2930 [D loss: 0.166408, acc.: 93.75%] [G loss: 3.051729]\n",
      "2931 [D loss: 0.176412, acc.: 93.75%] [G loss: 3.557167]\n",
      "2932 [D loss: 0.157140, acc.: 95.31%] [G loss: 2.899812]\n",
      "2933 [D loss: 0.157825, acc.: 95.31%] [G loss: 3.017956]\n",
      "2934 [D loss: 0.157087, acc.: 95.31%] [G loss: 3.070791]\n",
      "2935 [D loss: 0.164322, acc.: 93.75%] [G loss: 3.217702]\n",
      "2936 [D loss: 0.200590, acc.: 93.75%] [G loss: 2.855529]\n",
      "2937 [D loss: 0.228570, acc.: 92.19%] [G loss: 3.198095]\n",
      "2938 [D loss: 0.208869, acc.: 92.19%] [G loss: 2.737396]\n",
      "2939 [D loss: 0.211140, acc.: 92.19%] [G loss: 3.046751]\n",
      "2940 [D loss: 0.169901, acc.: 93.75%] [G loss: 3.352974]\n",
      "2941 [D loss: 0.227299, acc.: 92.19%] [G loss: 3.217489]\n",
      "2942 [D loss: 0.213323, acc.: 93.75%] [G loss: 2.960662]\n",
      "2943 [D loss: 0.157823, acc.: 95.31%] [G loss: 2.847814]\n",
      "2944 [D loss: 0.210228, acc.: 93.75%] [G loss: 3.242057]\n",
      "2945 [D loss: 0.161520, acc.: 93.75%] [G loss: 3.099634]\n",
      "2946 [D loss: 0.157572, acc.: 95.31%] [G loss: 3.346161]\n",
      "2947 [D loss: 0.192938, acc.: 93.75%] [G loss: 2.682982]\n",
      "2948 [D loss: 0.148711, acc.: 95.31%] [G loss: 3.203649]\n",
      "2949 [D loss: 0.150475, acc.: 95.31%] [G loss: 3.211043]\n",
      "2950 [D loss: 0.198954, acc.: 92.19%] [G loss: 3.053064]\n",
      "2951 [D loss: 0.182667, acc.: 93.75%] [G loss: 2.618994]\n",
      "2952 [D loss: 0.161504, acc.: 95.31%] [G loss: 2.890440]\n",
      "2953 [D loss: 0.167821, acc.: 95.31%] [G loss: 2.899784]\n",
      "2954 [D loss: 0.148719, acc.: 95.31%] [G loss: 2.912525]\n",
      "2955 [D loss: 0.158089, acc.: 95.31%] [G loss: 3.138155]\n",
      "2956 [D loss: 0.162839, acc.: 95.31%] [G loss: 2.949603]\n",
      "2957 [D loss: 0.153148, acc.: 95.31%] [G loss: 2.978788]\n",
      "2958 [D loss: 0.162550, acc.: 95.31%] [G loss: 3.309188]\n",
      "2959 [D loss: 0.153980, acc.: 95.31%] [G loss: 2.874140]\n",
      "2960 [D loss: 0.159944, acc.: 95.31%] [G loss: 2.941740]\n",
      "2961 [D loss: 0.145061, acc.: 95.31%] [G loss: 3.067699]\n",
      "2962 [D loss: 0.175489, acc.: 95.31%] [G loss: 3.276471]\n",
      "2963 [D loss: 0.169280, acc.: 93.75%] [G loss: 3.108690]\n",
      "2964 [D loss: 0.159651, acc.: 95.31%] [G loss: 3.188118]\n",
      "2965 [D loss: 0.169088, acc.: 95.31%] [G loss: 3.305779]\n",
      "2966 [D loss: 0.147050, acc.: 95.31%] [G loss: 2.886498]\n",
      "2967 [D loss: 0.187160, acc.: 93.75%] [G loss: 2.732765]\n",
      "2968 [D loss: 0.163632, acc.: 95.31%] [G loss: 2.800180]\n",
      "2969 [D loss: 0.149440, acc.: 95.31%] [G loss: 3.006079]\n",
      "2970 [D loss: 0.197544, acc.: 93.75%] [G loss: 3.199555]\n",
      "2971 [D loss: 0.145017, acc.: 93.75%] [G loss: 3.110798]\n",
      "2972 [D loss: 0.143352, acc.: 95.31%] [G loss: 3.412061]\n",
      "2973 [D loss: 0.169428, acc.: 95.31%] [G loss: 2.784432]\n",
      "2974 [D loss: 0.139306, acc.: 95.31%] [G loss: 3.350127]\n",
      "2975 [D loss: 0.160673, acc.: 93.75%] [G loss: 3.247964]\n",
      "2976 [D loss: 0.139239, acc.: 95.31%] [G loss: 2.678761]\n",
      "2977 [D loss: 0.220256, acc.: 93.75%] [G loss: 3.082011]\n",
      "2978 [D loss: 0.147263, acc.: 93.75%] [G loss: 3.252382]\n",
      "2979 [D loss: 0.159741, acc.: 93.75%] [G loss: 3.279475]\n",
      "2980 [D loss: 0.148371, acc.: 95.31%] [G loss: 2.594240]\n",
      "2981 [D loss: 0.131602, acc.: 95.31%] [G loss: 3.075303]\n",
      "2982 [D loss: 0.149078, acc.: 95.31%] [G loss: 3.447573]\n",
      "2983 [D loss: 0.166671, acc.: 93.75%] [G loss: 2.777926]\n",
      "2984 [D loss: 0.134671, acc.: 95.31%] [G loss: 3.190348]\n",
      "2985 [D loss: 0.147343, acc.: 95.31%] [G loss: 2.954293]\n",
      "2986 [D loss: 0.155651, acc.: 95.31%] [G loss: 2.927650]\n",
      "2987 [D loss: 0.140464, acc.: 95.31%] [G loss: 2.830784]\n",
      "2988 [D loss: 0.158813, acc.: 95.31%] [G loss: 2.781456]\n",
      "2989 [D loss: 0.143494, acc.: 93.75%] [G loss: 3.394479]\n",
      "2990 [D loss: 0.154901, acc.: 95.31%] [G loss: 3.014919]\n",
      "2991 [D loss: 0.170641, acc.: 95.31%] [G loss: 2.805731]\n",
      "2992 [D loss: 0.186101, acc.: 93.75%] [G loss: 3.215461]\n",
      "2993 [D loss: 0.149473, acc.: 95.31%] [G loss: 3.158094]\n",
      "2994 [D loss: 0.186889, acc.: 93.75%] [G loss: 3.124481]\n",
      "2995 [D loss: 0.180806, acc.: 93.75%] [G loss: 3.117231]\n",
      "2996 [D loss: 0.182317, acc.: 95.31%] [G loss: 2.919483]\n",
      "2997 [D loss: 0.225237, acc.: 92.19%] [G loss: 2.943609]\n",
      "2998 [D loss: 0.176381, acc.: 93.75%] [G loss: 3.333493]\n",
      "2999 [D loss: 0.156141, acc.: 95.31%] [G loss: 3.175534]\n",
      "3000 [D loss: 0.160395, acc.: 93.75%] [G loss: 2.936469]\n",
      "generated_data\n",
      "3001 [D loss: 0.144775, acc.: 95.31%] [G loss: 2.838651]\n",
      "3002 [D loss: 0.195675, acc.: 93.75%] [G loss: 2.999849]\n",
      "3003 [D loss: 0.203804, acc.: 93.75%] [G loss: 3.063475]\n",
      "3004 [D loss: 0.231738, acc.: 92.19%] [G loss: 3.130206]\n",
      "3005 [D loss: 0.169221, acc.: 93.75%] [G loss: 3.260829]\n",
      "3006 [D loss: 0.158288, acc.: 93.75%] [G loss: 2.916375]\n",
      "3007 [D loss: 0.174854, acc.: 95.31%] [G loss: 2.967855]\n",
      "3008 [D loss: 0.171685, acc.: 95.31%] [G loss: 2.769980]\n",
      "3009 [D loss: 0.163494, acc.: 95.31%] [G loss: 3.156271]\n",
      "3010 [D loss: 0.141152, acc.: 95.31%] [G loss: 3.135391]\n",
      "3011 [D loss: 0.149987, acc.: 95.31%] [G loss: 3.181021]\n",
      "3012 [D loss: 0.134242, acc.: 95.31%] [G loss: 3.308246]\n",
      "3013 [D loss: 0.158639, acc.: 95.31%] [G loss: 3.148099]\n",
      "3014 [D loss: 0.171259, acc.: 95.31%] [G loss: 2.999971]\n",
      "3015 [D loss: 0.140306, acc.: 95.31%] [G loss: 3.180878]\n",
      "3016 [D loss: 0.179021, acc.: 93.75%] [G loss: 3.068861]\n",
      "3017 [D loss: 0.140790, acc.: 93.75%] [G loss: 3.101009]\n",
      "3018 [D loss: 0.145874, acc.: 95.31%] [G loss: 3.030219]\n",
      "3019 [D loss: 0.167508, acc.: 93.75%] [G loss: 3.345386]\n",
      "3020 [D loss: 0.187830, acc.: 93.75%] [G loss: 3.117785]\n",
      "3021 [D loss: 0.167350, acc.: 93.75%] [G loss: 3.291789]\n",
      "3022 [D loss: 0.189078, acc.: 93.75%] [G loss: 3.455099]\n",
      "3023 [D loss: 0.142188, acc.: 95.31%] [G loss: 3.295419]\n",
      "3024 [D loss: 0.196379, acc.: 90.62%] [G loss: 3.146038]\n",
      "3025 [D loss: 0.128001, acc.: 95.31%] [G loss: 3.158831]\n",
      "3026 [D loss: 0.224510, acc.: 90.62%] [G loss: 2.842079]\n",
      "3027 [D loss: 0.172756, acc.: 93.75%] [G loss: 2.879415]\n",
      "3028 [D loss: 0.155947, acc.: 95.31%] [G loss: 3.125097]\n",
      "3029 [D loss: 0.165613, acc.: 93.75%] [G loss: 3.197146]\n",
      "3030 [D loss: 0.170031, acc.: 95.31%] [G loss: 2.773419]\n",
      "3031 [D loss: 0.168461, acc.: 93.75%] [G loss: 2.861970]\n",
      "3032 [D loss: 0.256280, acc.: 85.94%] [G loss: 3.415117]\n",
      "3033 [D loss: 0.229328, acc.: 90.62%] [G loss: 3.198155]\n",
      "3034 [D loss: 0.177405, acc.: 95.31%] [G loss: 3.126917]\n",
      "3035 [D loss: 0.190702, acc.: 92.19%] [G loss: 2.996713]\n",
      "3036 [D loss: 0.171957, acc.: 95.31%] [G loss: 2.524624]\n",
      "3037 [D loss: 0.207815, acc.: 93.75%] [G loss: 2.758663]\n",
      "3038 [D loss: 0.130058, acc.: 95.31%] [G loss: 3.563661]\n",
      "3039 [D loss: 0.200469, acc.: 93.75%] [G loss: 2.669665]\n",
      "3040 [D loss: 0.165346, acc.: 95.31%] [G loss: 2.733608]\n",
      "3041 [D loss: 0.173140, acc.: 95.31%] [G loss: 2.694170]\n",
      "3042 [D loss: 0.144758, acc.: 95.31%] [G loss: 3.148485]\n",
      "3043 [D loss: 0.165321, acc.: 95.31%] [G loss: 3.121240]\n",
      "3044 [D loss: 0.149951, acc.: 95.31%] [G loss: 2.681471]\n",
      "3045 [D loss: 0.215903, acc.: 93.75%] [G loss: 2.932367]\n",
      "3046 [D loss: 0.163951, acc.: 95.31%] [G loss: 2.904358]\n",
      "3047 [D loss: 0.165326, acc.: 95.31%] [G loss: 2.782891]\n",
      "3048 [D loss: 0.172102, acc.: 95.31%] [G loss: 3.101216]\n",
      "3049 [D loss: 0.137629, acc.: 95.31%] [G loss: 3.817269]\n",
      "3050 [D loss: 0.165093, acc.: 95.31%] [G loss: 2.603646]\n",
      "3051 [D loss: 0.213904, acc.: 92.19%] [G loss: 3.265987]\n",
      "3052 [D loss: 0.149525, acc.: 95.31%] [G loss: 3.203307]\n",
      "3053 [D loss: 0.241463, acc.: 92.19%] [G loss: 3.092505]\n",
      "3054 [D loss: 0.167221, acc.: 95.31%] [G loss: 3.020532]\n",
      "3055 [D loss: 0.132362, acc.: 95.31%] [G loss: 3.562668]\n",
      "3056 [D loss: 0.158059, acc.: 95.31%] [G loss: 2.824520]\n",
      "3057 [D loss: 0.149533, acc.: 95.31%] [G loss: 3.084949]\n",
      "3058 [D loss: 0.173272, acc.: 93.75%] [G loss: 3.410087]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3059 [D loss: 0.162542, acc.: 95.31%] [G loss: 2.831188]\n",
      "3060 [D loss: 0.160784, acc.: 95.31%] [G loss: 3.315554]\n",
      "3061 [D loss: 0.164782, acc.: 95.31%] [G loss: 3.146957]\n",
      "3062 [D loss: 0.161904, acc.: 95.31%] [G loss: 3.077628]\n",
      "3063 [D loss: 0.182003, acc.: 93.75%] [G loss: 3.094856]\n",
      "3064 [D loss: 0.180659, acc.: 95.31%] [G loss: 3.054544]\n",
      "3065 [D loss: 0.184628, acc.: 93.75%] [G loss: 2.758636]\n",
      "3066 [D loss: 0.201760, acc.: 92.19%] [G loss: 2.722279]\n",
      "3067 [D loss: 0.145064, acc.: 95.31%] [G loss: 3.372639]\n",
      "3068 [D loss: 0.195547, acc.: 92.19%] [G loss: 3.057760]\n",
      "3069 [D loss: 0.147238, acc.: 95.31%] [G loss: 2.890115]\n",
      "3070 [D loss: 0.177289, acc.: 95.31%] [G loss: 2.811175]\n",
      "3071 [D loss: 0.166715, acc.: 93.75%] [G loss: 2.855968]\n",
      "3072 [D loss: 0.174488, acc.: 93.75%] [G loss: 3.707420]\n",
      "3073 [D loss: 0.165727, acc.: 93.75%] [G loss: 3.275404]\n",
      "3074 [D loss: 0.226276, acc.: 93.75%] [G loss: 2.986487]\n",
      "3075 [D loss: 0.132350, acc.: 95.31%] [G loss: 3.491756]\n",
      "3076 [D loss: 0.150387, acc.: 95.31%] [G loss: 2.991296]\n",
      "3077 [D loss: 0.124102, acc.: 95.31%] [G loss: 3.602754]\n",
      "3078 [D loss: 0.163152, acc.: 95.31%] [G loss: 3.202485]\n",
      "3079 [D loss: 0.132654, acc.: 95.31%] [G loss: 3.176122]\n",
      "3080 [D loss: 0.181563, acc.: 92.19%] [G loss: 3.749895]\n",
      "3081 [D loss: 0.163292, acc.: 95.31%] [G loss: 3.058067]\n",
      "3082 [D loss: 0.173933, acc.: 95.31%] [G loss: 2.744432]\n",
      "3083 [D loss: 0.235202, acc.: 93.75%] [G loss: 3.044170]\n",
      "3084 [D loss: 0.193838, acc.: 93.75%] [G loss: 3.261656]\n",
      "3085 [D loss: 0.194439, acc.: 92.19%] [G loss: 3.094194]\n",
      "3086 [D loss: 0.163204, acc.: 95.31%] [G loss: 2.601300]\n",
      "3087 [D loss: 0.180991, acc.: 93.75%] [G loss: 2.612499]\n",
      "3088 [D loss: 0.182456, acc.: 95.31%] [G loss: 2.848484]\n",
      "3089 [D loss: 0.184032, acc.: 93.75%] [G loss: 2.637867]\n",
      "3090 [D loss: 0.165507, acc.: 93.75%] [G loss: 3.265715]\n",
      "3091 [D loss: 0.179902, acc.: 95.31%] [G loss: 3.032936]\n",
      "3092 [D loss: 0.183519, acc.: 93.75%] [G loss: 3.010188]\n",
      "3093 [D loss: 0.162030, acc.: 95.31%] [G loss: 3.180872]\n",
      "3094 [D loss: 0.199087, acc.: 93.75%] [G loss: 2.873912]\n",
      "3095 [D loss: 0.133626, acc.: 95.31%] [G loss: 3.331378]\n",
      "3096 [D loss: 0.148012, acc.: 95.31%] [G loss: 3.074992]\n",
      "3097 [D loss: 0.158890, acc.: 95.31%] [G loss: 2.475752]\n",
      "3098 [D loss: 0.145177, acc.: 95.31%] [G loss: 3.653757]\n",
      "3099 [D loss: 0.202647, acc.: 90.62%] [G loss: 2.906147]\n",
      "3100 [D loss: 0.181600, acc.: 93.75%] [G loss: 3.156900]\n",
      "generated_data\n",
      "3101 [D loss: 0.215420, acc.: 90.62%] [G loss: 2.983672]\n",
      "3102 [D loss: 0.168976, acc.: 95.31%] [G loss: 2.963469]\n",
      "3103 [D loss: 0.256203, acc.: 90.62%] [G loss: 2.910777]\n",
      "3104 [D loss: 0.202994, acc.: 93.75%] [G loss: 2.877887]\n",
      "3105 [D loss: 0.224283, acc.: 93.75%] [G loss: 2.961365]\n",
      "3106 [D loss: 0.199630, acc.: 93.75%] [G loss: 3.443207]\n",
      "3107 [D loss: 0.161717, acc.: 95.31%] [G loss: 2.894235]\n",
      "3108 [D loss: 0.202373, acc.: 93.75%] [G loss: 3.216470]\n",
      "3109 [D loss: 0.135670, acc.: 95.31%] [G loss: 3.207972]\n",
      "3110 [D loss: 0.184504, acc.: 93.75%] [G loss: 2.706430]\n",
      "3111 [D loss: 0.153467, acc.: 95.31%] [G loss: 3.050921]\n",
      "3112 [D loss: 0.158519, acc.: 93.75%] [G loss: 3.232577]\n",
      "3113 [D loss: 0.171043, acc.: 95.31%] [G loss: 3.009617]\n",
      "3114 [D loss: 0.165887, acc.: 95.31%] [G loss: 2.744437]\n",
      "3115 [D loss: 0.214495, acc.: 93.75%] [G loss: 2.808381]\n",
      "3116 [D loss: 0.163151, acc.: 95.31%] [G loss: 3.185419]\n",
      "3117 [D loss: 0.195121, acc.: 93.75%] [G loss: 3.067203]\n",
      "3118 [D loss: 0.199960, acc.: 93.75%] [G loss: 3.290402]\n",
      "3119 [D loss: 0.190004, acc.: 93.75%] [G loss: 2.987900]\n",
      "3120 [D loss: 0.206796, acc.: 92.19%] [G loss: 3.106250]\n",
      "3121 [D loss: 0.150282, acc.: 95.31%] [G loss: 3.156125]\n",
      "3122 [D loss: 0.173291, acc.: 93.75%] [G loss: 2.791958]\n",
      "3123 [D loss: 0.130994, acc.: 95.31%] [G loss: 2.934973]\n",
      "3124 [D loss: 0.156405, acc.: 95.31%] [G loss: 3.108788]\n",
      "3125 [D loss: 0.195802, acc.: 92.19%] [G loss: 3.084640]\n",
      "3126 [D loss: 0.168789, acc.: 93.75%] [G loss: 2.788581]\n",
      "3127 [D loss: 0.191037, acc.: 93.75%] [G loss: 3.228311]\n",
      "3128 [D loss: 0.153833, acc.: 95.31%] [G loss: 3.306479]\n",
      "3129 [D loss: 0.189610, acc.: 95.31%] [G loss: 2.832419]\n",
      "3130 [D loss: 0.182789, acc.: 93.75%] [G loss: 3.054391]\n",
      "3131 [D loss: 0.153591, acc.: 95.31%] [G loss: 3.067594]\n",
      "3132 [D loss: 0.194219, acc.: 93.75%] [G loss: 2.952328]\n",
      "3133 [D loss: 0.132543, acc.: 95.31%] [G loss: 3.216293]\n",
      "3134 [D loss: 0.180187, acc.: 93.75%] [G loss: 2.744413]\n",
      "3135 [D loss: 0.213843, acc.: 92.19%] [G loss: 3.111475]\n",
      "3136 [D loss: 0.180039, acc.: 95.31%] [G loss: 2.805152]\n",
      "3137 [D loss: 0.183652, acc.: 93.75%] [G loss: 2.791576]\n",
      "3138 [D loss: 0.247840, acc.: 93.75%] [G loss: 3.196551]\n",
      "3139 [D loss: 0.186762, acc.: 95.31%] [G loss: 3.078106]\n",
      "3140 [D loss: 0.177610, acc.: 95.31%] [G loss: 2.791592]\n",
      "3141 [D loss: 0.201316, acc.: 93.75%] [G loss: 3.163877]\n",
      "3142 [D loss: 0.184678, acc.: 92.19%] [G loss: 3.008971]\n",
      "3143 [D loss: 0.190580, acc.: 93.75%] [G loss: 3.021612]\n",
      "3144 [D loss: 0.182305, acc.: 93.75%] [G loss: 2.945257]\n",
      "3145 [D loss: 0.150919, acc.: 95.31%] [G loss: 3.214823]\n",
      "3146 [D loss: 0.195613, acc.: 93.75%] [G loss: 2.931213]\n",
      "3147 [D loss: 0.187731, acc.: 95.31%] [G loss: 2.687876]\n",
      "3148 [D loss: 0.205514, acc.: 93.75%] [G loss: 2.572730]\n",
      "3149 [D loss: 0.179530, acc.: 93.75%] [G loss: 2.561911]\n",
      "3150 [D loss: 0.200506, acc.: 93.75%] [G loss: 2.880942]\n",
      "3151 [D loss: 0.145716, acc.: 95.31%] [G loss: 3.143852]\n",
      "3152 [D loss: 0.211151, acc.: 92.19%] [G loss: 3.545189]\n",
      "3153 [D loss: 0.178295, acc.: 95.31%] [G loss: 2.837525]\n",
      "3154 [D loss: 0.150014, acc.: 93.75%] [G loss: 3.162352]\n",
      "3155 [D loss: 0.167062, acc.: 95.31%] [G loss: 3.054544]\n",
      "3156 [D loss: 0.170245, acc.: 95.31%] [G loss: 2.981077]\n",
      "3157 [D loss: 0.188412, acc.: 95.31%] [G loss: 3.068881]\n",
      "3158 [D loss: 0.159411, acc.: 95.31%] [G loss: 3.549892]\n",
      "3159 [D loss: 0.193747, acc.: 95.31%] [G loss: 3.068234]\n",
      "3160 [D loss: 0.187474, acc.: 93.75%] [G loss: 3.178243]\n",
      "3161 [D loss: 0.152199, acc.: 95.31%] [G loss: 3.507792]\n",
      "3162 [D loss: 0.159807, acc.: 95.31%] [G loss: 2.681892]\n",
      "3163 [D loss: 0.152071, acc.: 95.31%] [G loss: 3.314446]\n",
      "3164 [D loss: 0.164301, acc.: 93.75%] [G loss: 3.144103]\n",
      "3165 [D loss: 0.198784, acc.: 93.75%] [G loss: 2.796295]\n",
      "3166 [D loss: 0.177604, acc.: 95.31%] [G loss: 2.750643]\n",
      "3167 [D loss: 0.153503, acc.: 95.31%] [G loss: 3.368308]\n",
      "3168 [D loss: 0.183866, acc.: 95.31%] [G loss: 2.586885]\n",
      "3169 [D loss: 0.145031, acc.: 95.31%] [G loss: 2.772049]\n",
      "3170 [D loss: 0.188828, acc.: 92.19%] [G loss: 2.932077]\n",
      "3171 [D loss: 0.160035, acc.: 95.31%] [G loss: 3.016630]\n",
      "3172 [D loss: 0.177059, acc.: 95.31%] [G loss: 3.217801]\n",
      "3173 [D loss: 0.161016, acc.: 95.31%] [G loss: 3.292430]\n",
      "3174 [D loss: 0.154884, acc.: 95.31%] [G loss: 2.625941]\n",
      "3175 [D loss: 0.174573, acc.: 93.75%] [G loss: 3.189814]\n",
      "3176 [D loss: 0.158321, acc.: 95.31%] [G loss: 3.516153]\n",
      "3177 [D loss: 0.205979, acc.: 93.75%] [G loss: 2.772675]\n",
      "3178 [D loss: 0.123654, acc.: 95.31%] [G loss: 3.485448]\n",
      "3179 [D loss: 0.214425, acc.: 92.19%] [G loss: 3.145932]\n",
      "3180 [D loss: 0.142491, acc.: 93.75%] [G loss: 3.499586]\n",
      "3181 [D loss: 0.173974, acc.: 93.75%] [G loss: 3.232566]\n",
      "3182 [D loss: 0.123454, acc.: 95.31%] [G loss: 3.235708]\n",
      "3183 [D loss: 0.196262, acc.: 95.31%] [G loss: 2.538712]\n",
      "3184 [D loss: 0.188609, acc.: 92.19%] [G loss: 3.294308]\n",
      "3185 [D loss: 0.188229, acc.: 93.75%] [G loss: 3.193013]\n",
      "3186 [D loss: 0.157979, acc.: 93.75%] [G loss: 3.494326]\n",
      "3187 [D loss: 0.196868, acc.: 95.31%] [G loss: 3.018611]\n",
      "3188 [D loss: 0.181110, acc.: 95.31%] [G loss: 2.894558]\n",
      "3189 [D loss: 0.165148, acc.: 93.75%] [G loss: 3.225952]\n",
      "3190 [D loss: 0.194874, acc.: 92.19%] [G loss: 3.109466]\n",
      "3191 [D loss: 0.178799, acc.: 93.75%] [G loss: 3.051232]\n",
      "3192 [D loss: 0.180395, acc.: 93.75%] [G loss: 2.966653]\n",
      "3193 [D loss: 0.225242, acc.: 92.19%] [G loss: 3.065621]\n",
      "3194 [D loss: 0.177493, acc.: 93.75%] [G loss: 3.337720]\n",
      "3195 [D loss: 0.195335, acc.: 93.75%] [G loss: 2.951187]\n",
      "3196 [D loss: 0.196424, acc.: 92.19%] [G loss: 3.028556]\n",
      "3197 [D loss: 0.142157, acc.: 95.31%] [G loss: 3.118565]\n",
      "3198 [D loss: 0.202795, acc.: 92.19%] [G loss: 2.900160]\n",
      "3199 [D loss: 0.184969, acc.: 93.75%] [G loss: 2.865333]\n",
      "3200 [D loss: 0.154038, acc.: 93.75%] [G loss: 2.909205]\n",
      "generated_data\n",
      "3201 [D loss: 0.175010, acc.: 95.31%] [G loss: 2.724337]\n",
      "3202 [D loss: 0.179394, acc.: 95.31%] [G loss: 2.953136]\n",
      "3203 [D loss: 0.129378, acc.: 93.75%] [G loss: 3.409289]\n",
      "3204 [D loss: 0.192136, acc.: 93.75%] [G loss: 3.062361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3205 [D loss: 0.141091, acc.: 95.31%] [G loss: 3.392805]\n",
      "3206 [D loss: 0.178701, acc.: 95.31%] [G loss: 2.784760]\n",
      "3207 [D loss: 0.220966, acc.: 93.75%] [G loss: 3.191514]\n",
      "3208 [D loss: 0.165038, acc.: 93.75%] [G loss: 3.247540]\n",
      "3209 [D loss: 0.172585, acc.: 95.31%] [G loss: 2.875423]\n",
      "3210 [D loss: 0.183430, acc.: 92.19%] [G loss: 3.344539]\n",
      "3211 [D loss: 0.218308, acc.: 93.75%] [G loss: 3.002678]\n",
      "3212 [D loss: 0.170697, acc.: 95.31%] [G loss: 2.787153]\n",
      "3213 [D loss: 0.241101, acc.: 92.19%] [G loss: 3.167063]\n",
      "3214 [D loss: 0.178489, acc.: 93.75%] [G loss: 3.735004]\n",
      "3215 [D loss: 0.188707, acc.: 93.75%] [G loss: 3.127728]\n",
      "3216 [D loss: 0.144030, acc.: 95.31%] [G loss: 2.821305]\n",
      "3217 [D loss: 0.180429, acc.: 93.75%] [G loss: 2.961745]\n",
      "3218 [D loss: 0.177607, acc.: 95.31%] [G loss: 2.863879]\n",
      "3219 [D loss: 0.176249, acc.: 93.75%] [G loss: 2.928059]\n",
      "3220 [D loss: 0.258144, acc.: 90.62%] [G loss: 3.150644]\n",
      "3221 [D loss: 0.167344, acc.: 92.19%] [G loss: 3.050965]\n",
      "3222 [D loss: 0.185829, acc.: 95.31%] [G loss: 2.615615]\n",
      "3223 [D loss: 0.167734, acc.: 95.31%] [G loss: 2.666261]\n",
      "3224 [D loss: 0.138069, acc.: 95.31%] [G loss: 3.124274]\n",
      "3225 [D loss: 0.132153, acc.: 95.31%] [G loss: 3.004155]\n",
      "3226 [D loss: 0.165370, acc.: 95.31%] [G loss: 2.615700]\n",
      "3227 [D loss: 0.157660, acc.: 95.31%] [G loss: 2.873966]\n",
      "3228 [D loss: 0.188273, acc.: 93.75%] [G loss: 3.179533]\n",
      "3229 [D loss: 0.173110, acc.: 95.31%] [G loss: 2.849068]\n",
      "3230 [D loss: 0.159900, acc.: 95.31%] [G loss: 3.071585]\n",
      "3231 [D loss: 0.229288, acc.: 92.19%] [G loss: 3.334873]\n",
      "3232 [D loss: 0.214511, acc.: 92.19%] [G loss: 3.215309]\n",
      "3233 [D loss: 0.209317, acc.: 93.75%] [G loss: 2.529646]\n",
      "3234 [D loss: 0.162799, acc.: 95.31%] [G loss: 2.769699]\n",
      "3235 [D loss: 0.179558, acc.: 93.75%] [G loss: 2.802319]\n",
      "3236 [D loss: 0.180096, acc.: 93.75%] [G loss: 3.405420]\n",
      "3237 [D loss: 0.222193, acc.: 93.75%] [G loss: 2.869344]\n",
      "3238 [D loss: 0.219232, acc.: 92.19%] [G loss: 3.024995]\n",
      "3239 [D loss: 0.153055, acc.: 95.31%] [G loss: 3.243013]\n",
      "3240 [D loss: 0.188311, acc.: 93.75%] [G loss: 2.695129]\n",
      "3241 [D loss: 0.125403, acc.: 95.31%] [G loss: 3.611639]\n",
      "3242 [D loss: 0.204103, acc.: 92.19%] [G loss: 2.969676]\n",
      "3243 [D loss: 0.204866, acc.: 93.75%] [G loss: 2.662634]\n",
      "3244 [D loss: 0.191795, acc.: 93.75%] [G loss: 3.252455]\n",
      "3245 [D loss: 0.177678, acc.: 93.75%] [G loss: 2.963100]\n",
      "3246 [D loss: 0.159435, acc.: 95.31%] [G loss: 2.985232]\n",
      "3247 [D loss: 0.197966, acc.: 90.62%] [G loss: 3.145889]\n",
      "3248 [D loss: 0.163078, acc.: 93.75%] [G loss: 3.066764]\n",
      "3249 [D loss: 0.185454, acc.: 93.75%] [G loss: 2.859603]\n",
      "3250 [D loss: 0.160490, acc.: 93.75%] [G loss: 3.070951]\n",
      "3251 [D loss: 0.163604, acc.: 95.31%] [G loss: 2.981256]\n",
      "3252 [D loss: 0.190302, acc.: 95.31%] [G loss: 3.082360]\n",
      "3253 [D loss: 0.152986, acc.: 95.31%] [G loss: 3.179054]\n",
      "3254 [D loss: 0.174999, acc.: 93.75%] [G loss: 2.925112]\n",
      "3255 [D loss: 0.181716, acc.: 93.75%] [G loss: 2.729280]\n",
      "3256 [D loss: 0.145741, acc.: 93.75%] [G loss: 3.332997]\n",
      "3257 [D loss: 0.204135, acc.: 93.75%] [G loss: 2.875633]\n",
      "3258 [D loss: 0.208609, acc.: 92.19%] [G loss: 2.930435]\n",
      "3259 [D loss: 0.186536, acc.: 95.31%] [G loss: 3.219056]\n",
      "3260 [D loss: 0.207032, acc.: 92.19%] [G loss: 3.414665]\n",
      "3261 [D loss: 0.144558, acc.: 95.31%] [G loss: 2.962415]\n",
      "3262 [D loss: 0.158630, acc.: 95.31%] [G loss: 2.645308]\n",
      "3263 [D loss: 0.189164, acc.: 93.75%] [G loss: 2.989125]\n",
      "3264 [D loss: 0.175948, acc.: 93.75%] [G loss: 3.050974]\n",
      "3265 [D loss: 0.180968, acc.: 95.31%] [G loss: 2.722215]\n",
      "3266 [D loss: 0.245517, acc.: 92.19%] [G loss: 3.073678]\n",
      "3267 [D loss: 0.200587, acc.: 92.19%] [G loss: 3.100194]\n",
      "3268 [D loss: 0.205778, acc.: 92.19%] [G loss: 2.876448]\n",
      "3269 [D loss: 0.159135, acc.: 95.31%] [G loss: 2.933576]\n",
      "3270 [D loss: 0.176623, acc.: 95.31%] [G loss: 2.958327]\n",
      "3271 [D loss: 0.147418, acc.: 95.31%] [G loss: 2.890932]\n",
      "3272 [D loss: 0.163105, acc.: 93.75%] [G loss: 2.997536]\n",
      "3273 [D loss: 0.205317, acc.: 92.19%] [G loss: 2.993092]\n",
      "3274 [D loss: 0.200067, acc.: 92.19%] [G loss: 2.746348]\n",
      "3275 [D loss: 0.163072, acc.: 95.31%] [G loss: 3.543421]\n",
      "3276 [D loss: 0.229830, acc.: 92.19%] [G loss: 3.357120]\n",
      "3277 [D loss: 0.183301, acc.: 93.75%] [G loss: 3.467231]\n",
      "3278 [D loss: 0.195485, acc.: 93.75%] [G loss: 2.714336]\n",
      "3279 [D loss: 0.202519, acc.: 93.75%] [G loss: 3.142894]\n",
      "3280 [D loss: 0.149713, acc.: 95.31%] [G loss: 2.899879]\n",
      "3281 [D loss: 0.175384, acc.: 95.31%] [G loss: 2.584792]\n",
      "3282 [D loss: 0.187748, acc.: 93.75%] [G loss: 2.859929]\n",
      "3283 [D loss: 0.184113, acc.: 95.31%] [G loss: 3.231203]\n",
      "3284 [D loss: 0.178754, acc.: 93.75%] [G loss: 3.321298]\n",
      "3285 [D loss: 0.176701, acc.: 95.31%] [G loss: 3.258592]\n",
      "3286 [D loss: 0.161268, acc.: 95.31%] [G loss: 2.855455]\n",
      "3287 [D loss: 0.128368, acc.: 95.31%] [G loss: 3.202304]\n",
      "3288 [D loss: 0.165128, acc.: 93.75%] [G loss: 2.871275]\n",
      "3289 [D loss: 0.153037, acc.: 95.31%] [G loss: 2.933342]\n",
      "3290 [D loss: 0.168673, acc.: 95.31%] [G loss: 2.719561]\n",
      "3291 [D loss: 0.183202, acc.: 95.31%] [G loss: 2.888399]\n",
      "3292 [D loss: 0.183990, acc.: 95.31%] [G loss: 3.295252]\n",
      "3293 [D loss: 0.152648, acc.: 95.31%] [G loss: 3.244643]\n",
      "3294 [D loss: 0.184828, acc.: 95.31%] [G loss: 3.005421]\n",
      "3295 [D loss: 0.127654, acc.: 95.31%] [G loss: 3.614262]\n",
      "3296 [D loss: 0.156636, acc.: 93.75%] [G loss: 2.816161]\n",
      "3297 [D loss: 0.211967, acc.: 92.19%] [G loss: 2.820113]\n",
      "3298 [D loss: 0.192789, acc.: 93.75%] [G loss: 3.076845]\n",
      "3299 [D loss: 0.192916, acc.: 92.19%] [G loss: 2.807197]\n",
      "3300 [D loss: 0.167076, acc.: 95.31%] [G loss: 3.070542]\n",
      "generated_data\n",
      "3301 [D loss: 0.183415, acc.: 93.75%] [G loss: 3.052257]\n",
      "3302 [D loss: 0.164543, acc.: 95.31%] [G loss: 3.183789]\n",
      "3303 [D loss: 0.154768, acc.: 95.31%] [G loss: 3.330709]\n",
      "3304 [D loss: 0.175942, acc.: 95.31%] [G loss: 3.028664]\n",
      "3305 [D loss: 0.150129, acc.: 95.31%] [G loss: 3.086657]\n",
      "3306 [D loss: 0.193436, acc.: 93.75%] [G loss: 3.168709]\n",
      "3307 [D loss: 0.132648, acc.: 95.31%] [G loss: 3.751150]\n",
      "3308 [D loss: 0.197037, acc.: 93.75%] [G loss: 2.957826]\n",
      "3309 [D loss: 0.137836, acc.: 95.31%] [G loss: 3.697185]\n",
      "3310 [D loss: 0.138192, acc.: 95.31%] [G loss: 3.271253]\n",
      "3311 [D loss: 0.174354, acc.: 95.31%] [G loss: 3.232255]\n",
      "3312 [D loss: 0.173453, acc.: 93.75%] [G loss: 2.963656]\n",
      "3313 [D loss: 0.141539, acc.: 95.31%] [G loss: 3.539242]\n",
      "3314 [D loss: 0.165872, acc.: 95.31%] [G loss: 2.963605]\n",
      "3315 [D loss: 0.166127, acc.: 93.75%] [G loss: 3.113872]\n",
      "3316 [D loss: 0.143118, acc.: 95.31%] [G loss: 3.299632]\n",
      "3317 [D loss: 0.154761, acc.: 95.31%] [G loss: 2.634171]\n",
      "3318 [D loss: 0.201045, acc.: 93.75%] [G loss: 3.020331]\n",
      "3319 [D loss: 0.156498, acc.: 95.31%] [G loss: 3.220487]\n",
      "3320 [D loss: 0.227085, acc.: 92.19%] [G loss: 2.989300]\n",
      "3321 [D loss: 0.138009, acc.: 93.75%] [G loss: 3.256436]\n",
      "3322 [D loss: 0.152908, acc.: 95.31%] [G loss: 2.937313]\n",
      "3323 [D loss: 0.163705, acc.: 95.31%] [G loss: 2.896242]\n",
      "3324 [D loss: 0.145950, acc.: 95.31%] [G loss: 3.382685]\n",
      "3325 [D loss: 0.153990, acc.: 93.75%] [G loss: 3.396154]\n",
      "3326 [D loss: 0.193019, acc.: 90.62%] [G loss: 3.038706]\n",
      "3327 [D loss: 0.189903, acc.: 93.75%] [G loss: 3.410441]\n",
      "3328 [D loss: 0.207859, acc.: 93.75%] [G loss: 2.833210]\n",
      "3329 [D loss: 0.170179, acc.: 95.31%] [G loss: 2.959579]\n",
      "3330 [D loss: 0.183695, acc.: 92.19%] [G loss: 2.954124]\n",
      "3331 [D loss: 0.167516, acc.: 95.31%] [G loss: 2.872938]\n",
      "3332 [D loss: 0.175645, acc.: 93.75%] [G loss: 3.271667]\n",
      "3333 [D loss: 0.180551, acc.: 95.31%] [G loss: 2.643642]\n",
      "3334 [D loss: 0.150299, acc.: 95.31%] [G loss: 2.862062]\n",
      "3335 [D loss: 0.194140, acc.: 93.75%] [G loss: 3.161923]\n",
      "3336 [D loss: 0.145286, acc.: 95.31%] [G loss: 3.253609]\n",
      "3337 [D loss: 0.179867, acc.: 95.31%] [G loss: 2.979061]\n",
      "3338 [D loss: 0.181422, acc.: 95.31%] [G loss: 2.767828]\n",
      "3339 [D loss: 0.192505, acc.: 93.75%] [G loss: 2.847199]\n",
      "3340 [D loss: 0.194579, acc.: 93.75%] [G loss: 3.150078]\n",
      "3341 [D loss: 0.150793, acc.: 93.75%] [G loss: 3.426814]\n",
      "3342 [D loss: 0.157228, acc.: 95.31%] [G loss: 2.807622]\n",
      "3343 [D loss: 0.160089, acc.: 95.31%] [G loss: 3.072114]\n",
      "3344 [D loss: 0.143537, acc.: 95.31%] [G loss: 3.160709]\n",
      "3345 [D loss: 0.186488, acc.: 93.75%] [G loss: 2.720606]\n",
      "3346 [D loss: 0.160867, acc.: 92.19%] [G loss: 3.627317]\n",
      "3347 [D loss: 0.185302, acc.: 95.31%] [G loss: 2.919833]\n",
      "3348 [D loss: 0.171085, acc.: 95.31%] [G loss: 2.773699]\n",
      "3349 [D loss: 0.172691, acc.: 95.31%] [G loss: 3.866940]\n",
      "3350 [D loss: 0.166494, acc.: 95.31%] [G loss: 2.981176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3351 [D loss: 0.151380, acc.: 95.31%] [G loss: 2.796909]\n",
      "3352 [D loss: 0.142261, acc.: 95.31%] [G loss: 3.217343]\n",
      "3353 [D loss: 0.179257, acc.: 95.31%] [G loss: 2.914738]\n",
      "3354 [D loss: 0.177262, acc.: 95.31%] [G loss: 2.963011]\n",
      "3355 [D loss: 0.159453, acc.: 95.31%] [G loss: 2.861078]\n",
      "3356 [D loss: 0.168020, acc.: 93.75%] [G loss: 3.346997]\n",
      "3357 [D loss: 0.139738, acc.: 95.31%] [G loss: 3.267825]\n",
      "3358 [D loss: 0.210616, acc.: 93.75%] [G loss: 2.875315]\n",
      "3359 [D loss: 0.144646, acc.: 95.31%] [G loss: 2.829127]\n",
      "3360 [D loss: 0.183415, acc.: 95.31%] [G loss: 2.971408]\n",
      "3361 [D loss: 0.200132, acc.: 90.62%] [G loss: 3.099749]\n",
      "3362 [D loss: 0.166719, acc.: 93.75%] [G loss: 2.818279]\n",
      "3363 [D loss: 0.147528, acc.: 93.75%] [G loss: 2.661205]\n",
      "3364 [D loss: 0.178441, acc.: 95.31%] [G loss: 3.100656]\n",
      "3365 [D loss: 0.180380, acc.: 93.75%] [G loss: 3.202363]\n",
      "3366 [D loss: 0.150273, acc.: 95.31%] [G loss: 2.658529]\n",
      "3367 [D loss: 0.160899, acc.: 95.31%] [G loss: 2.673534]\n",
      "3368 [D loss: 0.218831, acc.: 93.75%] [G loss: 2.892920]\n",
      "3369 [D loss: 0.174528, acc.: 95.31%] [G loss: 3.047535]\n",
      "3370 [D loss: 0.157830, acc.: 95.31%] [G loss: 2.957736]\n",
      "3371 [D loss: 0.194422, acc.: 93.75%] [G loss: 3.056572]\n",
      "3372 [D loss: 0.156533, acc.: 95.31%] [G loss: 3.217379]\n",
      "3373 [D loss: 0.151500, acc.: 95.31%] [G loss: 3.141022]\n",
      "3374 [D loss: 0.157980, acc.: 95.31%] [G loss: 2.929881]\n",
      "3375 [D loss: 0.204456, acc.: 93.75%] [G loss: 3.020070]\n",
      "3376 [D loss: 0.160256, acc.: 95.31%] [G loss: 2.622466]\n",
      "3377 [D loss: 0.124123, acc.: 95.31%] [G loss: 3.386441]\n",
      "3378 [D loss: 0.181729, acc.: 95.31%] [G loss: 3.047979]\n",
      "3379 [D loss: 0.139040, acc.: 95.31%] [G loss: 3.239566]\n",
      "3380 [D loss: 0.170474, acc.: 95.31%] [G loss: 3.328239]\n",
      "3381 [D loss: 0.200588, acc.: 92.19%] [G loss: 2.846102]\n",
      "3382 [D loss: 0.174513, acc.: 93.75%] [G loss: 3.347503]\n",
      "3383 [D loss: 0.171940, acc.: 93.75%] [G loss: 2.822931]\n",
      "3384 [D loss: 0.172495, acc.: 95.31%] [G loss: 2.769432]\n",
      "3385 [D loss: 0.141540, acc.: 95.31%] [G loss: 3.302780]\n",
      "3386 [D loss: 0.172016, acc.: 95.31%] [G loss: 3.158513]\n",
      "3387 [D loss: 0.172184, acc.: 93.75%] [G loss: 3.130954]\n",
      "3388 [D loss: 0.147716, acc.: 95.31%] [G loss: 3.093982]\n",
      "3389 [D loss: 0.164187, acc.: 93.75%] [G loss: 3.270782]\n",
      "3390 [D loss: 0.164439, acc.: 95.31%] [G loss: 2.986338]\n",
      "3391 [D loss: 0.166341, acc.: 95.31%] [G loss: 2.658818]\n",
      "3392 [D loss: 0.173900, acc.: 95.31%] [G loss: 2.684477]\n",
      "3393 [D loss: 0.156389, acc.: 95.31%] [G loss: 2.754508]\n",
      "3394 [D loss: 0.141646, acc.: 95.31%] [G loss: 3.516757]\n",
      "3395 [D loss: 0.166187, acc.: 93.75%] [G loss: 3.470881]\n",
      "3396 [D loss: 0.152678, acc.: 95.31%] [G loss: 3.043593]\n",
      "3397 [D loss: 0.171921, acc.: 95.31%] [G loss: 2.439549]\n",
      "3398 [D loss: 0.190810, acc.: 93.75%] [G loss: 3.054014]\n",
      "3399 [D loss: 0.163532, acc.: 93.75%] [G loss: 3.089242]\n",
      "3400 [D loss: 0.175402, acc.: 93.75%] [G loss: 2.889901]\n",
      "generated_data\n",
      "3401 [D loss: 0.160051, acc.: 95.31%] [G loss: 2.755938]\n",
      "3402 [D loss: 0.174660, acc.: 93.75%] [G loss: 3.006233]\n",
      "3403 [D loss: 0.188863, acc.: 93.75%] [G loss: 3.464394]\n",
      "3404 [D loss: 0.205238, acc.: 92.19%] [G loss: 3.029657]\n",
      "3405 [D loss: 0.217645, acc.: 93.75%] [G loss: 2.993139]\n",
      "3406 [D loss: 0.166968, acc.: 93.75%] [G loss: 3.207115]\n",
      "3407 [D loss: 0.167288, acc.: 95.31%] [G loss: 2.598819]\n",
      "3408 [D loss: 0.167961, acc.: 95.31%] [G loss: 2.843686]\n",
      "3409 [D loss: 0.135855, acc.: 95.31%] [G loss: 2.857892]\n",
      "3410 [D loss: 0.169705, acc.: 95.31%] [G loss: 3.179268]\n",
      "3411 [D loss: 0.172193, acc.: 93.75%] [G loss: 3.090504]\n",
      "3412 [D loss: 0.196474, acc.: 95.31%] [G loss: 3.011620]\n",
      "3413 [D loss: 0.169459, acc.: 95.31%] [G loss: 2.917887]\n",
      "3414 [D loss: 0.144161, acc.: 95.31%] [G loss: 3.363251]\n",
      "3415 [D loss: 0.201320, acc.: 92.19%] [G loss: 2.927276]\n",
      "3416 [D loss: 0.174770, acc.: 93.75%] [G loss: 3.167242]\n",
      "3417 [D loss: 0.166721, acc.: 93.75%] [G loss: 2.845206]\n",
      "3418 [D loss: 0.183433, acc.: 95.31%] [G loss: 2.827798]\n",
      "3419 [D loss: 0.150003, acc.: 95.31%] [G loss: 3.116824]\n",
      "3420 [D loss: 0.161717, acc.: 95.31%] [G loss: 3.135122]\n",
      "3421 [D loss: 0.154123, acc.: 95.31%] [G loss: 2.949812]\n",
      "3422 [D loss: 0.148761, acc.: 95.31%] [G loss: 3.146622]\n",
      "3423 [D loss: 0.152263, acc.: 95.31%] [G loss: 3.142469]\n",
      "3424 [D loss: 0.199167, acc.: 93.75%] [G loss: 2.971038]\n",
      "3425 [D loss: 0.159048, acc.: 95.31%] [G loss: 3.097071]\n",
      "3426 [D loss: 0.149733, acc.: 95.31%] [G loss: 2.941511]\n",
      "3427 [D loss: 0.174147, acc.: 95.31%] [G loss: 2.807115]\n",
      "3428 [D loss: 0.151315, acc.: 95.31%] [G loss: 2.813079]\n",
      "3429 [D loss: 0.169586, acc.: 95.31%] [G loss: 3.209099]\n",
      "3430 [D loss: 0.179819, acc.: 93.75%] [G loss: 3.377583]\n",
      "3431 [D loss: 0.149111, acc.: 93.75%] [G loss: 2.997888]\n",
      "3432 [D loss: 0.211730, acc.: 93.75%] [G loss: 2.952126]\n",
      "3433 [D loss: 0.162763, acc.: 95.31%] [G loss: 3.102140]\n",
      "3434 [D loss: 0.170172, acc.: 95.31%] [G loss: 3.210010]\n",
      "3435 [D loss: 0.123537, acc.: 95.31%] [G loss: 3.404301]\n",
      "3436 [D loss: 0.174526, acc.: 93.75%] [G loss: 2.837633]\n",
      "3437 [D loss: 0.152397, acc.: 95.31%] [G loss: 3.333170]\n",
      "3438 [D loss: 0.158414, acc.: 95.31%] [G loss: 2.853500]\n",
      "3439 [D loss: 0.128541, acc.: 95.31%] [G loss: 3.227375]\n",
      "3440 [D loss: 0.137693, acc.: 95.31%] [G loss: 3.453187]\n",
      "3441 [D loss: 0.137358, acc.: 95.31%] [G loss: 2.960245]\n",
      "3442 [D loss: 0.124133, acc.: 95.31%] [G loss: 3.480606]\n",
      "3443 [D loss: 0.172529, acc.: 93.75%] [G loss: 2.995002]\n",
      "3444 [D loss: 0.154956, acc.: 95.31%] [G loss: 3.020797]\n",
      "3445 [D loss: 0.174529, acc.: 95.31%] [G loss: 2.690122]\n",
      "3446 [D loss: 0.157220, acc.: 95.31%] [G loss: 3.266485]\n",
      "3447 [D loss: 0.150853, acc.: 95.31%] [G loss: 3.385673]\n",
      "3448 [D loss: 0.174487, acc.: 95.31%] [G loss: 2.727385]\n",
      "3449 [D loss: 0.137020, acc.: 95.31%] [G loss: 3.310904]\n",
      "3450 [D loss: 0.173938, acc.: 95.31%] [G loss: 2.818674]\n",
      "3451 [D loss: 0.151588, acc.: 95.31%] [G loss: 3.029943]\n",
      "3452 [D loss: 0.171829, acc.: 93.75%] [G loss: 3.171176]\n",
      "3453 [D loss: 0.192536, acc.: 95.31%] [G loss: 2.912319]\n",
      "3454 [D loss: 0.159359, acc.: 93.75%] [G loss: 3.155155]\n",
      "3455 [D loss: 0.176814, acc.: 93.75%] [G loss: 3.006371]\n",
      "3456 [D loss: 0.146840, acc.: 95.31%] [G loss: 3.220313]\n",
      "3457 [D loss: 0.180939, acc.: 92.19%] [G loss: 3.613839]\n",
      "3458 [D loss: 0.155888, acc.: 95.31%] [G loss: 2.738020]\n",
      "3459 [D loss: 0.171438, acc.: 93.75%] [G loss: 3.549934]\n",
      "3460 [D loss: 0.161643, acc.: 95.31%] [G loss: 2.972155]\n",
      "3461 [D loss: 0.156969, acc.: 95.31%] [G loss: 3.315148]\n",
      "3462 [D loss: 0.187344, acc.: 93.75%] [G loss: 2.855781]\n",
      "3463 [D loss: 0.142021, acc.: 95.31%] [G loss: 3.074006]\n",
      "3464 [D loss: 0.152594, acc.: 95.31%] [G loss: 3.128018]\n",
      "3465 [D loss: 0.151482, acc.: 95.31%] [G loss: 2.937792]\n",
      "3466 [D loss: 0.182269, acc.: 93.75%] [G loss: 3.175524]\n",
      "3467 [D loss: 0.140543, acc.: 95.31%] [G loss: 2.818871]\n",
      "3468 [D loss: 0.172640, acc.: 95.31%] [G loss: 3.152052]\n",
      "3469 [D loss: 0.146360, acc.: 95.31%] [G loss: 3.073203]\n",
      "3470 [D loss: 0.168970, acc.: 95.31%] [G loss: 2.952267]\n",
      "3471 [D loss: 0.175338, acc.: 93.75%] [G loss: 2.982617]\n",
      "3472 [D loss: 0.212920, acc.: 93.75%] [G loss: 2.995043]\n",
      "3473 [D loss: 0.159424, acc.: 95.31%] [G loss: 3.425054]\n",
      "3474 [D loss: 0.190560, acc.: 93.75%] [G loss: 3.078276]\n",
      "3475 [D loss: 0.183303, acc.: 93.75%] [G loss: 3.223590]\n",
      "3476 [D loss: 0.147225, acc.: 95.31%] [G loss: 3.150728]\n",
      "3477 [D loss: 0.152661, acc.: 95.31%] [G loss: 3.176620]\n",
      "3478 [D loss: 0.164170, acc.: 95.31%] [G loss: 3.658615]\n",
      "3479 [D loss: 0.168082, acc.: 95.31%] [G loss: 2.757782]\n",
      "3480 [D loss: 0.145153, acc.: 95.31%] [G loss: 3.324654]\n",
      "3481 [D loss: 0.160315, acc.: 95.31%] [G loss: 2.897382]\n",
      "3482 [D loss: 0.172307, acc.: 93.75%] [G loss: 3.011838]\n",
      "3483 [D loss: 0.169467, acc.: 95.31%] [G loss: 2.962915]\n",
      "3484 [D loss: 0.167789, acc.: 93.75%] [G loss: 3.439243]\n",
      "3485 [D loss: 0.143888, acc.: 95.31%] [G loss: 2.772995]\n",
      "3486 [D loss: 0.138170, acc.: 93.75%] [G loss: 3.715201]\n",
      "3487 [D loss: 0.181265, acc.: 95.31%] [G loss: 2.818077]\n",
      "3488 [D loss: 0.125206, acc.: 95.31%] [G loss: 3.507318]\n",
      "3489 [D loss: 0.143416, acc.: 95.31%] [G loss: 3.054486]\n",
      "3490 [D loss: 0.140762, acc.: 95.31%] [G loss: 3.362953]\n",
      "3491 [D loss: 0.163160, acc.: 93.75%] [G loss: 2.847389]\n",
      "3492 [D loss: 0.197656, acc.: 93.75%] [G loss: 2.995226]\n",
      "3493 [D loss: 0.198031, acc.: 95.31%] [G loss: 2.739594]\n",
      "3494 [D loss: 0.148918, acc.: 95.31%] [G loss: 3.162004]\n",
      "3495 [D loss: 0.158141, acc.: 95.31%] [G loss: 3.210071]\n",
      "3496 [D loss: 0.173923, acc.: 95.31%] [G loss: 3.013970]\n",
      "3497 [D loss: 0.177089, acc.: 93.75%] [G loss: 2.900401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3498 [D loss: 0.157584, acc.: 93.75%] [G loss: 3.424541]\n",
      "3499 [D loss: 0.146080, acc.: 95.31%] [G loss: 3.130705]\n",
      "3500 [D loss: 0.169726, acc.: 95.31%] [G loss: 3.003349]\n",
      "generated_data\n",
      "3501 [D loss: 0.183187, acc.: 93.75%] [G loss: 3.251176]\n",
      "3502 [D loss: 0.181080, acc.: 92.19%] [G loss: 3.062005]\n",
      "3503 [D loss: 0.177725, acc.: 95.31%] [G loss: 3.299909]\n",
      "3504 [D loss: 0.183062, acc.: 95.31%] [G loss: 2.671970]\n",
      "3505 [D loss: 0.155548, acc.: 95.31%] [G loss: 2.806175]\n",
      "3506 [D loss: 0.145748, acc.: 93.75%] [G loss: 3.414947]\n",
      "3507 [D loss: 0.168248, acc.: 95.31%] [G loss: 2.842736]\n",
      "3508 [D loss: 0.172274, acc.: 95.31%] [G loss: 2.963758]\n",
      "3509 [D loss: 0.196170, acc.: 93.75%] [G loss: 3.010238]\n",
      "3510 [D loss: 0.138221, acc.: 95.31%] [G loss: 3.663898]\n",
      "3511 [D loss: 0.191860, acc.: 93.75%] [G loss: 2.704679]\n",
      "3512 [D loss: 0.138298, acc.: 95.31%] [G loss: 2.935313]\n",
      "3513 [D loss: 0.206023, acc.: 93.75%] [G loss: 2.559902]\n",
      "3514 [D loss: 0.141169, acc.: 95.31%] [G loss: 3.380303]\n",
      "3515 [D loss: 0.176728, acc.: 93.75%] [G loss: 2.892291]\n",
      "3516 [D loss: 0.135799, acc.: 95.31%] [G loss: 3.437000]\n",
      "3517 [D loss: 0.169935, acc.: 93.75%] [G loss: 3.000380]\n",
      "3518 [D loss: 0.192754, acc.: 92.19%] [G loss: 2.922234]\n",
      "3519 [D loss: 0.199012, acc.: 93.75%] [G loss: 2.565998]\n",
      "3520 [D loss: 0.182523, acc.: 93.75%] [G loss: 2.691850]\n",
      "3521 [D loss: 0.171273, acc.: 95.31%] [G loss: 2.907178]\n",
      "3522 [D loss: 0.185554, acc.: 93.75%] [G loss: 3.271303]\n",
      "3523 [D loss: 0.203206, acc.: 93.75%] [G loss: 3.129001]\n",
      "3524 [D loss: 0.172808, acc.: 93.75%] [G loss: 2.954939]\n",
      "3525 [D loss: 0.167137, acc.: 95.31%] [G loss: 3.282847]\n",
      "3526 [D loss: 0.159831, acc.: 95.31%] [G loss: 2.694694]\n",
      "3527 [D loss: 0.146208, acc.: 95.31%] [G loss: 2.954240]\n",
      "3528 [D loss: 0.132941, acc.: 95.31%] [G loss: 2.922273]\n",
      "3529 [D loss: 0.182400, acc.: 95.31%] [G loss: 2.799417]\n",
      "3530 [D loss: 0.171950, acc.: 95.31%] [G loss: 2.806206]\n",
      "3531 [D loss: 0.139752, acc.: 95.31%] [G loss: 3.259332]\n",
      "3532 [D loss: 0.244624, acc.: 93.75%] [G loss: 2.622770]\n",
      "3533 [D loss: 0.192722, acc.: 95.31%] [G loss: 2.706133]\n",
      "3534 [D loss: 0.157564, acc.: 93.75%] [G loss: 3.534376]\n",
      "3535 [D loss: 0.161537, acc.: 93.75%] [G loss: 2.826237]\n",
      "3536 [D loss: 0.168619, acc.: 95.31%] [G loss: 2.705773]\n",
      "3537 [D loss: 0.158422, acc.: 95.31%] [G loss: 3.229043]\n",
      "3538 [D loss: 0.176735, acc.: 93.75%] [G loss: 3.017086]\n",
      "3539 [D loss: 0.152441, acc.: 95.31%] [G loss: 3.170445]\n",
      "3540 [D loss: 0.158017, acc.: 95.31%] [G loss: 3.013394]\n",
      "3541 [D loss: 0.193106, acc.: 92.19%] [G loss: 3.212529]\n",
      "3542 [D loss: 0.181322, acc.: 95.31%] [G loss: 3.105840]\n",
      "3543 [D loss: 0.178158, acc.: 93.75%] [G loss: 2.933296]\n",
      "3544 [D loss: 0.170679, acc.: 95.31%] [G loss: 2.863583]\n",
      "3545 [D loss: 0.170328, acc.: 95.31%] [G loss: 2.651991]\n",
      "3546 [D loss: 0.156007, acc.: 95.31%] [G loss: 2.996467]\n",
      "3547 [D loss: 0.156589, acc.: 95.31%] [G loss: 2.972017]\n",
      "3548 [D loss: 0.163049, acc.: 95.31%] [G loss: 2.909877]\n",
      "3549 [D loss: 0.149749, acc.: 95.31%] [G loss: 3.088121]\n",
      "3550 [D loss: 0.147157, acc.: 95.31%] [G loss: 2.798156]\n",
      "3551 [D loss: 0.154172, acc.: 95.31%] [G loss: 2.978877]\n",
      "3552 [D loss: 0.152669, acc.: 95.31%] [G loss: 2.929841]\n",
      "3553 [D loss: 0.158692, acc.: 95.31%] [G loss: 2.883530]\n",
      "3554 [D loss: 0.162927, acc.: 95.31%] [G loss: 3.034844]\n",
      "3555 [D loss: 0.204748, acc.: 93.75%] [G loss: 3.294852]\n",
      "3556 [D loss: 0.206575, acc.: 93.75%] [G loss: 3.013045]\n",
      "3557 [D loss: 0.185088, acc.: 93.75%] [G loss: 3.376069]\n",
      "3558 [D loss: 0.161194, acc.: 95.31%] [G loss: 2.935157]\n",
      "3559 [D loss: 0.161344, acc.: 95.31%] [G loss: 3.095472]\n",
      "3560 [D loss: 0.158980, acc.: 95.31%] [G loss: 3.202521]\n",
      "3561 [D loss: 0.140249, acc.: 95.31%] [G loss: 3.003720]\n",
      "3562 [D loss: 0.152926, acc.: 95.31%] [G loss: 2.809060]\n",
      "3563 [D loss: 0.164564, acc.: 93.75%] [G loss: 3.186203]\n",
      "3564 [D loss: 0.171360, acc.: 93.75%] [G loss: 2.584454]\n",
      "3565 [D loss: 0.122659, acc.: 95.31%] [G loss: 3.768719]\n",
      "3566 [D loss: 0.245503, acc.: 89.06%] [G loss: 3.345303]\n",
      "3567 [D loss: 0.211758, acc.: 93.75%] [G loss: 3.255422]\n",
      "3568 [D loss: 0.220232, acc.: 92.19%] [G loss: 3.266325]\n",
      "3569 [D loss: 0.163809, acc.: 95.31%] [G loss: 3.000714]\n",
      "3570 [D loss: 0.182356, acc.: 93.75%] [G loss: 2.532418]\n",
      "3571 [D loss: 0.151381, acc.: 95.31%] [G loss: 3.203898]\n",
      "3572 [D loss: 0.151391, acc.: 95.31%] [G loss: 3.058609]\n",
      "3573 [D loss: 0.172635, acc.: 93.75%] [G loss: 2.870439]\n",
      "3574 [D loss: 0.211033, acc.: 92.19%] [G loss: 3.346748]\n",
      "3575 [D loss: 0.178755, acc.: 92.19%] [G loss: 3.020670]\n",
      "3576 [D loss: 0.191524, acc.: 90.62%] [G loss: 3.124022]\n",
      "3577 [D loss: 0.217947, acc.: 92.19%] [G loss: 3.180391]\n",
      "3578 [D loss: 0.226406, acc.: 92.19%] [G loss: 2.944604]\n",
      "3579 [D loss: 0.222415, acc.: 92.19%] [G loss: 2.698409]\n",
      "3580 [D loss: 0.207933, acc.: 93.75%] [G loss: 2.884759]\n",
      "3581 [D loss: 0.177929, acc.: 95.31%] [G loss: 3.309716]\n",
      "3582 [D loss: 0.176644, acc.: 93.75%] [G loss: 2.944933]\n",
      "3583 [D loss: 0.154292, acc.: 95.31%] [G loss: 2.728595]\n",
      "3584 [D loss: 0.217493, acc.: 93.75%] [G loss: 3.207696]\n",
      "3585 [D loss: 0.164273, acc.: 93.75%] [G loss: 3.262977]\n",
      "3586 [D loss: 0.153136, acc.: 95.31%] [G loss: 3.066718]\n",
      "3587 [D loss: 0.269784, acc.: 92.19%] [G loss: 3.083747]\n",
      "3588 [D loss: 0.184490, acc.: 93.75%] [G loss: 2.989694]\n",
      "3589 [D loss: 0.227124, acc.: 92.19%] [G loss: 2.593377]\n",
      "3590 [D loss: 0.161256, acc.: 95.31%] [G loss: 2.574546]\n",
      "3591 [D loss: 0.161599, acc.: 95.31%] [G loss: 2.720773]\n",
      "3592 [D loss: 0.187838, acc.: 93.75%] [G loss: 2.891838]\n",
      "3593 [D loss: 0.166085, acc.: 95.31%] [G loss: 2.840310]\n",
      "3594 [D loss: 0.176543, acc.: 93.75%] [G loss: 2.613992]\n",
      "3595 [D loss: 0.199987, acc.: 93.75%] [G loss: 2.702755]\n",
      "3596 [D loss: 0.208341, acc.: 93.75%] [G loss: 2.939141]\n",
      "3597 [D loss: 0.158632, acc.: 95.31%] [G loss: 3.319936]\n",
      "3598 [D loss: 0.194579, acc.: 93.75%] [G loss: 2.574065]\n",
      "3599 [D loss: 0.148108, acc.: 95.31%] [G loss: 3.097304]\n",
      "3600 [D loss: 0.167883, acc.: 95.31%] [G loss: 3.053585]\n",
      "generated_data\n",
      "3601 [D loss: 0.245125, acc.: 92.19%] [G loss: 3.352858]\n",
      "3602 [D loss: 0.154451, acc.: 93.75%] [G loss: 3.066005]\n",
      "3603 [D loss: 0.180272, acc.: 93.75%] [G loss: 3.001561]\n",
      "3604 [D loss: 0.186284, acc.: 93.75%] [G loss: 2.711054]\n",
      "3605 [D loss: 0.163722, acc.: 93.75%] [G loss: 3.218574]\n",
      "3606 [D loss: 0.153643, acc.: 95.31%] [G loss: 3.496724]\n",
      "3607 [D loss: 0.158375, acc.: 95.31%] [G loss: 2.498244]\n",
      "3608 [D loss: 0.125313, acc.: 95.31%] [G loss: 3.457380]\n",
      "3609 [D loss: 0.158338, acc.: 95.31%] [G loss: 2.693007]\n",
      "3610 [D loss: 0.175614, acc.: 93.75%] [G loss: 3.179473]\n",
      "3611 [D loss: 0.166143, acc.: 95.31%] [G loss: 2.779883]\n",
      "3612 [D loss: 0.146566, acc.: 93.75%] [G loss: 3.534777]\n",
      "3613 [D loss: 0.166808, acc.: 93.75%] [G loss: 2.703916]\n",
      "3614 [D loss: 0.157449, acc.: 95.31%] [G loss: 2.816174]\n",
      "3615 [D loss: 0.170842, acc.: 95.31%] [G loss: 2.994864]\n",
      "3616 [D loss: 0.156180, acc.: 95.31%] [G loss: 3.472604]\n",
      "3617 [D loss: 0.181400, acc.: 93.75%] [G loss: 3.241877]\n",
      "3618 [D loss: 0.171682, acc.: 95.31%] [G loss: 2.940449]\n",
      "3619 [D loss: 0.155586, acc.: 95.31%] [G loss: 2.761127]\n",
      "3620 [D loss: 0.146989, acc.: 95.31%] [G loss: 3.184126]\n",
      "3621 [D loss: 0.137793, acc.: 95.31%] [G loss: 3.311105]\n",
      "3622 [D loss: 0.159336, acc.: 95.31%] [G loss: 2.942196]\n",
      "3623 [D loss: 0.132972, acc.: 95.31%] [G loss: 3.247910]\n",
      "3624 [D loss: 0.155349, acc.: 95.31%] [G loss: 3.061693]\n",
      "3625 [D loss: 0.188966, acc.: 93.75%] [G loss: 3.120311]\n",
      "3626 [D loss: 0.144760, acc.: 95.31%] [G loss: 2.899104]\n",
      "3627 [D loss: 0.179471, acc.: 95.31%] [G loss: 3.065424]\n",
      "3628 [D loss: 0.180331, acc.: 93.75%] [G loss: 2.732753]\n",
      "3629 [D loss: 0.157469, acc.: 95.31%] [G loss: 3.122233]\n",
      "3630 [D loss: 0.159775, acc.: 95.31%] [G loss: 3.125292]\n",
      "3631 [D loss: 0.163307, acc.: 95.31%] [G loss: 2.971672]\n",
      "3632 [D loss: 0.201739, acc.: 93.75%] [G loss: 2.939930]\n",
      "3633 [D loss: 0.165335, acc.: 95.31%] [G loss: 3.018956]\n",
      "3634 [D loss: 0.148148, acc.: 95.31%] [G loss: 3.237615]\n",
      "3635 [D loss: 0.171219, acc.: 95.31%] [G loss: 3.208635]\n",
      "3636 [D loss: 0.167443, acc.: 93.75%] [G loss: 2.905243]\n",
      "3637 [D loss: 0.177617, acc.: 95.31%] [G loss: 2.771154]\n",
      "3638 [D loss: 0.186747, acc.: 93.75%] [G loss: 2.949437]\n",
      "3639 [D loss: 0.191634, acc.: 95.31%] [G loss: 3.165016]\n",
      "3640 [D loss: 0.174937, acc.: 93.75%] [G loss: 3.072263]\n",
      "3641 [D loss: 0.158523, acc.: 93.75%] [G loss: 3.125452]\n",
      "3642 [D loss: 0.180183, acc.: 95.31%] [G loss: 2.651118]\n",
      "3643 [D loss: 0.180844, acc.: 93.75%] [G loss: 2.994331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3644 [D loss: 0.149240, acc.: 95.31%] [G loss: 2.774065]\n",
      "3645 [D loss: 0.179788, acc.: 93.75%] [G loss: 2.730072]\n",
      "3646 [D loss: 0.183839, acc.: 90.62%] [G loss: 2.866562]\n",
      "3647 [D loss: 0.221601, acc.: 93.75%] [G loss: 3.024890]\n",
      "3648 [D loss: 0.213599, acc.: 92.19%] [G loss: 2.740368]\n",
      "3649 [D loss: 0.170958, acc.: 92.19%] [G loss: 2.890026]\n",
      "3650 [D loss: 0.268485, acc.: 90.62%] [G loss: 2.981146]\n",
      "3651 [D loss: 0.192833, acc.: 93.75%] [G loss: 3.125616]\n",
      "3652 [D loss: 0.194959, acc.: 93.75%] [G loss: 2.858369]\n",
      "3653 [D loss: 0.187573, acc.: 95.31%] [G loss: 2.787950]\n",
      "3654 [D loss: 0.188222, acc.: 93.75%] [G loss: 3.114941]\n",
      "3655 [D loss: 0.242548, acc.: 90.62%] [G loss: 3.104506]\n",
      "3656 [D loss: 0.195704, acc.: 93.75%] [G loss: 2.723173]\n",
      "3657 [D loss: 0.176973, acc.: 95.31%] [G loss: 2.877906]\n",
      "3658 [D loss: 0.168238, acc.: 93.75%] [G loss: 2.843286]\n",
      "3659 [D loss: 0.160000, acc.: 95.31%] [G loss: 2.881440]\n",
      "3660 [D loss: 0.187248, acc.: 93.75%] [G loss: 2.679211]\n",
      "3661 [D loss: 0.170734, acc.: 95.31%] [G loss: 2.810589]\n",
      "3662 [D loss: 0.171382, acc.: 95.31%] [G loss: 3.220750]\n",
      "3663 [D loss: 0.169718, acc.: 95.31%] [G loss: 2.971428]\n",
      "3664 [D loss: 0.147009, acc.: 95.31%] [G loss: 3.117196]\n",
      "3665 [D loss: 0.156145, acc.: 95.31%] [G loss: 3.067970]\n",
      "3666 [D loss: 0.165606, acc.: 95.31%] [G loss: 2.992156]\n",
      "3667 [D loss: 0.178506, acc.: 93.75%] [G loss: 2.951563]\n",
      "3668 [D loss: 0.166906, acc.: 95.31%] [G loss: 2.998236]\n",
      "3669 [D loss: 0.186570, acc.: 93.75%] [G loss: 2.903062]\n",
      "3670 [D loss: 0.159141, acc.: 95.31%] [G loss: 2.652357]\n",
      "3671 [D loss: 0.164795, acc.: 95.31%] [G loss: 2.923036]\n",
      "3672 [D loss: 0.140728, acc.: 95.31%] [G loss: 3.370026]\n",
      "3673 [D loss: 0.172798, acc.: 93.75%] [G loss: 2.827036]\n",
      "3674 [D loss: 0.148778, acc.: 95.31%] [G loss: 2.581880]\n",
      "3675 [D loss: 0.150337, acc.: 95.31%] [G loss: 3.051360]\n",
      "3676 [D loss: 0.169700, acc.: 95.31%] [G loss: 2.994674]\n",
      "3677 [D loss: 0.157005, acc.: 95.31%] [G loss: 2.836014]\n",
      "3678 [D loss: 0.169051, acc.: 93.75%] [G loss: 3.025443]\n",
      "3679 [D loss: 0.186460, acc.: 93.75%] [G loss: 2.836877]\n",
      "3680 [D loss: 0.200771, acc.: 92.19%] [G loss: 2.762211]\n",
      "3681 [D loss: 0.183186, acc.: 95.31%] [G loss: 2.758673]\n",
      "3682 [D loss: 0.174354, acc.: 93.75%] [G loss: 3.294174]\n",
      "3683 [D loss: 0.154431, acc.: 95.31%] [G loss: 3.070100]\n",
      "3684 [D loss: 0.164277, acc.: 95.31%] [G loss: 3.029951]\n",
      "3685 [D loss: 0.167282, acc.: 93.75%] [G loss: 3.167829]\n",
      "3686 [D loss: 0.135260, acc.: 95.31%] [G loss: 3.055703]\n",
      "3687 [D loss: 0.170139, acc.: 95.31%] [G loss: 3.025028]\n",
      "3688 [D loss: 0.141677, acc.: 95.31%] [G loss: 3.239314]\n",
      "3689 [D loss: 0.140113, acc.: 95.31%] [G loss: 3.057385]\n",
      "3690 [D loss: 0.224483, acc.: 92.19%] [G loss: 3.255061]\n",
      "3691 [D loss: 0.172620, acc.: 95.31%] [G loss: 3.212770]\n",
      "3692 [D loss: 0.196501, acc.: 92.19%] [G loss: 2.895806]\n",
      "3693 [D loss: 0.159193, acc.: 95.31%] [G loss: 3.049339]\n",
      "3694 [D loss: 0.131287, acc.: 95.31%] [G loss: 3.299073]\n",
      "3695 [D loss: 0.158610, acc.: 95.31%] [G loss: 2.692958]\n",
      "3696 [D loss: 0.146736, acc.: 95.31%] [G loss: 2.278242]\n",
      "3697 [D loss: 0.175710, acc.: 95.31%] [G loss: 3.239569]\n",
      "3698 [D loss: 0.169555, acc.: 93.75%] [G loss: 3.198125]\n",
      "3699 [D loss: 0.149933, acc.: 95.31%] [G loss: 3.153889]\n",
      "3700 [D loss: 0.144706, acc.: 95.31%] [G loss: 3.235405]\n",
      "generated_data\n",
      "3701 [D loss: 0.187091, acc.: 95.31%] [G loss: 3.113430]\n",
      "3702 [D loss: 0.137998, acc.: 95.31%] [G loss: 2.943159]\n",
      "3703 [D loss: 0.167034, acc.: 95.31%] [G loss: 2.823307]\n",
      "3704 [D loss: 0.186470, acc.: 93.75%] [G loss: 2.776599]\n",
      "3705 [D loss: 0.191480, acc.: 93.75%] [G loss: 2.881797]\n",
      "3706 [D loss: 0.170588, acc.: 95.31%] [G loss: 2.855239]\n",
      "3707 [D loss: 0.200296, acc.: 92.19%] [G loss: 2.704411]\n",
      "3708 [D loss: 0.204810, acc.: 92.19%] [G loss: 3.137660]\n",
      "3709 [D loss: 0.194215, acc.: 95.31%] [G loss: 2.790997]\n",
      "3710 [D loss: 0.173176, acc.: 93.75%] [G loss: 2.741577]\n",
      "3711 [D loss: 0.147349, acc.: 95.31%] [G loss: 3.199537]\n",
      "3712 [D loss: 0.161531, acc.: 93.75%] [G loss: 3.310047]\n",
      "3713 [D loss: 0.168823, acc.: 95.31%] [G loss: 3.062732]\n",
      "3714 [D loss: 0.181847, acc.: 95.31%] [G loss: 2.797464]\n",
      "3715 [D loss: 0.157187, acc.: 95.31%] [G loss: 2.840141]\n",
      "3716 [D loss: 0.149313, acc.: 95.31%] [G loss: 3.057013]\n",
      "3717 [D loss: 0.166851, acc.: 95.31%] [G loss: 2.977954]\n",
      "3718 [D loss: 0.167363, acc.: 93.75%] [G loss: 3.468985]\n",
      "3719 [D loss: 0.150189, acc.: 95.31%] [G loss: 3.631343]\n",
      "3720 [D loss: 0.157436, acc.: 95.31%] [G loss: 2.855979]\n",
      "3721 [D loss: 0.197319, acc.: 93.75%] [G loss: 2.561352]\n",
      "3722 [D loss: 0.155797, acc.: 95.31%] [G loss: 3.300083]\n",
      "3723 [D loss: 0.169487, acc.: 95.31%] [G loss: 3.433223]\n",
      "3724 [D loss: 0.163256, acc.: 93.75%] [G loss: 3.214014]\n",
      "3725 [D loss: 0.165158, acc.: 95.31%] [G loss: 2.996222]\n",
      "3726 [D loss: 0.119965, acc.: 95.31%] [G loss: 3.670111]\n",
      "3727 [D loss: 0.146150, acc.: 95.31%] [G loss: 2.503019]\n",
      "3728 [D loss: 0.146396, acc.: 93.75%] [G loss: 3.280020]\n",
      "3729 [D loss: 0.194862, acc.: 93.75%] [G loss: 3.366492]\n",
      "3730 [D loss: 0.168889, acc.: 93.75%] [G loss: 3.343705]\n",
      "3731 [D loss: 0.173760, acc.: 95.31%] [G loss: 2.711581]\n",
      "3732 [D loss: 0.161792, acc.: 95.31%] [G loss: 2.709773]\n",
      "3733 [D loss: 0.197115, acc.: 93.75%] [G loss: 2.755608]\n",
      "3734 [D loss: 0.145373, acc.: 95.31%] [G loss: 3.141550]\n",
      "3735 [D loss: 0.170502, acc.: 95.31%] [G loss: 3.153981]\n",
      "3736 [D loss: 0.217243, acc.: 92.19%] [G loss: 3.300281]\n",
      "3737 [D loss: 0.185071, acc.: 93.75%] [G loss: 3.049555]\n",
      "3738 [D loss: 0.153671, acc.: 93.75%] [G loss: 3.734622]\n",
      "3739 [D loss: 0.166006, acc.: 95.31%] [G loss: 2.632272]\n",
      "3740 [D loss: 0.133764, acc.: 95.31%] [G loss: 3.191744]\n",
      "3741 [D loss: 0.211387, acc.: 93.75%] [G loss: 3.261448]\n",
      "3742 [D loss: 0.180828, acc.: 95.31%] [G loss: 2.559201]\n",
      "3743 [D loss: 0.156247, acc.: 95.31%] [G loss: 2.630385]\n",
      "3744 [D loss: 0.153129, acc.: 95.31%] [G loss: 2.805726]\n",
      "3745 [D loss: 0.152409, acc.: 95.31%] [G loss: 3.369154]\n",
      "3746 [D loss: 0.182011, acc.: 95.31%] [G loss: 3.473121]\n",
      "3747 [D loss: 0.178002, acc.: 95.31%] [G loss: 2.696268]\n",
      "3748 [D loss: 0.165962, acc.: 95.31%] [G loss: 3.313573]\n",
      "3749 [D loss: 0.191406, acc.: 93.75%] [G loss: 3.489151]\n",
      "3750 [D loss: 0.158070, acc.: 95.31%] [G loss: 2.979961]\n",
      "3751 [D loss: 0.173697, acc.: 92.19%] [G loss: 2.355530]\n",
      "3752 [D loss: 0.159579, acc.: 95.31%] [G loss: 2.969131]\n",
      "3753 [D loss: 0.162394, acc.: 93.75%] [G loss: 2.956625]\n",
      "3754 [D loss: 0.177620, acc.: 93.75%] [G loss: 3.311745]\n",
      "3755 [D loss: 0.177039, acc.: 95.31%] [G loss: 2.835716]\n",
      "3756 [D loss: 0.211966, acc.: 92.19%] [G loss: 2.907981]\n",
      "3757 [D loss: 0.183515, acc.: 95.31%] [G loss: 2.888110]\n",
      "3758 [D loss: 0.179009, acc.: 95.31%] [G loss: 2.947787]\n",
      "3759 [D loss: 0.166677, acc.: 95.31%] [G loss: 2.988046]\n",
      "3760 [D loss: 0.144941, acc.: 95.31%] [G loss: 3.327249]\n",
      "3761 [D loss: 0.152486, acc.: 95.31%] [G loss: 2.697185]\n",
      "3762 [D loss: 0.161921, acc.: 95.31%] [G loss: 2.755083]\n",
      "3763 [D loss: 0.190566, acc.: 93.75%] [G loss: 2.759726]\n",
      "3764 [D loss: 0.209293, acc.: 92.19%] [G loss: 3.324936]\n",
      "3765 [D loss: 0.213930, acc.: 92.19%] [G loss: 3.194757]\n",
      "3766 [D loss: 0.219438, acc.: 92.19%] [G loss: 3.021280]\n",
      "3767 [D loss: 0.174423, acc.: 92.19%] [G loss: 3.471080]\n",
      "3768 [D loss: 0.163329, acc.: 95.31%] [G loss: 3.087896]\n",
      "3769 [D loss: 0.161896, acc.: 95.31%] [G loss: 3.099498]\n",
      "3770 [D loss: 0.207509, acc.: 93.75%] [G loss: 3.134310]\n",
      "3771 [D loss: 0.167539, acc.: 93.75%] [G loss: 3.357434]\n",
      "3772 [D loss: 0.204817, acc.: 92.19%] [G loss: 2.984986]\n",
      "3773 [D loss: 0.187209, acc.: 93.75%] [G loss: 3.078385]\n",
      "3774 [D loss: 0.179548, acc.: 95.31%] [G loss: 2.844609]\n",
      "3775 [D loss: 0.144345, acc.: 95.31%] [G loss: 3.178917]\n",
      "3776 [D loss: 0.158592, acc.: 95.31%] [G loss: 3.224505]\n",
      "3777 [D loss: 0.148945, acc.: 95.31%] [G loss: 2.801243]\n",
      "3778 [D loss: 0.149571, acc.: 95.31%] [G loss: 3.149409]\n",
      "3779 [D loss: 0.139731, acc.: 95.31%] [G loss: 3.004698]\n",
      "3780 [D loss: 0.161600, acc.: 95.31%] [G loss: 2.889117]\n",
      "3781 [D loss: 0.197522, acc.: 93.75%] [G loss: 3.061848]\n",
      "3782 [D loss: 0.190582, acc.: 93.75%] [G loss: 2.753987]\n",
      "3783 [D loss: 0.208204, acc.: 90.62%] [G loss: 3.128445]\n",
      "3784 [D loss: 0.167164, acc.: 93.75%] [G loss: 2.988270]\n",
      "3785 [D loss: 0.162764, acc.: 95.31%] [G loss: 2.713517]\n",
      "3786 [D loss: 0.164249, acc.: 95.31%] [G loss: 2.954104]\n",
      "3787 [D loss: 0.161107, acc.: 95.31%] [G loss: 2.909275]\n",
      "3788 [D loss: 0.176309, acc.: 95.31%] [G loss: 2.939035]\n",
      "3789 [D loss: 0.158575, acc.: 95.31%] [G loss: 2.911057]\n",
      "3790 [D loss: 0.151348, acc.: 95.31%] [G loss: 3.047484]\n",
      "3791 [D loss: 0.179053, acc.: 95.31%] [G loss: 3.262772]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3792 [D loss: 0.175564, acc.: 93.75%] [G loss: 3.149957]\n",
      "3793 [D loss: 0.167469, acc.: 95.31%] [G loss: 3.031628]\n",
      "3794 [D loss: 0.162564, acc.: 93.75%] [G loss: 3.001873]\n",
      "3795 [D loss: 0.143592, acc.: 95.31%] [G loss: 3.488679]\n",
      "3796 [D loss: 0.174526, acc.: 93.75%] [G loss: 2.950318]\n",
      "3797 [D loss: 0.224668, acc.: 92.19%] [G loss: 3.115939]\n",
      "3798 [D loss: 0.199877, acc.: 93.75%] [G loss: 2.975807]\n",
      "3799 [D loss: 0.211688, acc.: 93.75%] [G loss: 2.904016]\n",
      "3800 [D loss: 0.176887, acc.: 93.75%] [G loss: 2.975534]\n",
      "generated_data\n",
      "3801 [D loss: 0.181808, acc.: 93.75%] [G loss: 3.047889]\n",
      "3802 [D loss: 0.197138, acc.: 92.19%] [G loss: 3.094790]\n",
      "3803 [D loss: 0.208787, acc.: 92.19%] [G loss: 3.218105]\n",
      "3804 [D loss: 0.219238, acc.: 93.75%] [G loss: 2.724353]\n",
      "3805 [D loss: 0.191515, acc.: 93.75%] [G loss: 2.822476]\n",
      "3806 [D loss: 0.170315, acc.: 95.31%] [G loss: 2.785054]\n",
      "3807 [D loss: 0.151423, acc.: 95.31%] [G loss: 3.101121]\n",
      "3808 [D loss: 0.163702, acc.: 95.31%] [G loss: 2.735932]\n",
      "3809 [D loss: 0.211673, acc.: 92.19%] [G loss: 2.882162]\n",
      "3810 [D loss: 0.150427, acc.: 95.31%] [G loss: 3.472454]\n",
      "3811 [D loss: 0.151934, acc.: 93.75%] [G loss: 2.799423]\n",
      "3812 [D loss: 0.182690, acc.: 93.75%] [G loss: 2.980463]\n",
      "3813 [D loss: 0.179807, acc.: 95.31%] [G loss: 2.679730]\n",
      "3814 [D loss: 0.165567, acc.: 95.31%] [G loss: 3.106976]\n",
      "3815 [D loss: 0.143762, acc.: 95.31%] [G loss: 2.528473]\n",
      "3816 [D loss: 0.145575, acc.: 95.31%] [G loss: 2.822440]\n",
      "3817 [D loss: 0.154252, acc.: 95.31%] [G loss: 2.760576]\n",
      "3818 [D loss: 0.150592, acc.: 95.31%] [G loss: 3.045146]\n",
      "3819 [D loss: 0.186036, acc.: 93.75%] [G loss: 2.859668]\n",
      "3820 [D loss: 0.191420, acc.: 93.75%] [G loss: 3.162524]\n",
      "3821 [D loss: 0.162895, acc.: 95.31%] [G loss: 2.959047]\n",
      "3822 [D loss: 0.200867, acc.: 93.75%] [G loss: 2.758009]\n",
      "3823 [D loss: 0.236453, acc.: 90.62%] [G loss: 3.189867]\n",
      "3824 [D loss: 0.203582, acc.: 92.19%] [G loss: 2.935402]\n",
      "3825 [D loss: 0.177218, acc.: 95.31%] [G loss: 2.438764]\n",
      "3826 [D loss: 0.202516, acc.: 93.75%] [G loss: 3.010197]\n",
      "3827 [D loss: 0.164214, acc.: 95.31%] [G loss: 3.132566]\n",
      "3828 [D loss: 0.173543, acc.: 92.19%] [G loss: 3.048391]\n",
      "3829 [D loss: 0.145908, acc.: 95.31%] [G loss: 3.013357]\n",
      "3830 [D loss: 0.141788, acc.: 95.31%] [G loss: 2.948381]\n",
      "3831 [D loss: 0.165535, acc.: 95.31%] [G loss: 2.845796]\n",
      "3832 [D loss: 0.142806, acc.: 95.31%] [G loss: 2.847893]\n",
      "3833 [D loss: 0.170872, acc.: 95.31%] [G loss: 2.671701]\n",
      "3834 [D loss: 0.207560, acc.: 92.19%] [G loss: 3.217124]\n",
      "3835 [D loss: 0.207268, acc.: 93.75%] [G loss: 2.936090]\n",
      "3836 [D loss: 0.166176, acc.: 95.31%] [G loss: 3.026952]\n",
      "3837 [D loss: 0.164421, acc.: 93.75%] [G loss: 3.421509]\n",
      "3838 [D loss: 0.176458, acc.: 95.31%] [G loss: 2.922504]\n",
      "3839 [D loss: 0.179105, acc.: 93.75%] [G loss: 3.158670]\n",
      "3840 [D loss: 0.173333, acc.: 95.31%] [G loss: 2.532539]\n",
      "3841 [D loss: 0.170386, acc.: 95.31%] [G loss: 2.917269]\n",
      "3842 [D loss: 0.157615, acc.: 95.31%] [G loss: 3.069401]\n",
      "3843 [D loss: 0.155133, acc.: 93.75%] [G loss: 3.469480]\n",
      "3844 [D loss: 0.159890, acc.: 95.31%] [G loss: 2.759032]\n",
      "3845 [D loss: 0.204441, acc.: 92.19%] [G loss: 3.061573]\n",
      "3846 [D loss: 0.163410, acc.: 93.75%] [G loss: 3.342647]\n",
      "3847 [D loss: 0.167041, acc.: 93.75%] [G loss: 2.774890]\n",
      "3848 [D loss: 0.165309, acc.: 95.31%] [G loss: 3.588139]\n",
      "3849 [D loss: 0.168632, acc.: 95.31%] [G loss: 2.833565]\n",
      "3850 [D loss: 0.208553, acc.: 93.75%] [G loss: 3.119252]\n",
      "3851 [D loss: 0.165593, acc.: 95.31%] [G loss: 3.031334]\n",
      "3852 [D loss: 0.185626, acc.: 92.19%] [G loss: 3.287869]\n",
      "3853 [D loss: 0.166900, acc.: 95.31%] [G loss: 3.245258]\n",
      "3854 [D loss: 0.170716, acc.: 95.31%] [G loss: 3.388632]\n",
      "3855 [D loss: 0.164275, acc.: 93.75%] [G loss: 3.273534]\n",
      "3856 [D loss: 0.168238, acc.: 95.31%] [G loss: 2.661170]\n",
      "3857 [D loss: 0.161083, acc.: 95.31%] [G loss: 2.737936]\n",
      "3858 [D loss: 0.152342, acc.: 95.31%] [G loss: 3.190222]\n",
      "3859 [D loss: 0.170678, acc.: 93.75%] [G loss: 3.349360]\n",
      "3860 [D loss: 0.175861, acc.: 95.31%] [G loss: 2.918277]\n",
      "3861 [D loss: 0.160100, acc.: 95.31%] [G loss: 2.514186]\n",
      "3862 [D loss: 0.158052, acc.: 95.31%] [G loss: 3.395757]\n",
      "3863 [D loss: 0.167317, acc.: 93.75%] [G loss: 3.151999]\n",
      "3864 [D loss: 0.168785, acc.: 92.19%] [G loss: 3.084212]\n",
      "3865 [D loss: 0.196847, acc.: 93.75%] [G loss: 2.875453]\n",
      "3866 [D loss: 0.186300, acc.: 93.75%] [G loss: 2.713849]\n",
      "3867 [D loss: 0.154064, acc.: 93.75%] [G loss: 3.504198]\n",
      "3868 [D loss: 0.168835, acc.: 93.75%] [G loss: 2.852930]\n",
      "3869 [D loss: 0.168598, acc.: 95.31%] [G loss: 2.853280]\n",
      "3870 [D loss: 0.170593, acc.: 93.75%] [G loss: 2.988710]\n",
      "3871 [D loss: 0.163104, acc.: 93.75%] [G loss: 3.166080]\n",
      "3872 [D loss: 0.155364, acc.: 95.31%] [G loss: 3.317922]\n",
      "3873 [D loss: 0.159974, acc.: 95.31%] [G loss: 3.082999]\n",
      "3874 [D loss: 0.199864, acc.: 92.19%] [G loss: 3.011378]\n",
      "3875 [D loss: 0.179035, acc.: 93.75%] [G loss: 3.017431]\n",
      "3876 [D loss: 0.177399, acc.: 95.31%] [G loss: 3.012190]\n",
      "3877 [D loss: 0.156501, acc.: 95.31%] [G loss: 2.808727]\n",
      "3878 [D loss: 0.187246, acc.: 93.75%] [G loss: 2.769381]\n",
      "3879 [D loss: 0.156023, acc.: 95.31%] [G loss: 2.971553]\n",
      "3880 [D loss: 0.159913, acc.: 95.31%] [G loss: 2.997609]\n",
      "3881 [D loss: 0.132652, acc.: 95.31%] [G loss: 3.196069]\n",
      "3882 [D loss: 0.220348, acc.: 92.19%] [G loss: 2.947194]\n",
      "3883 [D loss: 0.151099, acc.: 95.31%] [G loss: 3.122616]\n",
      "3884 [D loss: 0.177937, acc.: 95.31%] [G loss: 2.945406]\n",
      "3885 [D loss: 0.176944, acc.: 95.31%] [G loss: 3.150921]\n",
      "3886 [D loss: 0.152869, acc.: 95.31%] [G loss: 3.165754]\n",
      "3887 [D loss: 0.178873, acc.: 93.75%] [G loss: 2.415687]\n",
      "3888 [D loss: 0.200088, acc.: 92.19%] [G loss: 2.946961]\n",
      "3889 [D loss: 0.165143, acc.: 95.31%] [G loss: 2.941648]\n",
      "3890 [D loss: 0.207461, acc.: 92.19%] [G loss: 3.122801]\n",
      "3891 [D loss: 0.177123, acc.: 95.31%] [G loss: 3.224145]\n",
      "3892 [D loss: 0.179559, acc.: 95.31%] [G loss: 2.736452]\n",
      "3893 [D loss: 0.230540, acc.: 92.19%] [G loss: 3.136796]\n",
      "3894 [D loss: 0.156802, acc.: 95.31%] [G loss: 3.588062]\n",
      "3895 [D loss: 0.185899, acc.: 95.31%] [G loss: 2.567779]\n",
      "3896 [D loss: 0.198193, acc.: 90.62%] [G loss: 3.171853]\n",
      "3897 [D loss: 0.174568, acc.: 93.75%] [G loss: 3.381830]\n",
      "3898 [D loss: 0.170618, acc.: 95.31%] [G loss: 2.936377]\n",
      "3899 [D loss: 0.153944, acc.: 95.31%] [G loss: 3.155463]\n",
      "3900 [D loss: 0.157789, acc.: 95.31%] [G loss: 2.990179]\n",
      "generated_data\n",
      "3901 [D loss: 0.170806, acc.: 93.75%] [G loss: 3.114470]\n",
      "3902 [D loss: 0.137704, acc.: 95.31%] [G loss: 3.549322]\n",
      "3903 [D loss: 0.179002, acc.: 95.31%] [G loss: 2.846586]\n",
      "3904 [D loss: 0.139878, acc.: 95.31%] [G loss: 3.503901]\n",
      "3905 [D loss: 0.175884, acc.: 93.75%] [G loss: 3.336630]\n",
      "3906 [D loss: 0.192737, acc.: 95.31%] [G loss: 2.692071]\n",
      "3907 [D loss: 0.142879, acc.: 95.31%] [G loss: 2.787147]\n",
      "3908 [D loss: 0.137123, acc.: 95.31%] [G loss: 3.509237]\n",
      "3909 [D loss: 0.161080, acc.: 95.31%] [G loss: 2.849727]\n",
      "3910 [D loss: 0.164742, acc.: 95.31%] [G loss: 3.409026]\n",
      "3911 [D loss: 0.176499, acc.: 93.75%] [G loss: 3.647550]\n",
      "3912 [D loss: 0.162158, acc.: 95.31%] [G loss: 2.840034]\n",
      "3913 [D loss: 0.226541, acc.: 92.19%] [G loss: 3.470753]\n",
      "3914 [D loss: 0.165389, acc.: 93.75%] [G loss: 3.582345]\n",
      "3915 [D loss: 0.183917, acc.: 93.75%] [G loss: 2.782186]\n",
      "3916 [D loss: 0.126065, acc.: 95.31%] [G loss: 3.472830]\n",
      "3917 [D loss: 0.173972, acc.: 95.31%] [G loss: 2.862295]\n",
      "3918 [D loss: 0.143754, acc.: 95.31%] [G loss: 2.818172]\n",
      "3919 [D loss: 0.201320, acc.: 93.75%] [G loss: 2.901323]\n",
      "3920 [D loss: 0.137515, acc.: 95.31%] [G loss: 3.202635]\n",
      "3921 [D loss: 0.153907, acc.: 95.31%] [G loss: 3.112455]\n",
      "3922 [D loss: 0.176281, acc.: 93.75%] [G loss: 3.085558]\n",
      "3923 [D loss: 0.193081, acc.: 92.19%] [G loss: 2.979373]\n",
      "3924 [D loss: 0.170770, acc.: 93.75%] [G loss: 3.025786]\n",
      "3925 [D loss: 0.213055, acc.: 93.75%] [G loss: 3.089615]\n",
      "3926 [D loss: 0.175857, acc.: 93.75%] [G loss: 2.867598]\n",
      "3927 [D loss: 0.149026, acc.: 95.31%] [G loss: 2.859601]\n",
      "3928 [D loss: 0.133707, acc.: 95.31%] [G loss: 3.040385]\n",
      "3929 [D loss: 0.159807, acc.: 95.31%] [G loss: 3.332553]\n",
      "3930 [D loss: 0.158945, acc.: 95.31%] [G loss: 2.827556]\n",
      "3931 [D loss: 0.148040, acc.: 95.31%] [G loss: 2.790365]\n",
      "3932 [D loss: 0.144172, acc.: 95.31%] [G loss: 3.526374]\n",
      "3933 [D loss: 0.140523, acc.: 95.31%] [G loss: 2.605201]\n",
      "3934 [D loss: 0.124983, acc.: 95.31%] [G loss: 3.470720]\n",
      "3935 [D loss: 0.156969, acc.: 93.75%] [G loss: 3.181826]\n",
      "3936 [D loss: 0.203497, acc.: 95.31%] [G loss: 3.069575]\n",
      "3937 [D loss: 0.158323, acc.: 93.75%] [G loss: 3.260190]\n",
      "3938 [D loss: 0.147820, acc.: 93.75%] [G loss: 3.003040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3939 [D loss: 0.155238, acc.: 95.31%] [G loss: 2.829620]\n",
      "3940 [D loss: 0.160521, acc.: 95.31%] [G loss: 3.210632]\n",
      "3941 [D loss: 0.167943, acc.: 95.31%] [G loss: 3.116725]\n",
      "3942 [D loss: 0.142643, acc.: 95.31%] [G loss: 3.251152]\n",
      "3943 [D loss: 0.154344, acc.: 95.31%] [G loss: 3.044895]\n",
      "3944 [D loss: 0.160017, acc.: 95.31%] [G loss: 3.153050]\n",
      "3945 [D loss: 0.244351, acc.: 92.19%] [G loss: 2.803316]\n",
      "3946 [D loss: 0.224813, acc.: 92.19%] [G loss: 3.004728]\n",
      "3947 [D loss: 0.228497, acc.: 92.19%] [G loss: 3.071746]\n",
      "3948 [D loss: 0.200886, acc.: 92.19%] [G loss: 3.171251]\n",
      "3949 [D loss: 0.180193, acc.: 95.31%] [G loss: 3.362297]\n",
      "3950 [D loss: 0.145109, acc.: 95.31%] [G loss: 3.014625]\n",
      "3951 [D loss: 0.152740, acc.: 95.31%] [G loss: 2.684675]\n",
      "3952 [D loss: 0.170565, acc.: 93.75%] [G loss: 3.015874]\n",
      "3953 [D loss: 0.123252, acc.: 95.31%] [G loss: 3.371390]\n",
      "3954 [D loss: 0.200223, acc.: 95.31%] [G loss: 2.404772]\n",
      "3955 [D loss: 0.125481, acc.: 95.31%] [G loss: 3.750035]\n",
      "3956 [D loss: 0.175347, acc.: 95.31%] [G loss: 2.990777]\n",
      "3957 [D loss: 0.153936, acc.: 93.75%] [G loss: 4.052497]\n",
      "3958 [D loss: 0.136059, acc.: 95.31%] [G loss: 3.489853]\n",
      "3959 [D loss: 0.160102, acc.: 93.75%] [G loss: 3.277400]\n",
      "3960 [D loss: 0.130837, acc.: 95.31%] [G loss: 2.974780]\n",
      "3961 [D loss: 0.161339, acc.: 95.31%] [G loss: 2.749453]\n",
      "3962 [D loss: 0.143392, acc.: 95.31%] [G loss: 3.362240]\n",
      "3963 [D loss: 0.152180, acc.: 95.31%] [G loss: 3.019070]\n",
      "3964 [D loss: 0.124407, acc.: 95.31%] [G loss: 3.486372]\n",
      "3965 [D loss: 0.164270, acc.: 95.31%] [G loss: 3.217309]\n",
      "3966 [D loss: 0.150296, acc.: 93.75%] [G loss: 3.472121]\n",
      "3967 [D loss: 0.215931, acc.: 90.62%] [G loss: 2.795520]\n",
      "3968 [D loss: 0.234029, acc.: 92.19%] [G loss: 2.822781]\n",
      "3969 [D loss: 0.204569, acc.: 93.75%] [G loss: 2.754084]\n",
      "3970 [D loss: 0.196429, acc.: 93.75%] [G loss: 2.838112]\n",
      "3971 [D loss: 0.158377, acc.: 95.31%] [G loss: 2.824488]\n",
      "3972 [D loss: 0.181973, acc.: 95.31%] [G loss: 2.844830]\n",
      "3973 [D loss: 0.173903, acc.: 93.75%] [G loss: 3.020850]\n",
      "3974 [D loss: 0.167055, acc.: 95.31%] [G loss: 2.952079]\n",
      "3975 [D loss: 0.173853, acc.: 93.75%] [G loss: 2.643969]\n",
      "3976 [D loss: 0.192361, acc.: 92.19%] [G loss: 3.420550]\n",
      "3977 [D loss: 0.164364, acc.: 95.31%] [G loss: 3.210116]\n",
      "3978 [D loss: 0.176521, acc.: 95.31%] [G loss: 2.610710]\n",
      "3979 [D loss: 0.158238, acc.: 95.31%] [G loss: 2.913440]\n",
      "3980 [D loss: 0.132926, acc.: 95.31%] [G loss: 3.363093]\n",
      "3981 [D loss: 0.159754, acc.: 95.31%] [G loss: 2.880217]\n",
      "3982 [D loss: 0.199906, acc.: 93.75%] [G loss: 2.852733]\n",
      "3983 [D loss: 0.139173, acc.: 95.31%] [G loss: 2.863397]\n",
      "3984 [D loss: 0.171025, acc.: 95.31%] [G loss: 3.166421]\n",
      "3985 [D loss: 0.142946, acc.: 95.31%] [G loss: 3.070318]\n",
      "3986 [D loss: 0.139142, acc.: 95.31%] [G loss: 3.006876]\n",
      "3987 [D loss: 0.175720, acc.: 93.75%] [G loss: 3.303119]\n",
      "3988 [D loss: 0.125601, acc.: 95.31%] [G loss: 3.228491]\n",
      "3989 [D loss: 0.143100, acc.: 95.31%] [G loss: 3.484513]\n",
      "3990 [D loss: 0.168528, acc.: 93.75%] [G loss: 2.865362]\n",
      "3991 [D loss: 0.156955, acc.: 95.31%] [G loss: 2.842325]\n",
      "3992 [D loss: 0.189813, acc.: 93.75%] [G loss: 3.267153]\n",
      "3993 [D loss: 0.163470, acc.: 95.31%] [G loss: 2.820006]\n",
      "3994 [D loss: 0.182126, acc.: 93.75%] [G loss: 3.028964]\n",
      "3995 [D loss: 0.163215, acc.: 93.75%] [G loss: 3.229982]\n",
      "3996 [D loss: 0.192320, acc.: 92.19%] [G loss: 3.367027]\n",
      "3997 [D loss: 0.191602, acc.: 95.31%] [G loss: 2.567394]\n",
      "3998 [D loss: 0.135488, acc.: 95.31%] [G loss: 3.226335]\n",
      "3999 [D loss: 0.165907, acc.: 95.31%] [G loss: 2.911598]\n",
      "4000 [D loss: 0.165901, acc.: 95.31%] [G loss: 2.722397]\n",
      "generated_data\n",
      "4001 [D loss: 0.224412, acc.: 92.19%] [G loss: 3.263091]\n",
      "4002 [D loss: 0.204843, acc.: 93.75%] [G loss: 3.199375]\n",
      "4003 [D loss: 0.204710, acc.: 92.19%] [G loss: 2.688094]\n",
      "4004 [D loss: 0.172736, acc.: 93.75%] [G loss: 3.023610]\n",
      "4005 [D loss: 0.165158, acc.: 95.31%] [G loss: 3.452438]\n",
      "4006 [D loss: 0.142440, acc.: 95.31%] [G loss: 3.121969]\n",
      "4007 [D loss: 0.155167, acc.: 95.31%] [G loss: 3.004035]\n",
      "4008 [D loss: 0.163803, acc.: 95.31%] [G loss: 2.833010]\n",
      "4009 [D loss: 0.144000, acc.: 93.75%] [G loss: 3.497086]\n",
      "4010 [D loss: 0.130537, acc.: 95.31%] [G loss: 3.508290]\n",
      "4011 [D loss: 0.176342, acc.: 95.31%] [G loss: 3.054101]\n",
      "4012 [D loss: 0.149156, acc.: 95.31%] [G loss: 2.624698]\n",
      "4013 [D loss: 0.149865, acc.: 95.31%] [G loss: 3.314991]\n",
      "4014 [D loss: 0.155186, acc.: 95.31%] [G loss: 2.963346]\n",
      "4015 [D loss: 0.162754, acc.: 93.75%] [G loss: 2.923060]\n",
      "4016 [D loss: 0.220533, acc.: 92.19%] [G loss: 3.017069]\n",
      "4017 [D loss: 0.165092, acc.: 93.75%] [G loss: 3.363991]\n",
      "4018 [D loss: 0.163961, acc.: 93.75%] [G loss: 3.615481]\n",
      "4019 [D loss: 0.162337, acc.: 95.31%] [G loss: 2.479214]\n",
      "4020 [D loss: 0.163958, acc.: 93.75%] [G loss: 3.210971]\n",
      "4021 [D loss: 0.192752, acc.: 92.19%] [G loss: 3.780175]\n",
      "4022 [D loss: 0.178808, acc.: 93.75%] [G loss: 2.684098]\n",
      "4023 [D loss: 0.142652, acc.: 95.31%] [G loss: 3.466473]\n",
      "4024 [D loss: 0.149975, acc.: 95.31%] [G loss: 2.871256]\n",
      "4025 [D loss: 0.140191, acc.: 95.31%] [G loss: 3.232764]\n",
      "4026 [D loss: 0.150961, acc.: 95.31%] [G loss: 3.551272]\n",
      "4027 [D loss: 0.156875, acc.: 95.31%] [G loss: 2.783602]\n",
      "4028 [D loss: 0.129103, acc.: 95.31%] [G loss: 3.051727]\n",
      "4029 [D loss: 0.140389, acc.: 95.31%] [G loss: 3.188847]\n",
      "4030 [D loss: 0.137713, acc.: 95.31%] [G loss: 2.783723]\n",
      "4031 [D loss: 0.143854, acc.: 95.31%] [G loss: 2.926282]\n",
      "4032 [D loss: 0.126133, acc.: 95.31%] [G loss: 3.611701]\n",
      "4033 [D loss: 0.137892, acc.: 95.31%] [G loss: 3.060020]\n",
      "4034 [D loss: 0.143521, acc.: 95.31%] [G loss: 3.194275]\n",
      "4035 [D loss: 0.180595, acc.: 93.75%] [G loss: 3.479276]\n",
      "4036 [D loss: 0.180710, acc.: 93.75%] [G loss: 2.627633]\n",
      "4037 [D loss: 0.161562, acc.: 93.75%] [G loss: 3.167673]\n",
      "4038 [D loss: 0.168695, acc.: 93.75%] [G loss: 3.360683]\n",
      "4039 [D loss: 0.154910, acc.: 95.31%] [G loss: 3.079733]\n",
      "4040 [D loss: 0.188568, acc.: 93.75%] [G loss: 3.258137]\n",
      "4041 [D loss: 0.144535, acc.: 95.31%] [G loss: 3.043020]\n",
      "4042 [D loss: 0.165733, acc.: 93.75%] [G loss: 3.149378]\n",
      "4043 [D loss: 0.173159, acc.: 95.31%] [G loss: 2.904429]\n",
      "4044 [D loss: 0.187407, acc.: 93.75%] [G loss: 3.101975]\n",
      "4045 [D loss: 0.165398, acc.: 95.31%] [G loss: 3.500849]\n",
      "4046 [D loss: 0.179579, acc.: 95.31%] [G loss: 2.668869]\n",
      "4047 [D loss: 0.172587, acc.: 95.31%] [G loss: 3.334083]\n",
      "4048 [D loss: 0.172467, acc.: 95.31%] [G loss: 2.951678]\n",
      "4049 [D loss: 0.173422, acc.: 95.31%] [G loss: 2.800276]\n",
      "4050 [D loss: 0.139002, acc.: 95.31%] [G loss: 2.953399]\n",
      "4051 [D loss: 0.149173, acc.: 95.31%] [G loss: 2.869868]\n",
      "4052 [D loss: 0.167003, acc.: 93.75%] [G loss: 3.122219]\n",
      "4053 [D loss: 0.157797, acc.: 95.31%] [G loss: 3.168031]\n",
      "4054 [D loss: 0.233633, acc.: 90.62%] [G loss: 3.543513]\n",
      "4055 [D loss: 0.275677, acc.: 90.62%] [G loss: 3.322107]\n",
      "4056 [D loss: 0.198152, acc.: 93.75%] [G loss: 2.890430]\n",
      "4057 [D loss: 0.217113, acc.: 93.75%] [G loss: 2.739887]\n",
      "4058 [D loss: 0.181703, acc.: 95.31%] [G loss: 3.021198]\n",
      "4059 [D loss: 0.167338, acc.: 95.31%] [G loss: 3.213248]\n",
      "4060 [D loss: 0.144276, acc.: 95.31%] [G loss: 3.110274]\n",
      "4061 [D loss: 0.227500, acc.: 92.19%] [G loss: 3.207743]\n",
      "4062 [D loss: 0.196086, acc.: 92.19%] [G loss: 3.092593]\n",
      "4063 [D loss: 0.191946, acc.: 92.19%] [G loss: 3.491918]\n",
      "4064 [D loss: 0.197293, acc.: 93.75%] [G loss: 2.537597]\n",
      "4065 [D loss: 0.143571, acc.: 95.31%] [G loss: 3.594193]\n",
      "4066 [D loss: 0.182728, acc.: 93.75%] [G loss: 3.391370]\n",
      "4067 [D loss: 0.175698, acc.: 95.31%] [G loss: 2.923733]\n",
      "4068 [D loss: 0.171085, acc.: 95.31%] [G loss: 2.778844]\n",
      "4069 [D loss: 0.147277, acc.: 95.31%] [G loss: 3.264652]\n",
      "4070 [D loss: 0.139823, acc.: 95.31%] [G loss: 2.825044]\n",
      "4071 [D loss: 0.175439, acc.: 93.75%] [G loss: 2.849218]\n",
      "4072 [D loss: 0.170183, acc.: 95.31%] [G loss: 2.987362]\n",
      "4073 [D loss: 0.140315, acc.: 95.31%] [G loss: 3.494024]\n",
      "4074 [D loss: 0.159877, acc.: 95.31%] [G loss: 2.859345]\n",
      "4075 [D loss: 0.126191, acc.: 95.31%] [G loss: 3.193726]\n",
      "4076 [D loss: 0.156403, acc.: 95.31%] [G loss: 3.189525]\n",
      "4077 [D loss: 0.147456, acc.: 95.31%] [G loss: 3.346565]\n",
      "4078 [D loss: 0.175224, acc.: 93.75%] [G loss: 2.537399]\n",
      "4079 [D loss: 0.174753, acc.: 95.31%] [G loss: 3.112039]\n",
      "4080 [D loss: 0.173746, acc.: 93.75%] [G loss: 3.082957]\n",
      "4081 [D loss: 0.176891, acc.: 95.31%] [G loss: 3.181581]\n",
      "4082 [D loss: 0.140221, acc.: 95.31%] [G loss: 3.164011]\n",
      "4083 [D loss: 0.153278, acc.: 95.31%] [G loss: 3.150402]\n",
      "4084 [D loss: 0.136146, acc.: 95.31%] [G loss: 3.291261]\n",
      "4085 [D loss: 0.182811, acc.: 93.75%] [G loss: 3.079709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4086 [D loss: 0.139789, acc.: 95.31%] [G loss: 3.216723]\n",
      "4087 [D loss: 0.156732, acc.: 95.31%] [G loss: 3.307383]\n",
      "4088 [D loss: 0.174833, acc.: 93.75%] [G loss: 2.874613]\n",
      "4089 [D loss: 0.179116, acc.: 93.75%] [G loss: 3.152277]\n",
      "4090 [D loss: 0.142883, acc.: 93.75%] [G loss: 3.861498]\n",
      "4091 [D loss: 0.185181, acc.: 95.31%] [G loss: 2.958036]\n",
      "4092 [D loss: 0.228671, acc.: 93.75%] [G loss: 2.981199]\n",
      "4093 [D loss: 0.167739, acc.: 95.31%] [G loss: 2.966295]\n",
      "4094 [D loss: 0.202051, acc.: 93.75%] [G loss: 2.935139]\n",
      "4095 [D loss: 0.175023, acc.: 95.31%] [G loss: 3.239101]\n",
      "4096 [D loss: 0.160408, acc.: 95.31%] [G loss: 3.295672]\n",
      "4097 [D loss: 0.172632, acc.: 95.31%] [G loss: 3.231211]\n",
      "4098 [D loss: 0.148869, acc.: 95.31%] [G loss: 2.615261]\n",
      "4099 [D loss: 0.155106, acc.: 95.31%] [G loss: 2.825152]\n",
      "4100 [D loss: 0.148973, acc.: 93.75%] [G loss: 3.262426]\n",
      "generated_data\n",
      "4101 [D loss: 0.152521, acc.: 95.31%] [G loss: 3.612747]\n",
      "4102 [D loss: 0.153545, acc.: 95.31%] [G loss: 2.772386]\n",
      "4103 [D loss: 0.145976, acc.: 93.75%] [G loss: 3.110737]\n",
      "4104 [D loss: 0.131202, acc.: 95.31%] [G loss: 2.841347]\n",
      "4105 [D loss: 0.162711, acc.: 95.31%] [G loss: 3.213121]\n",
      "4106 [D loss: 0.136456, acc.: 95.31%] [G loss: 3.673490]\n",
      "4107 [D loss: 0.173576, acc.: 95.31%] [G loss: 2.659040]\n",
      "4108 [D loss: 0.153396, acc.: 95.31%] [G loss: 3.062494]\n",
      "4109 [D loss: 0.150432, acc.: 95.31%] [G loss: 3.436437]\n",
      "4110 [D loss: 0.210160, acc.: 93.75%] [G loss: 3.286588]\n",
      "4111 [D loss: 0.162878, acc.: 95.31%] [G loss: 2.775552]\n",
      "4112 [D loss: 0.222421, acc.: 93.75%] [G loss: 2.925605]\n",
      "4113 [D loss: 0.181441, acc.: 93.75%] [G loss: 3.538886]\n",
      "4114 [D loss: 0.175843, acc.: 93.75%] [G loss: 2.893408]\n",
      "4115 [D loss: 0.207939, acc.: 93.75%] [G loss: 3.310324]\n",
      "4116 [D loss: 0.184158, acc.: 93.75%] [G loss: 3.075164]\n",
      "4117 [D loss: 0.187157, acc.: 95.31%] [G loss: 2.803299]\n",
      "4118 [D loss: 0.140013, acc.: 95.31%] [G loss: 3.434767]\n",
      "4119 [D loss: 0.166589, acc.: 95.31%] [G loss: 2.974838]\n",
      "4120 [D loss: 0.163476, acc.: 95.31%] [G loss: 2.848701]\n",
      "4121 [D loss: 0.222243, acc.: 93.75%] [G loss: 3.329679]\n",
      "4122 [D loss: 0.175901, acc.: 93.75%] [G loss: 3.037183]\n",
      "4123 [D loss: 0.168749, acc.: 93.75%] [G loss: 3.613692]\n",
      "4124 [D loss: 0.145426, acc.: 95.31%] [G loss: 3.305745]\n",
      "4125 [D loss: 0.162117, acc.: 95.31%] [G loss: 2.821897]\n",
      "4126 [D loss: 0.163285, acc.: 95.31%] [G loss: 3.118161]\n",
      "4127 [D loss: 0.211119, acc.: 92.19%] [G loss: 3.383868]\n",
      "4128 [D loss: 0.163732, acc.: 95.31%] [G loss: 2.902998]\n",
      "4129 [D loss: 0.232571, acc.: 92.19%] [G loss: 2.951376]\n",
      "4130 [D loss: 0.170125, acc.: 95.31%] [G loss: 3.234224]\n",
      "4131 [D loss: 0.149284, acc.: 95.31%] [G loss: 2.692812]\n",
      "4132 [D loss: 0.125455, acc.: 95.31%] [G loss: 3.418103]\n",
      "4133 [D loss: 0.178709, acc.: 93.75%] [G loss: 3.304868]\n",
      "4134 [D loss: 0.184099, acc.: 93.75%] [G loss: 3.165199]\n",
      "4135 [D loss: 0.199271, acc.: 93.75%] [G loss: 3.231957]\n",
      "4136 [D loss: 0.166479, acc.: 95.31%] [G loss: 3.160073]\n",
      "4137 [D loss: 0.211979, acc.: 92.19%] [G loss: 2.955732]\n",
      "4138 [D loss: 0.147422, acc.: 95.31%] [G loss: 3.333340]\n",
      "4139 [D loss: 0.157809, acc.: 95.31%] [G loss: 2.875010]\n",
      "4140 [D loss: 0.162946, acc.: 95.31%] [G loss: 2.917538]\n",
      "4141 [D loss: 0.185632, acc.: 93.75%] [G loss: 3.055222]\n",
      "4142 [D loss: 0.164433, acc.: 95.31%] [G loss: 3.137887]\n",
      "4143 [D loss: 0.206083, acc.: 93.75%] [G loss: 2.976319]\n",
      "4144 [D loss: 0.192244, acc.: 95.31%] [G loss: 2.711672]\n",
      "4145 [D loss: 0.177109, acc.: 93.75%] [G loss: 3.194157]\n",
      "4146 [D loss: 0.155832, acc.: 95.31%] [G loss: 3.140478]\n",
      "4147 [D loss: 0.158719, acc.: 95.31%] [G loss: 2.837588]\n",
      "4148 [D loss: 0.185907, acc.: 95.31%] [G loss: 2.910025]\n",
      "4149 [D loss: 0.175153, acc.: 95.31%] [G loss: 3.013711]\n",
      "4150 [D loss: 0.177332, acc.: 95.31%] [G loss: 2.712542]\n",
      "4151 [D loss: 0.186276, acc.: 95.31%] [G loss: 2.822774]\n",
      "4152 [D loss: 0.175928, acc.: 93.75%] [G loss: 3.255364]\n",
      "4153 [D loss: 0.167437, acc.: 95.31%] [G loss: 2.912798]\n",
      "4154 [D loss: 0.201495, acc.: 93.75%] [G loss: 2.822749]\n",
      "4155 [D loss: 0.185691, acc.: 93.75%] [G loss: 2.594078]\n",
      "4156 [D loss: 0.162025, acc.: 95.31%] [G loss: 3.502059]\n",
      "4157 [D loss: 0.170309, acc.: 93.75%] [G loss: 2.983059]\n",
      "4158 [D loss: 0.153300, acc.: 95.31%] [G loss: 3.115204]\n",
      "4159 [D loss: 0.176011, acc.: 93.75%] [G loss: 3.246222]\n",
      "4160 [D loss: 0.164645, acc.: 95.31%] [G loss: 2.932851]\n",
      "4161 [D loss: 0.182939, acc.: 95.31%] [G loss: 2.637425]\n",
      "4162 [D loss: 0.183330, acc.: 93.75%] [G loss: 3.274939]\n",
      "4163 [D loss: 0.166171, acc.: 95.31%] [G loss: 3.125396]\n",
      "4164 [D loss: 0.145646, acc.: 95.31%] [G loss: 3.081706]\n",
      "4165 [D loss: 0.130231, acc.: 95.31%] [G loss: 3.377699]\n",
      "4166 [D loss: 0.155993, acc.: 95.31%] [G loss: 2.933704]\n",
      "4167 [D loss: 0.176244, acc.: 93.75%] [G loss: 2.633982]\n",
      "4168 [D loss: 0.169266, acc.: 93.75%] [G loss: 2.953514]\n",
      "4169 [D loss: 0.190359, acc.: 92.19%] [G loss: 2.919720]\n",
      "4170 [D loss: 0.189915, acc.: 93.75%] [G loss: 3.188460]\n",
      "4171 [D loss: 0.158248, acc.: 95.31%] [G loss: 2.911131]\n",
      "4172 [D loss: 0.173449, acc.: 95.31%] [G loss: 2.632077]\n",
      "4173 [D loss: 0.136468, acc.: 95.31%] [G loss: 3.004296]\n",
      "4174 [D loss: 0.173999, acc.: 95.31%] [G loss: 3.221296]\n",
      "4175 [D loss: 0.167368, acc.: 93.75%] [G loss: 3.084806]\n",
      "4176 [D loss: 0.159398, acc.: 95.31%] [G loss: 3.116468]\n",
      "4177 [D loss: 0.180661, acc.: 95.31%] [G loss: 2.866715]\n",
      "4178 [D loss: 0.154278, acc.: 95.31%] [G loss: 2.686696]\n",
      "4179 [D loss: 0.163239, acc.: 95.31%] [G loss: 2.950683]\n",
      "4180 [D loss: 0.161710, acc.: 95.31%] [G loss: 3.093595]\n",
      "4181 [D loss: 0.172308, acc.: 95.31%] [G loss: 2.792711]\n",
      "4182 [D loss: 0.160580, acc.: 95.31%] [G loss: 2.907194]\n",
      "4183 [D loss: 0.203133, acc.: 93.75%] [G loss: 2.826427]\n",
      "4184 [D loss: 0.157257, acc.: 95.31%] [G loss: 3.008567]\n",
      "4185 [D loss: 0.168564, acc.: 93.75%] [G loss: 2.648436]\n",
      "4186 [D loss: 0.211108, acc.: 93.75%] [G loss: 2.541328]\n",
      "4187 [D loss: 0.157946, acc.: 95.31%] [G loss: 2.984784]\n",
      "4188 [D loss: 0.155941, acc.: 95.31%] [G loss: 2.873396]\n",
      "4189 [D loss: 0.146086, acc.: 95.31%] [G loss: 3.464418]\n",
      "4190 [D loss: 0.172594, acc.: 95.31%] [G loss: 2.558893]\n",
      "4191 [D loss: 0.194897, acc.: 93.75%] [G loss: 2.864160]\n",
      "4192 [D loss: 0.166771, acc.: 95.31%] [G loss: 2.824330]\n",
      "4193 [D loss: 0.134417, acc.: 95.31%] [G loss: 3.542639]\n",
      "4194 [D loss: 0.190405, acc.: 93.75%] [G loss: 2.782376]\n",
      "4195 [D loss: 0.183089, acc.: 93.75%] [G loss: 2.889432]\n",
      "4196 [D loss: 0.186065, acc.: 93.75%] [G loss: 2.976681]\n",
      "4197 [D loss: 0.154868, acc.: 95.31%] [G loss: 2.813609]\n",
      "4198 [D loss: 0.178557, acc.: 93.75%] [G loss: 2.816142]\n",
      "4199 [D loss: 0.143916, acc.: 95.31%] [G loss: 2.998390]\n",
      "4200 [D loss: 0.181905, acc.: 95.31%] [G loss: 2.859711]\n",
      "generated_data\n",
      "4201 [D loss: 0.170211, acc.: 93.75%] [G loss: 2.870333]\n",
      "4202 [D loss: 0.142484, acc.: 95.31%] [G loss: 3.266936]\n",
      "4203 [D loss: 0.218193, acc.: 92.19%] [G loss: 3.078479]\n",
      "4204 [D loss: 0.190203, acc.: 93.75%] [G loss: 3.175423]\n",
      "4205 [D loss: 0.168551, acc.: 95.31%] [G loss: 3.025404]\n",
      "4206 [D loss: 0.167624, acc.: 95.31%] [G loss: 2.911560]\n",
      "4207 [D loss: 0.239371, acc.: 90.62%] [G loss: 2.972112]\n",
      "4208 [D loss: 0.231154, acc.: 93.75%] [G loss: 3.307006]\n",
      "4209 [D loss: 0.216319, acc.: 92.19%] [G loss: 3.277163]\n",
      "4210 [D loss: 0.179995, acc.: 95.31%] [G loss: 2.643688]\n",
      "4211 [D loss: 0.199788, acc.: 92.19%] [G loss: 3.045597]\n",
      "4212 [D loss: 0.162357, acc.: 95.31%] [G loss: 2.717194]\n",
      "4213 [D loss: 0.186384, acc.: 95.31%] [G loss: 2.850701]\n",
      "4214 [D loss: 0.169613, acc.: 95.31%] [G loss: 2.829827]\n",
      "4215 [D loss: 0.172770, acc.: 95.31%] [G loss: 2.817201]\n",
      "4216 [D loss: 0.157874, acc.: 95.31%] [G loss: 3.082144]\n",
      "4217 [D loss: 0.185846, acc.: 93.75%] [G loss: 3.570111]\n",
      "4218 [D loss: 0.190376, acc.: 93.75%] [G loss: 2.848405]\n",
      "4219 [D loss: 0.168856, acc.: 95.31%] [G loss: 2.702012]\n",
      "4220 [D loss: 0.172642, acc.: 95.31%] [G loss: 2.812475]\n",
      "4221 [D loss: 0.157116, acc.: 93.75%] [G loss: 3.323134]\n",
      "4222 [D loss: 0.172457, acc.: 95.31%] [G loss: 2.618430]\n",
      "4223 [D loss: 0.159807, acc.: 95.31%] [G loss: 2.689152]\n",
      "4224 [D loss: 0.195903, acc.: 92.19%] [G loss: 3.024975]\n",
      "4225 [D loss: 0.174663, acc.: 95.31%] [G loss: 2.608733]\n",
      "4226 [D loss: 0.207180, acc.: 93.75%] [G loss: 2.751875]\n",
      "4227 [D loss: 0.222938, acc.: 93.75%] [G loss: 2.828585]\n",
      "4228 [D loss: 0.178308, acc.: 95.31%] [G loss: 3.395918]\n",
      "4229 [D loss: 0.198276, acc.: 92.19%] [G loss: 2.968625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4230 [D loss: 0.168865, acc.: 95.31%] [G loss: 3.141038]\n",
      "4231 [D loss: 0.235109, acc.: 93.75%] [G loss: 2.743176]\n",
      "4232 [D loss: 0.181776, acc.: 93.75%] [G loss: 2.601992]\n",
      "4233 [D loss: 0.206751, acc.: 92.19%] [G loss: 2.484788]\n",
      "4234 [D loss: 0.172381, acc.: 95.31%] [G loss: 2.642869]\n",
      "4235 [D loss: 0.167042, acc.: 95.31%] [G loss: 2.718255]\n",
      "4236 [D loss: 0.257179, acc.: 92.19%] [G loss: 2.714686]\n",
      "4237 [D loss: 0.159669, acc.: 95.31%] [G loss: 3.137502]\n",
      "4238 [D loss: 0.171735, acc.: 93.75%] [G loss: 2.687698]\n",
      "4239 [D loss: 0.224217, acc.: 93.75%] [G loss: 2.863782]\n",
      "4240 [D loss: 0.184618, acc.: 93.75%] [G loss: 2.849061]\n",
      "4241 [D loss: 0.188974, acc.: 95.31%] [G loss: 2.982775]\n",
      "4242 [D loss: 0.172584, acc.: 95.31%] [G loss: 3.309052]\n",
      "4243 [D loss: 0.159216, acc.: 95.31%] [G loss: 2.993265]\n",
      "4244 [D loss: 0.171808, acc.: 95.31%] [G loss: 2.598019]\n",
      "4245 [D loss: 0.137734, acc.: 95.31%] [G loss: 2.909461]\n",
      "4246 [D loss: 0.153339, acc.: 93.75%] [G loss: 3.232790]\n",
      "4247 [D loss: 0.153534, acc.: 95.31%] [G loss: 3.010939]\n",
      "4248 [D loss: 0.178840, acc.: 95.31%] [G loss: 2.815808]\n",
      "4249 [D loss: 0.139350, acc.: 95.31%] [G loss: 3.116593]\n",
      "4250 [D loss: 0.155023, acc.: 95.31%] [G loss: 2.634500]\n",
      "4251 [D loss: 0.151207, acc.: 95.31%] [G loss: 2.983485]\n",
      "4252 [D loss: 0.180249, acc.: 92.19%] [G loss: 3.572536]\n",
      "4253 [D loss: 0.177510, acc.: 93.75%] [G loss: 2.682292]\n",
      "4254 [D loss: 0.194061, acc.: 93.75%] [G loss: 2.876622]\n",
      "4255 [D loss: 0.141624, acc.: 95.31%] [G loss: 3.251748]\n",
      "4256 [D loss: 0.159625, acc.: 95.31%] [G loss: 2.735092]\n",
      "4257 [D loss: 0.152650, acc.: 95.31%] [G loss: 2.995702]\n",
      "4258 [D loss: 0.192546, acc.: 92.19%] [G loss: 3.494359]\n",
      "4259 [D loss: 0.191424, acc.: 95.31%] [G loss: 2.436954]\n",
      "4260 [D loss: 0.178900, acc.: 93.75%] [G loss: 3.194491]\n",
      "4261 [D loss: 0.162529, acc.: 95.31%] [G loss: 3.078164]\n",
      "4262 [D loss: 0.182834, acc.: 95.31%] [G loss: 2.773441]\n",
      "4263 [D loss: 0.219777, acc.: 92.19%] [G loss: 2.991821]\n",
      "4264 [D loss: 0.169983, acc.: 93.75%] [G loss: 2.968883]\n",
      "4265 [D loss: 0.187691, acc.: 95.31%] [G loss: 2.650495]\n",
      "4266 [D loss: 0.189400, acc.: 93.75%] [G loss: 3.125125]\n",
      "4267 [D loss: 0.172339, acc.: 93.75%] [G loss: 3.065372]\n",
      "4268 [D loss: 0.171250, acc.: 95.31%] [G loss: 2.792591]\n",
      "4269 [D loss: 0.175307, acc.: 95.31%] [G loss: 2.715473]\n",
      "4270 [D loss: 0.169624, acc.: 95.31%] [G loss: 3.192422]\n",
      "4271 [D loss: 0.206680, acc.: 93.75%] [G loss: 3.426105]\n",
      "4272 [D loss: 0.163756, acc.: 95.31%] [G loss: 2.785964]\n",
      "4273 [D loss: 0.146752, acc.: 93.75%] [G loss: 3.772515]\n",
      "4274 [D loss: 0.179318, acc.: 93.75%] [G loss: 2.447087]\n",
      "4275 [D loss: 0.178957, acc.: 93.75%] [G loss: 3.236692]\n",
      "4276 [D loss: 0.152414, acc.: 93.75%] [G loss: 3.211032]\n",
      "4277 [D loss: 0.151501, acc.: 93.75%] [G loss: 3.733772]\n",
      "4278 [D loss: 0.181748, acc.: 93.75%] [G loss: 2.641668]\n",
      "4279 [D loss: 0.151387, acc.: 95.31%] [G loss: 3.442104]\n",
      "4280 [D loss: 0.176310, acc.: 95.31%] [G loss: 2.742195]\n",
      "4281 [D loss: 0.124590, acc.: 95.31%] [G loss: 4.028127]\n",
      "4282 [D loss: 0.176422, acc.: 95.31%] [G loss: 3.073793]\n",
      "4283 [D loss: 0.110901, acc.: 95.31%] [G loss: 3.854545]\n",
      "4284 [D loss: 0.129165, acc.: 95.31%] [G loss: 3.271480]\n",
      "4285 [D loss: 0.173915, acc.: 95.31%] [G loss: 2.853523]\n",
      "4286 [D loss: 0.147233, acc.: 95.31%] [G loss: 3.025426]\n",
      "4287 [D loss: 0.121921, acc.: 95.31%] [G loss: 3.392994]\n",
      "4288 [D loss: 0.185851, acc.: 93.75%] [G loss: 3.148347]\n",
      "4289 [D loss: 0.181314, acc.: 93.75%] [G loss: 2.781509]\n",
      "4290 [D loss: 0.166562, acc.: 95.31%] [G loss: 3.243768]\n",
      "4291 [D loss: 0.198462, acc.: 92.19%] [G loss: 3.123289]\n",
      "4292 [D loss: 0.141756, acc.: 95.31%] [G loss: 3.174668]\n",
      "4293 [D loss: 0.174532, acc.: 95.31%] [G loss: 2.875422]\n",
      "4294 [D loss: 0.174412, acc.: 93.75%] [G loss: 2.860306]\n",
      "4295 [D loss: 0.165676, acc.: 93.75%] [G loss: 3.114059]\n",
      "4296 [D loss: 0.170176, acc.: 95.31%] [G loss: 3.065922]\n",
      "4297 [D loss: 0.204369, acc.: 93.75%] [G loss: 3.228485]\n",
      "4298 [D loss: 0.194916, acc.: 92.19%] [G loss: 3.036802]\n",
      "4299 [D loss: 0.192439, acc.: 92.19%] [G loss: 2.358193]\n",
      "4300 [D loss: 0.165053, acc.: 95.31%] [G loss: 2.625948]\n",
      "generated_data\n",
      "4301 [D loss: 0.154895, acc.: 95.31%] [G loss: 3.387191]\n",
      "4302 [D loss: 0.144961, acc.: 95.31%] [G loss: 3.092523]\n",
      "4303 [D loss: 0.139688, acc.: 95.31%] [G loss: 2.863214]\n",
      "4304 [D loss: 0.177428, acc.: 95.31%] [G loss: 2.574287]\n",
      "4305 [D loss: 0.163531, acc.: 95.31%] [G loss: 2.643954]\n",
      "4306 [D loss: 0.150489, acc.: 95.31%] [G loss: 3.088251]\n",
      "4307 [D loss: 0.153518, acc.: 95.31%] [G loss: 3.086262]\n",
      "4308 [D loss: 0.129494, acc.: 95.31%] [G loss: 2.896384]\n",
      "4309 [D loss: 0.174686, acc.: 93.75%] [G loss: 2.869219]\n",
      "4310 [D loss: 0.206728, acc.: 93.75%] [G loss: 2.950415]\n",
      "4311 [D loss: 0.171096, acc.: 95.31%] [G loss: 3.025277]\n",
      "4312 [D loss: 0.153653, acc.: 95.31%] [G loss: 3.328769]\n",
      "4313 [D loss: 0.178150, acc.: 95.31%] [G loss: 2.794314]\n",
      "4314 [D loss: 0.174359, acc.: 93.75%] [G loss: 3.050184]\n",
      "4315 [D loss: 0.143839, acc.: 95.31%] [G loss: 3.373412]\n",
      "4316 [D loss: 0.167866, acc.: 95.31%] [G loss: 2.666188]\n",
      "4317 [D loss: 0.162887, acc.: 95.31%] [G loss: 3.247482]\n",
      "4318 [D loss: 0.138448, acc.: 95.31%] [G loss: 3.274226]\n",
      "4319 [D loss: 0.159042, acc.: 93.75%] [G loss: 2.941215]\n",
      "4320 [D loss: 0.132497, acc.: 95.31%] [G loss: 3.122306]\n",
      "4321 [D loss: 0.185952, acc.: 93.75%] [G loss: 2.876415]\n",
      "4322 [D loss: 0.203905, acc.: 93.75%] [G loss: 2.786717]\n",
      "4323 [D loss: 0.166768, acc.: 93.75%] [G loss: 2.482567]\n",
      "4324 [D loss: 0.154285, acc.: 92.19%] [G loss: 3.421736]\n",
      "4325 [D loss: 0.168232, acc.: 95.31%] [G loss: 3.021554]\n",
      "4326 [D loss: 0.175210, acc.: 95.31%] [G loss: 2.923763]\n",
      "4327 [D loss: 0.149881, acc.: 95.31%] [G loss: 2.965176]\n",
      "4328 [D loss: 0.153583, acc.: 95.31%] [G loss: 2.969781]\n",
      "4329 [D loss: 0.196902, acc.: 93.75%] [G loss: 2.928831]\n",
      "4330 [D loss: 0.165107, acc.: 95.31%] [G loss: 3.594011]\n",
      "4331 [D loss: 0.200645, acc.: 92.19%] [G loss: 3.036969]\n",
      "4332 [D loss: 0.164429, acc.: 93.75%] [G loss: 3.471440]\n",
      "4333 [D loss: 0.176303, acc.: 93.75%] [G loss: 2.972674]\n",
      "4334 [D loss: 0.173085, acc.: 95.31%] [G loss: 3.072547]\n",
      "4335 [D loss: 0.171316, acc.: 93.75%] [G loss: 3.424136]\n",
      "4336 [D loss: 0.163823, acc.: 95.31%] [G loss: 2.880593]\n",
      "4337 [D loss: 0.149091, acc.: 93.75%] [G loss: 3.010910]\n",
      "4338 [D loss: 0.143527, acc.: 95.31%] [G loss: 3.170596]\n",
      "4339 [D loss: 0.154550, acc.: 95.31%] [G loss: 3.138531]\n",
      "4340 [D loss: 0.152856, acc.: 95.31%] [G loss: 3.594905]\n",
      "4341 [D loss: 0.143014, acc.: 95.31%] [G loss: 3.115591]\n",
      "4342 [D loss: 0.186122, acc.: 93.75%] [G loss: 2.550716]\n",
      "4343 [D loss: 0.149569, acc.: 95.31%] [G loss: 3.037570]\n",
      "4344 [D loss: 0.193166, acc.: 92.19%] [G loss: 3.505541]\n",
      "4345 [D loss: 0.190157, acc.: 95.31%] [G loss: 2.857962]\n",
      "4346 [D loss: 0.153498, acc.: 93.75%] [G loss: 3.042356]\n",
      "4347 [D loss: 0.169890, acc.: 95.31%] [G loss: 2.914766]\n",
      "4348 [D loss: 0.136826, acc.: 95.31%] [G loss: 3.062207]\n",
      "4349 [D loss: 0.139631, acc.: 95.31%] [G loss: 3.087789]\n",
      "4350 [D loss: 0.140293, acc.: 95.31%] [G loss: 3.117419]\n",
      "4351 [D loss: 0.125682, acc.: 95.31%] [G loss: 3.416118]\n",
      "4352 [D loss: 0.157679, acc.: 95.31%] [G loss: 3.154838]\n",
      "4353 [D loss: 0.149051, acc.: 93.75%] [G loss: 4.128037]\n",
      "4354 [D loss: 0.139163, acc.: 95.31%] [G loss: 2.754903]\n",
      "4355 [D loss: 0.126938, acc.: 95.31%] [G loss: 3.581644]\n",
      "4356 [D loss: 0.134335, acc.: 95.31%] [G loss: 3.116636]\n",
      "4357 [D loss: 0.104405, acc.: 95.31%] [G loss: 4.147090]\n",
      "4358 [D loss: 0.178912, acc.: 95.31%] [G loss: 3.018141]\n",
      "4359 [D loss: 0.122223, acc.: 95.31%] [G loss: 3.525953]\n",
      "4360 [D loss: 0.151194, acc.: 95.31%] [G loss: 2.957033]\n",
      "4361 [D loss: 0.153365, acc.: 95.31%] [G loss: 3.601549]\n",
      "4362 [D loss: 0.168728, acc.: 93.75%] [G loss: 3.209224]\n",
      "4363 [D loss: 0.168128, acc.: 95.31%] [G loss: 3.038224]\n",
      "4364 [D loss: 0.163884, acc.: 95.31%] [G loss: 3.404915]\n",
      "4365 [D loss: 0.163807, acc.: 95.31%] [G loss: 3.241799]\n",
      "4366 [D loss: 0.178819, acc.: 95.31%] [G loss: 3.294907]\n",
      "4367 [D loss: 0.211494, acc.: 93.75%] [G loss: 3.526899]\n",
      "4368 [D loss: 0.188354, acc.: 95.31%] [G loss: 3.037270]\n",
      "4369 [D loss: 0.198391, acc.: 95.31%] [G loss: 2.700263]\n",
      "4370 [D loss: 0.182908, acc.: 95.31%] [G loss: 3.080036]\n",
      "4371 [D loss: 0.155554, acc.: 95.31%] [G loss: 3.403124]\n",
      "4372 [D loss: 0.156360, acc.: 95.31%] [G loss: 2.992793]\n",
      "4373 [D loss: 0.177694, acc.: 95.31%] [G loss: 3.031196]\n",
      "4374 [D loss: 0.167552, acc.: 95.31%] [G loss: 2.796208]\n",
      "4375 [D loss: 0.160031, acc.: 95.31%] [G loss: 2.982306]\n",
      "4376 [D loss: 0.155977, acc.: 95.31%] [G loss: 2.849502]\n",
      "4377 [D loss: 0.149297, acc.: 95.31%] [G loss: 3.211719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4378 [D loss: 0.151441, acc.: 95.31%] [G loss: 3.484259]\n",
      "4379 [D loss: 0.169548, acc.: 95.31%] [G loss: 2.476792]\n",
      "4380 [D loss: 0.141502, acc.: 95.31%] [G loss: 3.493213]\n",
      "4381 [D loss: 0.136579, acc.: 95.31%] [G loss: 3.209952]\n",
      "4382 [D loss: 0.156129, acc.: 95.31%] [G loss: 3.025383]\n",
      "4383 [D loss: 0.147820, acc.: 95.31%] [G loss: 3.770895]\n",
      "4384 [D loss: 0.154920, acc.: 93.75%] [G loss: 3.469298]\n",
      "4385 [D loss: 0.159343, acc.: 95.31%] [G loss: 2.729282]\n",
      "4386 [D loss: 0.139044, acc.: 95.31%] [G loss: 3.498157]\n",
      "4387 [D loss: 0.154717, acc.: 93.75%] [G loss: 3.520923]\n",
      "4388 [D loss: 0.178960, acc.: 95.31%] [G loss: 3.164046]\n",
      "4389 [D loss: 0.126114, acc.: 95.31%] [G loss: 3.713347]\n",
      "4390 [D loss: 0.166341, acc.: 95.31%] [G loss: 3.010035]\n",
      "4391 [D loss: 0.176375, acc.: 93.75%] [G loss: 2.848789]\n",
      "4392 [D loss: 0.158538, acc.: 95.31%] [G loss: 3.445500]\n",
      "4393 [D loss: 0.143827, acc.: 95.31%] [G loss: 3.358794]\n",
      "4394 [D loss: 0.171484, acc.: 95.31%] [G loss: 2.909859]\n",
      "4395 [D loss: 0.173670, acc.: 93.75%] [G loss: 3.192864]\n",
      "4396 [D loss: 0.181242, acc.: 95.31%] [G loss: 3.493350]\n",
      "4397 [D loss: 0.177560, acc.: 95.31%] [G loss: 3.059572]\n",
      "4398 [D loss: 0.168364, acc.: 95.31%] [G loss: 2.831199]\n",
      "4399 [D loss: 0.135517, acc.: 95.31%] [G loss: 3.130210]\n",
      "4400 [D loss: 0.171132, acc.: 95.31%] [G loss: 2.846729]\n",
      "generated_data\n",
      "4401 [D loss: 0.165693, acc.: 95.31%] [G loss: 2.858555]\n",
      "4402 [D loss: 0.169520, acc.: 93.75%] [G loss: 3.703710]\n",
      "4403 [D loss: 0.129459, acc.: 95.31%] [G loss: 3.240152]\n",
      "4404 [D loss: 0.150803, acc.: 95.31%] [G loss: 2.833764]\n",
      "4405 [D loss: 0.162231, acc.: 93.75%] [G loss: 3.316844]\n",
      "4406 [D loss: 0.157727, acc.: 95.31%] [G loss: 3.195039]\n",
      "4407 [D loss: 0.175932, acc.: 93.75%] [G loss: 3.014902]\n",
      "4408 [D loss: 0.191320, acc.: 92.19%] [G loss: 3.261173]\n",
      "4409 [D loss: 0.158333, acc.: 93.75%] [G loss: 4.012904]\n",
      "4410 [D loss: 0.170114, acc.: 95.31%] [G loss: 3.045735]\n",
      "4411 [D loss: 0.132371, acc.: 95.31%] [G loss: 3.731848]\n",
      "4412 [D loss: 0.154824, acc.: 93.75%] [G loss: 3.161728]\n",
      "4413 [D loss: 0.140273, acc.: 95.31%] [G loss: 2.922309]\n",
      "4414 [D loss: 0.237999, acc.: 92.19%] [G loss: 3.451762]\n",
      "4415 [D loss: 0.214679, acc.: 92.19%] [G loss: 3.308809]\n",
      "4416 [D loss: 0.193808, acc.: 93.75%] [G loss: 3.071111]\n",
      "4417 [D loss: 0.171339, acc.: 95.31%] [G loss: 2.821825]\n",
      "4418 [D loss: 0.165017, acc.: 95.31%] [G loss: 3.160805]\n",
      "4419 [D loss: 0.155544, acc.: 95.31%] [G loss: 3.589961]\n",
      "4420 [D loss: 0.174618, acc.: 95.31%] [G loss: 3.190620]\n",
      "4421 [D loss: 0.174639, acc.: 95.31%] [G loss: 2.851977]\n",
      "4422 [D loss: 0.171499, acc.: 95.31%] [G loss: 2.819796]\n",
      "4423 [D loss: 0.202764, acc.: 93.75%] [G loss: 3.060630]\n",
      "4424 [D loss: 0.164384, acc.: 95.31%] [G loss: 3.063070]\n",
      "4425 [D loss: 0.166853, acc.: 95.31%] [G loss: 2.918686]\n",
      "4426 [D loss: 0.196716, acc.: 93.75%] [G loss: 3.038893]\n",
      "4427 [D loss: 0.180432, acc.: 95.31%] [G loss: 2.943836]\n",
      "4428 [D loss: 0.142653, acc.: 95.31%] [G loss: 2.930728]\n",
      "4429 [D loss: 0.156843, acc.: 95.31%] [G loss: 3.028204]\n",
      "4430 [D loss: 0.178998, acc.: 95.31%] [G loss: 3.007990]\n",
      "4431 [D loss: 0.157354, acc.: 95.31%] [G loss: 3.413116]\n",
      "4432 [D loss: 0.166994, acc.: 95.31%] [G loss: 3.088484]\n",
      "4433 [D loss: 0.143985, acc.: 95.31%] [G loss: 2.805861]\n",
      "4434 [D loss: 0.151514, acc.: 95.31%] [G loss: 3.221076]\n",
      "4435 [D loss: 0.156427, acc.: 93.75%] [G loss: 3.130523]\n",
      "4436 [D loss: 0.172398, acc.: 95.31%] [G loss: 2.957960]\n",
      "4437 [D loss: 0.165365, acc.: 95.31%] [G loss: 2.984036]\n",
      "4438 [D loss: 0.171478, acc.: 93.75%] [G loss: 3.147814]\n",
      "4439 [D loss: 0.247004, acc.: 92.19%] [G loss: 2.728651]\n",
      "4440 [D loss: 0.144717, acc.: 95.31%] [G loss: 3.429247]\n",
      "4441 [D loss: 0.170050, acc.: 93.75%] [G loss: 3.288788]\n",
      "4442 [D loss: 0.159104, acc.: 95.31%] [G loss: 2.557538]\n",
      "4443 [D loss: 0.125403, acc.: 95.31%] [G loss: 3.302979]\n",
      "4444 [D loss: 0.154997, acc.: 95.31%] [G loss: 3.025032]\n",
      "4445 [D loss: 0.130271, acc.: 95.31%] [G loss: 3.364023]\n",
      "4446 [D loss: 0.175887, acc.: 93.75%] [G loss: 2.847422]\n",
      "4447 [D loss: 0.114567, acc.: 95.31%] [G loss: 4.041573]\n",
      "4448 [D loss: 0.194233, acc.: 93.75%] [G loss: 3.724960]\n",
      "4449 [D loss: 0.134674, acc.: 95.31%] [G loss: 3.818143]\n",
      "4450 [D loss: 0.124908, acc.: 95.31%] [G loss: 3.437367]\n",
      "4451 [D loss: 0.137394, acc.: 95.31%] [G loss: 2.879235]\n",
      "4452 [D loss: 0.216248, acc.: 93.75%] [G loss: 2.850869]\n",
      "4453 [D loss: 0.141554, acc.: 95.31%] [G loss: 3.334663]\n",
      "4454 [D loss: 0.161641, acc.: 95.31%] [G loss: 2.493433]\n",
      "4455 [D loss: 0.126438, acc.: 95.31%] [G loss: 3.275159]\n",
      "4456 [D loss: 0.215933, acc.: 93.75%] [G loss: 3.090006]\n",
      "4457 [D loss: 0.159469, acc.: 95.31%] [G loss: 2.854399]\n",
      "4458 [D loss: 0.187777, acc.: 93.75%] [G loss: 2.608984]\n",
      "4459 [D loss: 0.166916, acc.: 95.31%] [G loss: 2.719419]\n",
      "4460 [D loss: 0.160874, acc.: 95.31%] [G loss: 2.829571]\n",
      "4461 [D loss: 0.175564, acc.: 95.31%] [G loss: 2.996407]\n",
      "4462 [D loss: 0.152622, acc.: 95.31%] [G loss: 2.974529]\n",
      "4463 [D loss: 0.165857, acc.: 95.31%] [G loss: 3.116364]\n",
      "4464 [D loss: 0.152818, acc.: 95.31%] [G loss: 3.111702]\n",
      "4465 [D loss: 0.153688, acc.: 95.31%] [G loss: 3.172262]\n",
      "4466 [D loss: 0.138327, acc.: 95.31%] [G loss: 2.681468]\n",
      "4467 [D loss: 0.168589, acc.: 95.31%] [G loss: 3.163248]\n",
      "4468 [D loss: 0.191433, acc.: 93.75%] [G loss: 3.126279]\n",
      "4469 [D loss: 0.141662, acc.: 95.31%] [G loss: 3.453506]\n",
      "4470 [D loss: 0.180883, acc.: 93.75%] [G loss: 2.865044]\n",
      "4471 [D loss: 0.121339, acc.: 95.31%] [G loss: 3.322985]\n",
      "4472 [D loss: 0.141607, acc.: 95.31%] [G loss: 3.330235]\n",
      "4473 [D loss: 0.171637, acc.: 93.75%] [G loss: 2.927782]\n",
      "4474 [D loss: 0.172791, acc.: 95.31%] [G loss: 2.601267]\n",
      "4475 [D loss: 0.187210, acc.: 93.75%] [G loss: 3.363374]\n",
      "4476 [D loss: 0.136523, acc.: 95.31%] [G loss: 3.189563]\n",
      "4477 [D loss: 0.179565, acc.: 93.75%] [G loss: 3.328301]\n",
      "4478 [D loss: 0.141449, acc.: 95.31%] [G loss: 3.609252]\n",
      "4479 [D loss: 0.168240, acc.: 95.31%] [G loss: 3.146831]\n",
      "4480 [D loss: 0.163923, acc.: 95.31%] [G loss: 2.914192]\n",
      "4481 [D loss: 0.217937, acc.: 92.19%] [G loss: 2.681411]\n",
      "4482 [D loss: 0.145531, acc.: 95.31%] [G loss: 3.491080]\n",
      "4483 [D loss: 0.171158, acc.: 93.75%] [G loss: 3.217490]\n",
      "4484 [D loss: 0.189756, acc.: 92.19%] [G loss: 3.053661]\n",
      "4485 [D loss: 0.187924, acc.: 93.75%] [G loss: 2.581934]\n",
      "4486 [D loss: 0.165683, acc.: 93.75%] [G loss: 2.778503]\n",
      "4487 [D loss: 0.165826, acc.: 95.31%] [G loss: 2.706851]\n",
      "4488 [D loss: 0.150001, acc.: 95.31%] [G loss: 3.382010]\n",
      "4489 [D loss: 0.201409, acc.: 92.19%] [G loss: 3.949837]\n",
      "4490 [D loss: 0.193657, acc.: 93.75%] [G loss: 2.842485]\n",
      "4491 [D loss: 0.120732, acc.: 95.31%] [G loss: 4.516312]\n",
      "4492 [D loss: 0.177041, acc.: 93.75%] [G loss: 3.457524]\n",
      "4493 [D loss: 0.116825, acc.: 95.31%] [G loss: 3.568166]\n",
      "4494 [D loss: 0.200313, acc.: 92.19%] [G loss: 3.323531]\n",
      "4495 [D loss: 0.125745, acc.: 95.31%] [G loss: 3.567140]\n",
      "4496 [D loss: 0.189310, acc.: 93.75%] [G loss: 3.156257]\n",
      "4497 [D loss: 0.120968, acc.: 95.31%] [G loss: 3.486282]\n",
      "4498 [D loss: 0.156097, acc.: 95.31%] [G loss: 2.921275]\n",
      "4499 [D loss: 0.145112, acc.: 93.75%] [G loss: 4.235989]\n",
      "4500 [D loss: 0.166489, acc.: 95.31%] [G loss: 3.018278]\n",
      "generated_data\n",
      "4501 [D loss: 0.129461, acc.: 95.31%] [G loss: 3.460839]\n",
      "4502 [D loss: 0.180618, acc.: 95.31%] [G loss: 2.722071]\n",
      "4503 [D loss: 0.176825, acc.: 93.75%] [G loss: 3.278428]\n",
      "4504 [D loss: 0.162940, acc.: 95.31%] [G loss: 3.003160]\n",
      "4505 [D loss: 0.160633, acc.: 95.31%] [G loss: 2.789844]\n",
      "4506 [D loss: 0.155176, acc.: 95.31%] [G loss: 3.049665]\n",
      "4507 [D loss: 0.154397, acc.: 95.31%] [G loss: 3.326164]\n",
      "4508 [D loss: 0.189857, acc.: 93.75%] [G loss: 3.033178]\n",
      "4509 [D loss: 0.154613, acc.: 95.31%] [G loss: 3.068970]\n",
      "4510 [D loss: 0.148008, acc.: 95.31%] [G loss: 3.162371]\n",
      "4511 [D loss: 0.191586, acc.: 93.75%] [G loss: 3.068242]\n",
      "4512 [D loss: 0.162060, acc.: 95.31%] [G loss: 2.918531]\n",
      "4513 [D loss: 0.172801, acc.: 95.31%] [G loss: 3.159593]\n",
      "4514 [D loss: 0.164617, acc.: 95.31%] [G loss: 2.955979]\n",
      "4515 [D loss: 0.171564, acc.: 95.31%] [G loss: 2.941043]\n",
      "4516 [D loss: 0.159124, acc.: 95.31%] [G loss: 2.944438]\n",
      "4517 [D loss: 0.165750, acc.: 95.31%] [G loss: 2.882102]\n",
      "4518 [D loss: 0.160625, acc.: 95.31%] [G loss: 3.269542]\n",
      "4519 [D loss: 0.168033, acc.: 93.75%] [G loss: 3.368446]\n",
      "4520 [D loss: 0.211843, acc.: 93.75%] [G loss: 3.009923]\n",
      "4521 [D loss: 0.188346, acc.: 93.75%] [G loss: 2.920820]\n",
      "4522 [D loss: 0.170005, acc.: 95.31%] [G loss: 2.984594]\n",
      "4523 [D loss: 0.171481, acc.: 95.31%] [G loss: 3.158400]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4524 [D loss: 0.188948, acc.: 95.31%] [G loss: 2.758838]\n",
      "4525 [D loss: 0.153636, acc.: 95.31%] [G loss: 3.202270]\n",
      "4526 [D loss: 0.172858, acc.: 95.31%] [G loss: 2.921453]\n",
      "4527 [D loss: 0.159903, acc.: 95.31%] [G loss: 2.693990]\n",
      "4528 [D loss: 0.188525, acc.: 93.75%] [G loss: 3.240093]\n",
      "4529 [D loss: 0.183061, acc.: 93.75%] [G loss: 2.939623]\n",
      "4530 [D loss: 0.168570, acc.: 95.31%] [G loss: 3.025815]\n",
      "4531 [D loss: 0.195799, acc.: 92.19%] [G loss: 3.727980]\n",
      "4532 [D loss: 0.171092, acc.: 95.31%] [G loss: 3.233081]\n",
      "4533 [D loss: 0.201210, acc.: 90.62%] [G loss: 2.461762]\n",
      "4534 [D loss: 0.175554, acc.: 95.31%] [G loss: 2.908000]\n",
      "4535 [D loss: 0.164245, acc.: 95.31%] [G loss: 2.996127]\n",
      "4536 [D loss: 0.186946, acc.: 93.75%] [G loss: 3.185366]\n",
      "4537 [D loss: 0.167635, acc.: 95.31%] [G loss: 2.632396]\n",
      "4538 [D loss: 0.188373, acc.: 93.75%] [G loss: 3.027134]\n",
      "4539 [D loss: 0.171913, acc.: 95.31%] [G loss: 2.887422]\n",
      "4540 [D loss: 0.243801, acc.: 92.19%] [G loss: 3.020182]\n",
      "4541 [D loss: 0.160935, acc.: 92.19%] [G loss: 3.295334]\n",
      "4542 [D loss: 0.154086, acc.: 95.31%] [G loss: 2.773769]\n",
      "4543 [D loss: 0.173206, acc.: 93.75%] [G loss: 2.502470]\n",
      "4544 [D loss: 0.208351, acc.: 90.62%] [G loss: 3.181863]\n",
      "4545 [D loss: 0.170173, acc.: 95.31%] [G loss: 3.374324]\n",
      "4546 [D loss: 0.194346, acc.: 95.31%] [G loss: 2.878794]\n",
      "4547 [D loss: 0.154552, acc.: 95.31%] [G loss: 2.690002]\n",
      "4548 [D loss: 0.182543, acc.: 93.75%] [G loss: 3.294711]\n",
      "4549 [D loss: 0.164334, acc.: 95.31%] [G loss: 3.449054]\n",
      "4550 [D loss: 0.186956, acc.: 93.75%] [G loss: 2.719406]\n",
      "4551 [D loss: 0.165764, acc.: 95.31%] [G loss: 2.861328]\n",
      "4552 [D loss: 0.182991, acc.: 93.75%] [G loss: 3.051733]\n",
      "4553 [D loss: 0.154946, acc.: 95.31%] [G loss: 3.133805]\n",
      "4554 [D loss: 0.160701, acc.: 93.75%] [G loss: 3.272584]\n",
      "4555 [D loss: 0.156823, acc.: 95.31%] [G loss: 3.056237]\n",
      "4556 [D loss: 0.164710, acc.: 95.31%] [G loss: 3.488463]\n",
      "4557 [D loss: 0.170241, acc.: 95.31%] [G loss: 3.186716]\n",
      "4558 [D loss: 0.153166, acc.: 95.31%] [G loss: 3.029117]\n",
      "4559 [D loss: 0.162247, acc.: 95.31%] [G loss: 2.776771]\n",
      "4560 [D loss: 0.204848, acc.: 92.19%] [G loss: 3.100938]\n",
      "4561 [D loss: 0.156379, acc.: 93.75%] [G loss: 3.935630]\n",
      "4562 [D loss: 0.191501, acc.: 93.75%] [G loss: 2.397624]\n",
      "4563 [D loss: 0.174533, acc.: 95.31%] [G loss: 2.590098]\n",
      "4564 [D loss: 0.211449, acc.: 93.75%] [G loss: 3.178320]\n",
      "4565 [D loss: 0.197617, acc.: 95.31%] [G loss: 3.092286]\n",
      "4566 [D loss: 0.215873, acc.: 92.19%] [G loss: 2.858974]\n",
      "4567 [D loss: 0.168205, acc.: 93.75%] [G loss: 3.210296]\n",
      "4568 [D loss: 0.188304, acc.: 93.75%] [G loss: 3.178954]\n",
      "4569 [D loss: 0.161570, acc.: 95.31%] [G loss: 3.058345]\n",
      "4570 [D loss: 0.173958, acc.: 95.31%] [G loss: 3.317461]\n",
      "4571 [D loss: 0.162948, acc.: 95.31%] [G loss: 2.718489]\n",
      "4572 [D loss: 0.165071, acc.: 95.31%] [G loss: 3.016270]\n",
      "4573 [D loss: 0.170437, acc.: 93.75%] [G loss: 3.297100]\n",
      "4574 [D loss: 0.179567, acc.: 95.31%] [G loss: 2.996442]\n",
      "4575 [D loss: 0.168453, acc.: 95.31%] [G loss: 2.817280]\n",
      "4576 [D loss: 0.163583, acc.: 95.31%] [G loss: 2.805189]\n",
      "4577 [D loss: 0.158267, acc.: 95.31%] [G loss: 2.948087]\n",
      "4578 [D loss: 0.176121, acc.: 95.31%] [G loss: 2.917396]\n",
      "4579 [D loss: 0.166708, acc.: 95.31%] [G loss: 2.667705]\n",
      "4580 [D loss: 0.166146, acc.: 95.31%] [G loss: 2.846274]\n",
      "4581 [D loss: 0.138424, acc.: 95.31%] [G loss: 3.195696]\n",
      "4582 [D loss: 0.159951, acc.: 95.31%] [G loss: 2.698272]\n",
      "4583 [D loss: 0.172073, acc.: 95.31%] [G loss: 2.759596]\n",
      "4584 [D loss: 0.154271, acc.: 95.31%] [G loss: 2.949674]\n",
      "4585 [D loss: 0.195714, acc.: 93.75%] [G loss: 3.386050]\n",
      "4586 [D loss: 0.171556, acc.: 95.31%] [G loss: 3.049345]\n",
      "4587 [D loss: 0.157432, acc.: 95.31%] [G loss: 2.825260]\n",
      "4588 [D loss: 0.181324, acc.: 93.75%] [G loss: 2.770628]\n",
      "4589 [D loss: 0.180298, acc.: 93.75%] [G loss: 3.087613]\n",
      "4590 [D loss: 0.165354, acc.: 95.31%] [G loss: 2.772148]\n",
      "4591 [D loss: 0.172669, acc.: 95.31%] [G loss: 2.956882]\n",
      "4592 [D loss: 0.153468, acc.: 95.31%] [G loss: 2.857783]\n",
      "4593 [D loss: 0.152904, acc.: 95.31%] [G loss: 2.719122]\n",
      "4594 [D loss: 0.150039, acc.: 95.31%] [G loss: 2.965194]\n",
      "4595 [D loss: 0.142857, acc.: 95.31%] [G loss: 3.276265]\n",
      "4596 [D loss: 0.170973, acc.: 93.75%] [G loss: 2.911503]\n",
      "4597 [D loss: 0.163481, acc.: 95.31%] [G loss: 2.640228]\n",
      "4598 [D loss: 0.161854, acc.: 95.31%] [G loss: 3.027116]\n",
      "4599 [D loss: 0.213022, acc.: 93.75%] [G loss: 3.188117]\n",
      "4600 [D loss: 0.155956, acc.: 95.31%] [G loss: 3.157963]\n",
      "generated_data\n",
      "4601 [D loss: 0.158519, acc.: 95.31%] [G loss: 2.859609]\n",
      "4602 [D loss: 0.151546, acc.: 95.31%] [G loss: 2.926609]\n",
      "4603 [D loss: 0.181228, acc.: 93.75%] [G loss: 2.812212]\n",
      "4604 [D loss: 0.157372, acc.: 95.31%] [G loss: 2.923108]\n",
      "4605 [D loss: 0.160547, acc.: 95.31%] [G loss: 2.707771]\n",
      "4606 [D loss: 0.158359, acc.: 95.31%] [G loss: 2.917770]\n",
      "4607 [D loss: 0.134480, acc.: 95.31%] [G loss: 3.498814]\n",
      "4608 [D loss: 0.143270, acc.: 95.31%] [G loss: 2.639534]\n",
      "4609 [D loss: 0.158121, acc.: 95.31%] [G loss: 2.820258]\n",
      "4610 [D loss: 0.182376, acc.: 93.75%] [G loss: 2.946554]\n",
      "4611 [D loss: 0.149259, acc.: 95.31%] [G loss: 3.378981]\n",
      "4612 [D loss: 0.168855, acc.: 95.31%] [G loss: 2.745257]\n",
      "4613 [D loss: 0.155990, acc.: 93.75%] [G loss: 3.077394]\n",
      "4614 [D loss: 0.155891, acc.: 95.31%] [G loss: 3.363755]\n",
      "4615 [D loss: 0.138601, acc.: 95.31%] [G loss: 3.011250]\n",
      "4616 [D loss: 0.145246, acc.: 95.31%] [G loss: 3.075856]\n",
      "4617 [D loss: 0.126638, acc.: 95.31%] [G loss: 3.619038]\n",
      "4618 [D loss: 0.169326, acc.: 93.75%] [G loss: 3.091493]\n",
      "4619 [D loss: 0.161725, acc.: 95.31%] [G loss: 2.718394]\n",
      "4620 [D loss: 0.180528, acc.: 95.31%] [G loss: 2.929860]\n",
      "4621 [D loss: 0.152144, acc.: 95.31%] [G loss: 2.721638]\n",
      "4622 [D loss: 0.163280, acc.: 95.31%] [G loss: 2.526165]\n",
      "4623 [D loss: 0.162589, acc.: 95.31%] [G loss: 2.968113]\n",
      "4624 [D loss: 0.157809, acc.: 95.31%] [G loss: 3.068831]\n",
      "4625 [D loss: 0.143673, acc.: 95.31%] [G loss: 3.401290]\n",
      "4626 [D loss: 0.156467, acc.: 95.31%] [G loss: 2.821579]\n",
      "4627 [D loss: 0.169214, acc.: 93.75%] [G loss: 3.355434]\n",
      "4628 [D loss: 0.184276, acc.: 93.75%] [G loss: 3.026126]\n",
      "4629 [D loss: 0.167086, acc.: 95.31%] [G loss: 2.761751]\n",
      "4630 [D loss: 0.191476, acc.: 92.19%] [G loss: 3.488481]\n",
      "4631 [D loss: 0.189606, acc.: 93.75%] [G loss: 3.562101]\n",
      "4632 [D loss: 0.152533, acc.: 93.75%] [G loss: 2.415603]\n",
      "4633 [D loss: 0.115882, acc.: 95.31%] [G loss: 3.391579]\n",
      "4634 [D loss: 0.173111, acc.: 93.75%] [G loss: 2.595881]\n",
      "4635 [D loss: 0.144861, acc.: 95.31%] [G loss: 3.323605]\n",
      "4636 [D loss: 0.174913, acc.: 95.31%] [G loss: 2.914189]\n",
      "4637 [D loss: 0.211433, acc.: 92.19%] [G loss: 2.912212]\n",
      "4638 [D loss: 0.228046, acc.: 93.75%] [G loss: 3.140663]\n",
      "4639 [D loss: 0.200605, acc.: 92.19%] [G loss: 3.309797]\n",
      "4640 [D loss: 0.189577, acc.: 93.75%] [G loss: 2.659185]\n",
      "4641 [D loss: 0.262937, acc.: 92.19%] [G loss: 2.788947]\n",
      "4642 [D loss: 0.192405, acc.: 93.75%] [G loss: 2.921556]\n",
      "4643 [D loss: 0.186806, acc.: 93.75%] [G loss: 3.053451]\n",
      "4644 [D loss: 0.155100, acc.: 95.31%] [G loss: 3.127439]\n",
      "4645 [D loss: 0.135933, acc.: 95.31%] [G loss: 3.370517]\n",
      "4646 [D loss: 0.232966, acc.: 92.19%] [G loss: 3.177593]\n",
      "4647 [D loss: 0.159865, acc.: 95.31%] [G loss: 2.797764]\n",
      "4648 [D loss: 0.176171, acc.: 93.75%] [G loss: 2.751026]\n",
      "4649 [D loss: 0.164718, acc.: 95.31%] [G loss: 3.081733]\n",
      "4650 [D loss: 0.180315, acc.: 92.19%] [G loss: 3.490013]\n",
      "4651 [D loss: 0.155227, acc.: 95.31%] [G loss: 2.777311]\n",
      "4652 [D loss: 0.251326, acc.: 92.19%] [G loss: 2.994609]\n",
      "4653 [D loss: 0.148800, acc.: 95.31%] [G loss: 3.416953]\n",
      "4654 [D loss: 0.216553, acc.: 92.19%] [G loss: 2.670175]\n",
      "4655 [D loss: 0.220833, acc.: 92.19%] [G loss: 3.247009]\n",
      "4656 [D loss: 0.173014, acc.: 95.31%] [G loss: 2.767305]\n",
      "4657 [D loss: 0.181551, acc.: 93.75%] [G loss: 3.191555]\n",
      "4658 [D loss: 0.156298, acc.: 95.31%] [G loss: 2.852099]\n",
      "4659 [D loss: 0.183180, acc.: 93.75%] [G loss: 2.734211]\n",
      "4660 [D loss: 0.157929, acc.: 95.31%] [G loss: 3.087404]\n",
      "4661 [D loss: 0.155974, acc.: 95.31%] [G loss: 2.954653]\n",
      "4662 [D loss: 0.181565, acc.: 95.31%] [G loss: 2.753555]\n",
      "4663 [D loss: 0.155575, acc.: 95.31%] [G loss: 3.096314]\n",
      "4664 [D loss: 0.150700, acc.: 95.31%] [G loss: 2.864211]\n",
      "4665 [D loss: 0.163301, acc.: 95.31%] [G loss: 2.887090]\n",
      "4666 [D loss: 0.162441, acc.: 95.31%] [G loss: 3.087335]\n",
      "4667 [D loss: 0.176958, acc.: 93.75%] [G loss: 3.426011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4668 [D loss: 0.170362, acc.: 95.31%] [G loss: 2.850452]\n",
      "4669 [D loss: 0.155071, acc.: 95.31%] [G loss: 3.062909]\n",
      "4670 [D loss: 0.143271, acc.: 95.31%] [G loss: 3.361370]\n",
      "4671 [D loss: 0.158248, acc.: 95.31%] [G loss: 2.811626]\n",
      "4672 [D loss: 0.143892, acc.: 95.31%] [G loss: 2.632456]\n",
      "4673 [D loss: 0.160338, acc.: 93.75%] [G loss: 3.211039]\n",
      "4674 [D loss: 0.156225, acc.: 95.31%] [G loss: 2.896992]\n",
      "4675 [D loss: 0.192759, acc.: 93.75%] [G loss: 2.881449]\n",
      "4676 [D loss: 0.153257, acc.: 95.31%] [G loss: 3.262537]\n",
      "4677 [D loss: 0.167041, acc.: 93.75%] [G loss: 2.818805]\n",
      "4678 [D loss: 0.170355, acc.: 93.75%] [G loss: 2.707154]\n",
      "4679 [D loss: 0.165516, acc.: 93.75%] [G loss: 3.129184]\n",
      "4680 [D loss: 0.162435, acc.: 95.31%] [G loss: 3.066239]\n",
      "4681 [D loss: 0.166369, acc.: 95.31%] [G loss: 2.799641]\n",
      "4682 [D loss: 0.189826, acc.: 93.75%] [G loss: 2.980146]\n",
      "4683 [D loss: 0.153333, acc.: 95.31%] [G loss: 3.030020]\n",
      "4684 [D loss: 0.167229, acc.: 93.75%] [G loss: 2.851995]\n",
      "4685 [D loss: 0.176723, acc.: 95.31%] [G loss: 2.842433]\n",
      "4686 [D loss: 0.201702, acc.: 92.19%] [G loss: 3.538200]\n",
      "4687 [D loss: 0.168921, acc.: 93.75%] [G loss: 3.316414]\n",
      "4688 [D loss: 0.199187, acc.: 93.75%] [G loss: 2.723191]\n",
      "4689 [D loss: 0.139332, acc.: 95.31%] [G loss: 3.097753]\n",
      "4690 [D loss: 0.143367, acc.: 95.31%] [G loss: 3.313467]\n",
      "4691 [D loss: 0.167075, acc.: 95.31%] [G loss: 3.055160]\n",
      "4692 [D loss: 0.138891, acc.: 95.31%] [G loss: 2.877370]\n",
      "4693 [D loss: 0.165875, acc.: 95.31%] [G loss: 2.899053]\n",
      "4694 [D loss: 0.164601, acc.: 95.31%] [G loss: 3.179038]\n",
      "4695 [D loss: 0.171304, acc.: 95.31%] [G loss: 3.165152]\n",
      "4696 [D loss: 0.189424, acc.: 93.75%] [G loss: 2.777196]\n",
      "4697 [D loss: 0.162213, acc.: 95.31%] [G loss: 2.825096]\n",
      "4698 [D loss: 0.169206, acc.: 95.31%] [G loss: 3.070621]\n",
      "4699 [D loss: 0.160802, acc.: 95.31%] [G loss: 3.111093]\n",
      "4700 [D loss: 0.158977, acc.: 95.31%] [G loss: 2.856829]\n",
      "generated_data\n",
      "4701 [D loss: 0.170961, acc.: 95.31%] [G loss: 2.704784]\n",
      "4702 [D loss: 0.159059, acc.: 95.31%] [G loss: 2.764170]\n",
      "4703 [D loss: 0.134661, acc.: 95.31%] [G loss: 3.047034]\n",
      "4704 [D loss: 0.181461, acc.: 93.75%] [G loss: 2.830662]\n",
      "4705 [D loss: 0.183462, acc.: 93.75%] [G loss: 2.860029]\n",
      "4706 [D loss: 0.169004, acc.: 95.31%] [G loss: 2.671586]\n",
      "4707 [D loss: 0.142666, acc.: 95.31%] [G loss: 3.299737]\n",
      "4708 [D loss: 0.146639, acc.: 95.31%] [G loss: 2.888887]\n",
      "4709 [D loss: 0.190206, acc.: 93.75%] [G loss: 2.959341]\n",
      "4710 [D loss: 0.158466, acc.: 95.31%] [G loss: 3.202709]\n",
      "4711 [D loss: 0.151632, acc.: 95.31%] [G loss: 3.479002]\n",
      "4712 [D loss: 0.164433, acc.: 95.31%] [G loss: 2.936294]\n",
      "4713 [D loss: 0.158352, acc.: 95.31%] [G loss: 2.781397]\n",
      "4714 [D loss: 0.166308, acc.: 95.31%] [G loss: 3.121449]\n",
      "4715 [D loss: 0.168972, acc.: 95.31%] [G loss: 2.685616]\n",
      "4716 [D loss: 0.145890, acc.: 95.31%] [G loss: 2.937849]\n",
      "4717 [D loss: 0.182241, acc.: 93.75%] [G loss: 2.982291]\n",
      "4718 [D loss: 0.152829, acc.: 95.31%] [G loss: 3.192147]\n",
      "4719 [D loss: 0.160427, acc.: 95.31%] [G loss: 3.097320]\n",
      "4720 [D loss: 0.181410, acc.: 93.75%] [G loss: 2.839130]\n",
      "4721 [D loss: 0.159970, acc.: 95.31%] [G loss: 2.625090]\n",
      "4722 [D loss: 0.145719, acc.: 95.31%] [G loss: 3.534420]\n",
      "4723 [D loss: 0.176345, acc.: 95.31%] [G loss: 2.965976]\n",
      "4724 [D loss: 0.137086, acc.: 95.31%] [G loss: 3.574502]\n",
      "4725 [D loss: 0.146153, acc.: 95.31%] [G loss: 3.186138]\n",
      "4726 [D loss: 0.195347, acc.: 93.75%] [G loss: 2.953440]\n",
      "4727 [D loss: 0.152819, acc.: 95.31%] [G loss: 3.032970]\n",
      "4728 [D loss: 0.167791, acc.: 93.75%] [G loss: 3.078134]\n",
      "4729 [D loss: 0.164059, acc.: 95.31%] [G loss: 2.917383]\n",
      "4730 [D loss: 0.185058, acc.: 93.75%] [G loss: 3.197857]\n",
      "4731 [D loss: 0.184560, acc.: 95.31%] [G loss: 3.098334]\n",
      "4732 [D loss: 0.155010, acc.: 95.31%] [G loss: 3.009762]\n",
      "4733 [D loss: 0.149067, acc.: 95.31%] [G loss: 2.795686]\n",
      "4734 [D loss: 0.163761, acc.: 95.31%] [G loss: 2.675764]\n",
      "4735 [D loss: 0.167377, acc.: 95.31%] [G loss: 2.867140]\n",
      "4736 [D loss: 0.143058, acc.: 95.31%] [G loss: 3.165563]\n",
      "4737 [D loss: 0.242277, acc.: 90.62%] [G loss: 2.998997]\n",
      "4738 [D loss: 0.215298, acc.: 93.75%] [G loss: 2.788521]\n",
      "4739 [D loss: 0.191941, acc.: 93.75%] [G loss: 2.639572]\n",
      "4740 [D loss: 0.150543, acc.: 95.31%] [G loss: 3.099571]\n",
      "4741 [D loss: 0.201763, acc.: 93.75%] [G loss: 3.275920]\n",
      "4742 [D loss: 0.184886, acc.: 93.75%] [G loss: 2.915042]\n",
      "4743 [D loss: 0.185848, acc.: 93.75%] [G loss: 3.080251]\n",
      "4744 [D loss: 0.177252, acc.: 95.31%] [G loss: 3.061536]\n",
      "4745 [D loss: 0.153754, acc.: 95.31%] [G loss: 2.664114]\n",
      "4746 [D loss: 0.164427, acc.: 95.31%] [G loss: 2.784479]\n",
      "4747 [D loss: 0.202304, acc.: 90.62%] [G loss: 3.827645]\n",
      "4748 [D loss: 0.191083, acc.: 93.75%] [G loss: 3.233185]\n",
      "4749 [D loss: 0.236365, acc.: 92.19%] [G loss: 2.913801]\n",
      "4750 [D loss: 0.171508, acc.: 93.75%] [G loss: 2.841013]\n",
      "4751 [D loss: 0.204290, acc.: 90.62%] [G loss: 3.343391]\n",
      "4752 [D loss: 0.157543, acc.: 93.75%] [G loss: 3.051288]\n",
      "4753 [D loss: 0.168711, acc.: 93.75%] [G loss: 2.838812]\n",
      "4754 [D loss: 0.140250, acc.: 95.31%] [G loss: 3.068662]\n",
      "4755 [D loss: 0.157562, acc.: 95.31%] [G loss: 2.955373]\n",
      "4756 [D loss: 0.150556, acc.: 95.31%] [G loss: 2.877269]\n",
      "4757 [D loss: 0.176266, acc.: 93.75%] [G loss: 3.324931]\n",
      "4758 [D loss: 0.182985, acc.: 95.31%] [G loss: 2.993364]\n",
      "4759 [D loss: 0.181156, acc.: 93.75%] [G loss: 2.877143]\n",
      "4760 [D loss: 0.134083, acc.: 95.31%] [G loss: 3.538673]\n",
      "4761 [D loss: 0.149960, acc.: 95.31%] [G loss: 2.721020]\n",
      "4762 [D loss: 0.167219, acc.: 95.31%] [G loss: 2.788694]\n",
      "4763 [D loss: 0.146350, acc.: 95.31%] [G loss: 3.378995]\n",
      "4764 [D loss: 0.202591, acc.: 93.75%] [G loss: 2.513514]\n",
      "4765 [D loss: 0.144508, acc.: 95.31%] [G loss: 3.241211]\n",
      "4766 [D loss: 0.167373, acc.: 93.75%] [G loss: 3.214078]\n",
      "4767 [D loss: 0.156775, acc.: 93.75%] [G loss: 2.733712]\n",
      "4768 [D loss: 0.163672, acc.: 95.31%] [G loss: 3.516735]\n",
      "4769 [D loss: 0.139801, acc.: 95.31%] [G loss: 3.101620]\n",
      "4770 [D loss: 0.169088, acc.: 95.31%] [G loss: 2.794827]\n",
      "4771 [D loss: 0.156484, acc.: 92.19%] [G loss: 3.523440]\n",
      "4772 [D loss: 0.153843, acc.: 95.31%] [G loss: 2.731505]\n",
      "4773 [D loss: 0.144334, acc.: 93.75%] [G loss: 3.196439]\n",
      "4774 [D loss: 0.158414, acc.: 95.31%] [G loss: 2.991588]\n",
      "4775 [D loss: 0.166482, acc.: 95.31%] [G loss: 2.852565]\n",
      "4776 [D loss: 0.161550, acc.: 95.31%] [G loss: 2.882492]\n",
      "4777 [D loss: 0.145635, acc.: 95.31%] [G loss: 3.533334]\n",
      "4778 [D loss: 0.178558, acc.: 93.75%] [G loss: 3.200506]\n",
      "4779 [D loss: 0.184204, acc.: 93.75%] [G loss: 3.159779]\n",
      "4780 [D loss: 0.171004, acc.: 95.31%] [G loss: 3.096037]\n",
      "4781 [D loss: 0.172533, acc.: 95.31%] [G loss: 2.492391]\n",
      "4782 [D loss: 0.149355, acc.: 95.31%] [G loss: 3.059985]\n",
      "4783 [D loss: 0.235728, acc.: 90.62%] [G loss: 3.595939]\n",
      "4784 [D loss: 0.198091, acc.: 93.75%] [G loss: 2.825094]\n",
      "4785 [D loss: 0.142455, acc.: 93.75%] [G loss: 3.483658]\n",
      "4786 [D loss: 0.170402, acc.: 95.31%] [G loss: 2.339364]\n",
      "4787 [D loss: 0.147579, acc.: 93.75%] [G loss: 3.219984]\n",
      "4788 [D loss: 0.155687, acc.: 95.31%] [G loss: 3.320492]\n",
      "4789 [D loss: 0.169364, acc.: 95.31%] [G loss: 3.095717]\n",
      "4790 [D loss: 0.175471, acc.: 95.31%] [G loss: 2.517830]\n",
      "4791 [D loss: 0.162943, acc.: 95.31%] [G loss: 2.646442]\n",
      "4792 [D loss: 0.178522, acc.: 95.31%] [G loss: 2.792371]\n",
      "4793 [D loss: 0.165049, acc.: 95.31%] [G loss: 3.254969]\n",
      "4794 [D loss: 0.143314, acc.: 95.31%] [G loss: 3.319256]\n",
      "4795 [D loss: 0.149428, acc.: 95.31%] [G loss: 3.062498]\n",
      "4796 [D loss: 0.186806, acc.: 93.75%] [G loss: 3.310193]\n",
      "4797 [D loss: 0.164046, acc.: 95.31%] [G loss: 2.872454]\n",
      "4798 [D loss: 0.139478, acc.: 95.31%] [G loss: 3.158459]\n",
      "4799 [D loss: 0.144520, acc.: 95.31%] [G loss: 3.679835]\n",
      "4800 [D loss: 0.153407, acc.: 95.31%] [G loss: 2.675119]\n",
      "generated_data\n",
      "4801 [D loss: 0.146314, acc.: 95.31%] [G loss: 3.703707]\n",
      "4802 [D loss: 0.179164, acc.: 93.75%] [G loss: 2.584113]\n",
      "4803 [D loss: 0.178188, acc.: 93.75%] [G loss: 2.763115]\n",
      "4804 [D loss: 0.142474, acc.: 95.31%] [G loss: 3.386222]\n",
      "4805 [D loss: 0.184270, acc.: 93.75%] [G loss: 2.628454]\n",
      "4806 [D loss: 0.161377, acc.: 95.31%] [G loss: 2.913458]\n",
      "4807 [D loss: 0.158567, acc.: 95.31%] [G loss: 3.147950]\n",
      "4808 [D loss: 0.165250, acc.: 95.31%] [G loss: 2.613485]\n",
      "4809 [D loss: 0.188418, acc.: 93.75%] [G loss: 2.679948]\n",
      "4810 [D loss: 0.154934, acc.: 95.31%] [G loss: 3.497661]\n",
      "4811 [D loss: 0.201055, acc.: 93.75%] [G loss: 2.875677]\n",
      "4812 [D loss: 0.140612, acc.: 95.31%] [G loss: 3.342545]\n",
      "4813 [D loss: 0.181183, acc.: 92.19%] [G loss: 3.616957]\n",
      "4814 [D loss: 0.144699, acc.: 95.31%] [G loss: 3.068615]\n",
      "4815 [D loss: 0.142327, acc.: 95.31%] [G loss: 3.204828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4816 [D loss: 0.174142, acc.: 95.31%] [G loss: 2.676152]\n",
      "4817 [D loss: 0.143824, acc.: 93.75%] [G loss: 3.416781]\n",
      "4818 [D loss: 0.210128, acc.: 93.75%] [G loss: 3.288744]\n",
      "4819 [D loss: 0.197555, acc.: 95.31%] [G loss: 2.624248]\n",
      "4820 [D loss: 0.152590, acc.: 95.31%] [G loss: 2.957660]\n",
      "4821 [D loss: 0.177679, acc.: 93.75%] [G loss: 3.279453]\n",
      "4822 [D loss: 0.155221, acc.: 95.31%] [G loss: 2.790842]\n",
      "4823 [D loss: 0.160287, acc.: 95.31%] [G loss: 2.882072]\n",
      "4824 [D loss: 0.140653, acc.: 95.31%] [G loss: 3.562329]\n",
      "4825 [D loss: 0.169307, acc.: 95.31%] [G loss: 3.097266]\n",
      "4826 [D loss: 0.143776, acc.: 95.31%] [G loss: 3.046671]\n",
      "4827 [D loss: 0.135927, acc.: 95.31%] [G loss: 3.022820]\n",
      "4828 [D loss: 0.134714, acc.: 95.31%] [G loss: 3.282937]\n",
      "4829 [D loss: 0.196430, acc.: 93.75%] [G loss: 3.146773]\n",
      "4830 [D loss: 0.186119, acc.: 93.75%] [G loss: 3.622680]\n",
      "4831 [D loss: 0.196033, acc.: 93.75%] [G loss: 3.171329]\n",
      "4832 [D loss: 0.187036, acc.: 95.31%] [G loss: 2.518425]\n",
      "4833 [D loss: 0.176829, acc.: 95.31%] [G loss: 2.655179]\n",
      "4834 [D loss: 0.158146, acc.: 95.31%] [G loss: 2.960063]\n",
      "4835 [D loss: 0.166790, acc.: 95.31%] [G loss: 3.072837]\n",
      "4836 [D loss: 0.148894, acc.: 95.31%] [G loss: 2.883416]\n",
      "4837 [D loss: 0.142579, acc.: 95.31%] [G loss: 3.617270]\n",
      "4838 [D loss: 0.151024, acc.: 95.31%] [G loss: 2.823394]\n",
      "4839 [D loss: 0.167698, acc.: 95.31%] [G loss: 2.803163]\n",
      "4840 [D loss: 0.195588, acc.: 93.75%] [G loss: 3.042107]\n",
      "4841 [D loss: 0.147224, acc.: 95.31%] [G loss: 3.163919]\n",
      "4842 [D loss: 0.152652, acc.: 95.31%] [G loss: 2.877687]\n",
      "4843 [D loss: 0.166286, acc.: 95.31%] [G loss: 3.092001]\n",
      "4844 [D loss: 0.162985, acc.: 95.31%] [G loss: 2.798516]\n",
      "4845 [D loss: 0.171156, acc.: 93.75%] [G loss: 3.251230]\n",
      "4846 [D loss: 0.179879, acc.: 95.31%] [G loss: 2.971735]\n",
      "4847 [D loss: 0.156487, acc.: 95.31%] [G loss: 2.795191]\n",
      "4848 [D loss: 0.140342, acc.: 95.31%] [G loss: 2.887544]\n",
      "4849 [D loss: 0.192798, acc.: 93.75%] [G loss: 3.091827]\n",
      "4850 [D loss: 0.162842, acc.: 95.31%] [G loss: 3.039462]\n",
      "4851 [D loss: 0.157700, acc.: 95.31%] [G loss: 2.648821]\n",
      "4852 [D loss: 0.131188, acc.: 95.31%] [G loss: 3.333495]\n",
      "4853 [D loss: 0.145326, acc.: 95.31%] [G loss: 3.043196]\n",
      "4854 [D loss: 0.183314, acc.: 93.75%] [G loss: 3.000735]\n",
      "4855 [D loss: 0.133450, acc.: 95.31%] [G loss: 3.596177]\n",
      "4856 [D loss: 0.151881, acc.: 95.31%] [G loss: 2.857643]\n",
      "4857 [D loss: 0.109665, acc.: 95.31%] [G loss: 3.393393]\n",
      "4858 [D loss: 0.205103, acc.: 93.75%] [G loss: 2.819527]\n",
      "4859 [D loss: 0.137723, acc.: 93.75%] [G loss: 3.852921]\n",
      "4860 [D loss: 0.172146, acc.: 95.31%] [G loss: 2.836479]\n",
      "4861 [D loss: 0.160603, acc.: 95.31%] [G loss: 2.893495]\n",
      "4862 [D loss: 0.174937, acc.: 95.31%] [G loss: 3.034068]\n",
      "4863 [D loss: 0.158156, acc.: 95.31%] [G loss: 2.831814]\n",
      "4864 [D loss: 0.171478, acc.: 95.31%] [G loss: 2.440469]\n",
      "4865 [D loss: 0.144764, acc.: 95.31%] [G loss: 3.012443]\n",
      "4866 [D loss: 0.132757, acc.: 95.31%] [G loss: 3.316142]\n",
      "4867 [D loss: 0.161834, acc.: 95.31%] [G loss: 2.922189]\n",
      "4868 [D loss: 0.151465, acc.: 93.75%] [G loss: 3.616591]\n",
      "4869 [D loss: 0.157092, acc.: 95.31%] [G loss: 3.088039]\n",
      "4870 [D loss: 0.196185, acc.: 92.19%] [G loss: 3.060613]\n",
      "4871 [D loss: 0.162108, acc.: 95.31%] [G loss: 3.082262]\n",
      "4872 [D loss: 0.154578, acc.: 95.31%] [G loss: 2.804502]\n",
      "4873 [D loss: 0.191117, acc.: 95.31%] [G loss: 2.721403]\n",
      "4874 [D loss: 0.161157, acc.: 95.31%] [G loss: 2.997007]\n",
      "4875 [D loss: 0.159123, acc.: 95.31%] [G loss: 2.839719]\n",
      "4876 [D loss: 0.165204, acc.: 95.31%] [G loss: 2.899144]\n",
      "4877 [D loss: 0.154235, acc.: 95.31%] [G loss: 3.329793]\n",
      "4878 [D loss: 0.145726, acc.: 95.31%] [G loss: 3.028162]\n",
      "4879 [D loss: 0.164377, acc.: 95.31%] [G loss: 2.735693]\n",
      "4880 [D loss: 0.166502, acc.: 95.31%] [G loss: 3.080487]\n",
      "4881 [D loss: 0.153457, acc.: 95.31%] [G loss: 3.275972]\n",
      "4882 [D loss: 0.168649, acc.: 95.31%] [G loss: 2.890687]\n",
      "4883 [D loss: 0.146788, acc.: 95.31%] [G loss: 2.933869]\n",
      "4884 [D loss: 0.161177, acc.: 93.75%] [G loss: 3.403641]\n",
      "4885 [D loss: 0.134940, acc.: 95.31%] [G loss: 3.031302]\n",
      "4886 [D loss: 0.160817, acc.: 95.31%] [G loss: 2.796583]\n",
      "4887 [D loss: 0.177397, acc.: 93.75%] [G loss: 3.343820]\n",
      "4888 [D loss: 0.155140, acc.: 95.31%] [G loss: 3.187982]\n",
      "4889 [D loss: 0.174333, acc.: 93.75%] [G loss: 2.734649]\n",
      "4890 [D loss: 0.162529, acc.: 95.31%] [G loss: 2.890652]\n",
      "4891 [D loss: 0.160085, acc.: 95.31%] [G loss: 2.862817]\n",
      "4892 [D loss: 0.178245, acc.: 95.31%] [G loss: 2.683023]\n",
      "4893 [D loss: 0.184070, acc.: 93.75%] [G loss: 2.827218]\n",
      "4894 [D loss: 0.145436, acc.: 95.31%] [G loss: 3.317065]\n",
      "4895 [D loss: 0.181971, acc.: 93.75%] [G loss: 3.025026]\n",
      "4896 [D loss: 0.177100, acc.: 95.31%] [G loss: 2.681437]\n",
      "4897 [D loss: 0.143786, acc.: 95.31%] [G loss: 3.526774]\n",
      "4898 [D loss: 0.159225, acc.: 95.31%] [G loss: 2.973746]\n",
      "4899 [D loss: 0.138276, acc.: 95.31%] [G loss: 3.033367]\n",
      "4900 [D loss: 0.133527, acc.: 95.31%] [G loss: 3.448549]\n",
      "generated_data\n",
      "4901 [D loss: 0.167876, acc.: 95.31%] [G loss: 2.934982]\n",
      "4902 [D loss: 0.165290, acc.: 95.31%] [G loss: 2.973521]\n",
      "4903 [D loss: 0.183724, acc.: 93.75%] [G loss: 3.210454]\n",
      "4904 [D loss: 0.150431, acc.: 95.31%] [G loss: 3.061458]\n",
      "4905 [D loss: 0.224649, acc.: 93.75%] [G loss: 2.834556]\n",
      "4906 [D loss: 0.168393, acc.: 95.31%] [G loss: 2.983668]\n",
      "4907 [D loss: 0.176692, acc.: 93.75%] [G loss: 3.311620]\n",
      "4908 [D loss: 0.170052, acc.: 93.75%] [G loss: 3.334894]\n",
      "4909 [D loss: 0.162892, acc.: 95.31%] [G loss: 2.818098]\n",
      "4910 [D loss: 0.177806, acc.: 93.75%] [G loss: 2.732206]\n",
      "4911 [D loss: 0.160474, acc.: 95.31%] [G loss: 2.831273]\n",
      "4912 [D loss: 0.159280, acc.: 95.31%] [G loss: 2.843683]\n",
      "4913 [D loss: 0.179619, acc.: 95.31%] [G loss: 2.773873]\n",
      "4914 [D loss: 0.162196, acc.: 95.31%] [G loss: 3.135306]\n",
      "4915 [D loss: 0.153375, acc.: 95.31%] [G loss: 3.122978]\n",
      "4916 [D loss: 0.170342, acc.: 95.31%] [G loss: 2.753926]\n",
      "4917 [D loss: 0.153102, acc.: 95.31%] [G loss: 3.488721]\n",
      "4918 [D loss: 0.156636, acc.: 95.31%] [G loss: 3.097274]\n",
      "4919 [D loss: 0.159436, acc.: 95.31%] [G loss: 3.369599]\n",
      "4920 [D loss: 0.138932, acc.: 95.31%] [G loss: 3.057860]\n",
      "4921 [D loss: 0.128110, acc.: 95.31%] [G loss: 3.713226]\n",
      "4922 [D loss: 0.172257, acc.: 95.31%] [G loss: 2.810974]\n",
      "4923 [D loss: 0.141831, acc.: 95.31%] [G loss: 4.165488]\n",
      "4924 [D loss: 0.189286, acc.: 93.75%] [G loss: 3.253647]\n",
      "4925 [D loss: 0.160727, acc.: 95.31%] [G loss: 3.032704]\n",
      "4926 [D loss: 0.187067, acc.: 93.75%] [G loss: 2.829797]\n",
      "4927 [D loss: 0.147598, acc.: 95.31%] [G loss: 3.530232]\n",
      "4928 [D loss: 0.177333, acc.: 93.75%] [G loss: 3.033856]\n",
      "4929 [D loss: 0.166288, acc.: 95.31%] [G loss: 2.934134]\n",
      "4930 [D loss: 0.159224, acc.: 95.31%] [G loss: 3.121763]\n",
      "4931 [D loss: 0.147146, acc.: 95.31%] [G loss: 3.497880]\n",
      "4932 [D loss: 0.157134, acc.: 95.31%] [G loss: 2.753597]\n",
      "4933 [D loss: 0.137753, acc.: 95.31%] [G loss: 3.710017]\n",
      "4934 [D loss: 0.195866, acc.: 93.75%] [G loss: 3.082538]\n",
      "4935 [D loss: 0.135171, acc.: 95.31%] [G loss: 3.606831]\n",
      "4936 [D loss: 0.129372, acc.: 95.31%] [G loss: 3.475597]\n",
      "4937 [D loss: 0.151999, acc.: 95.31%] [G loss: 2.949291]\n",
      "4938 [D loss: 0.153420, acc.: 95.31%] [G loss: 3.261782]\n",
      "4939 [D loss: 0.164902, acc.: 95.31%] [G loss: 2.788536]\n",
      "4940 [D loss: 0.152601, acc.: 95.31%] [G loss: 2.719552]\n",
      "4941 [D loss: 0.155301, acc.: 95.31%] [G loss: 3.172345]\n",
      "4942 [D loss: 0.206842, acc.: 93.75%] [G loss: 3.028141]\n",
      "4943 [D loss: 0.171275, acc.: 95.31%] [G loss: 2.976020]\n",
      "4944 [D loss: 0.206009, acc.: 92.19%] [G loss: 3.177075]\n",
      "4945 [D loss: 0.174080, acc.: 93.75%] [G loss: 2.855458]\n",
      "4946 [D loss: 0.184007, acc.: 95.31%] [G loss: 2.883004]\n",
      "4947 [D loss: 0.168476, acc.: 95.31%] [G loss: 3.116159]\n",
      "4948 [D loss: 0.185731, acc.: 93.75%] [G loss: 3.049881]\n",
      "4949 [D loss: 0.164201, acc.: 95.31%] [G loss: 3.253788]\n",
      "4950 [D loss: 0.154481, acc.: 93.75%] [G loss: 3.036989]\n",
      "4951 [D loss: 0.169211, acc.: 93.75%] [G loss: 2.926456]\n",
      "4952 [D loss: 0.156807, acc.: 95.31%] [G loss: 3.260858]\n",
      "4953 [D loss: 0.148198, acc.: 95.31%] [G loss: 3.009671]\n",
      "4954 [D loss: 0.175892, acc.: 95.31%] [G loss: 2.815620]\n",
      "4955 [D loss: 0.189615, acc.: 93.75%] [G loss: 2.687085]\n",
      "4956 [D loss: 0.210558, acc.: 93.75%] [G loss: 2.856985]\n",
      "4957 [D loss: 0.192333, acc.: 92.19%] [G loss: 2.871258]\n",
      "4958 [D loss: 0.172464, acc.: 95.31%] [G loss: 3.145802]\n",
      "4959 [D loss: 0.182943, acc.: 93.75%] [G loss: 2.699007]\n",
      "4960 [D loss: 0.178897, acc.: 95.31%] [G loss: 2.603749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4961 [D loss: 0.151611, acc.: 95.31%] [G loss: 2.888065]\n",
      "4962 [D loss: 0.148282, acc.: 95.31%] [G loss: 3.048382]\n",
      "4963 [D loss: 0.177321, acc.: 95.31%] [G loss: 3.181022]\n",
      "4964 [D loss: 0.192056, acc.: 93.75%] [G loss: 3.376420]\n",
      "4965 [D loss: 0.173451, acc.: 95.31%] [G loss: 2.948910]\n",
      "4966 [D loss: 0.155375, acc.: 95.31%] [G loss: 2.838145]\n",
      "4967 [D loss: 0.175113, acc.: 95.31%] [G loss: 2.635291]\n",
      "4968 [D loss: 0.148367, acc.: 95.31%] [G loss: 2.935454]\n",
      "4969 [D loss: 0.156710, acc.: 95.31%] [G loss: 2.988550]\n",
      "4970 [D loss: 0.152360, acc.: 95.31%] [G loss: 3.255716]\n",
      "4971 [D loss: 0.143723, acc.: 95.31%] [G loss: 3.369333]\n",
      "4972 [D loss: 0.171603, acc.: 95.31%] [G loss: 2.768929]\n",
      "4973 [D loss: 0.149177, acc.: 95.31%] [G loss: 3.065334]\n",
      "4974 [D loss: 0.162121, acc.: 95.31%] [G loss: 3.037390]\n",
      "4975 [D loss: 0.156209, acc.: 95.31%] [G loss: 3.388630]\n",
      "4976 [D loss: 0.143019, acc.: 95.31%] [G loss: 3.139423]\n",
      "4977 [D loss: 0.154955, acc.: 95.31%] [G loss: 2.882811]\n",
      "4978 [D loss: 0.141864, acc.: 95.31%] [G loss: 3.073036]\n",
      "4979 [D loss: 0.157247, acc.: 95.31%] [G loss: 2.773222]\n",
      "4980 [D loss: 0.139867, acc.: 95.31%] [G loss: 3.317508]\n",
      "4981 [D loss: 0.131105, acc.: 95.31%] [G loss: 3.357392]\n",
      "4982 [D loss: 0.161654, acc.: 95.31%] [G loss: 3.023766]\n",
      "4983 [D loss: 0.159594, acc.: 95.31%] [G loss: 2.938567]\n",
      "4984 [D loss: 0.155270, acc.: 95.31%] [G loss: 3.014767]\n",
      "4985 [D loss: 0.144762, acc.: 95.31%] [G loss: 3.129763]\n",
      "4986 [D loss: 0.158120, acc.: 95.31%] [G loss: 3.425818]\n",
      "4987 [D loss: 0.177551, acc.: 95.31%] [G loss: 3.171390]\n",
      "4988 [D loss: 0.139971, acc.: 95.31%] [G loss: 3.211178]\n",
      "4989 [D loss: 0.143248, acc.: 95.31%] [G loss: 3.092176]\n",
      "4990 [D loss: 0.272522, acc.: 90.62%] [G loss: 3.154946]\n",
      "4991 [D loss: 0.204069, acc.: 93.75%] [G loss: 3.119058]\n",
      "4992 [D loss: 0.224761, acc.: 93.75%] [G loss: 2.465668]\n",
      "4993 [D loss: 0.187431, acc.: 95.31%] [G loss: 2.672647]\n",
      "4994 [D loss: 0.183477, acc.: 93.75%] [G loss: 3.203430]\n",
      "4995 [D loss: 0.140319, acc.: 95.31%] [G loss: 3.408688]\n",
      "4996 [D loss: 0.144849, acc.: 95.31%] [G loss: 2.823332]\n",
      "4997 [D loss: 0.184522, acc.: 95.31%] [G loss: 2.851311]\n",
      "4998 [D loss: 0.169120, acc.: 95.31%] [G loss: 2.850965]\n",
      "4999 [D loss: 0.164449, acc.: 95.31%] [G loss: 3.091566]\n",
      "5000 [D loss: 0.154034, acc.: 95.31%] [G loss: 3.340208]\n",
      "generated_data\n"
     ]
    }
   ],
   "source": [
    "model = GAN\n",
    "\n",
    "#Training the GAN model chosen: Vanilla GAN, CGAN, DCGAN, etc.\n",
    "synthesizer = model(gan_args)\n",
    "synthesizer.train(df, train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a47f8853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can easily save the trained generator and loaded it aftwerwards\n",
    "\n",
    "synthesizer.save('model/gan/saved', 'generator_patients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31543fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(32, 32)]                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, 128)                 4224      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (32, 256)                 33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (32, 512)                 131584    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (32, 11)                  5643      \n",
      "=================================================================\n",
      "Total params: 174,475\n",
      "Trainable params: 174,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Lets look at discriminator model\n",
    "\n",
    "synthesizer.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95ce814e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(32, 11)]                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (32, 512)                 6144      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (32, 512)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (32, 256)                 131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (32, 256)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (32, 128)                 32896     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (32, 1)                   129       \n",
      "=================================================================\n",
      "Total params: 170,497\n",
      "Trainable params: 0\n",
      "Non-trainable params: 170,497\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "synthesizer.discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150dfdb",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "\n",
    "\n",
    "Now, that we have trained the model let's see if the generated data is similar to the actual data.\n",
    "\n",
    "We plot the generated data for some of the model steps and see how the plot for the generated data changes as the networks learns the embedding more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dc9bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'GAN': ['GAN', False, synthesizer.generator]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a91d1973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['num_of_doors'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-30c3460e87e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mgen_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_z\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mgen_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Generated_sample.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mgen_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmarkers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgecolors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfacecolors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'none'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Generated Data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_cols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\dl\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3028\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3029\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3030\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3032\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\dl\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1264\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1266\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1267\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\dl\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1308\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1310\u001b[0m             \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['num_of_doors'], dtype='object')] are in the [columns]\""
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x2376 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup parameters visualization parameters\n",
    "seed = 17\n",
    "test_size = 492 # number of fraud cases\n",
    "noise_dim = 32\n",
    "\n",
    "np.random.seed(seed)\n",
    "z = np.random.normal(size=(test_size, noise_dim))\n",
    "real = synthesizer.get_data_batch(train=df, batch_size=test_size, seed=seed)\n",
    "real_samples = pd.DataFrame(real, columns=data_cols)\n",
    "\n",
    "model_names = ['GAN']\n",
    "colors = ['deepskyblue','blue']\n",
    "markers = ['o','^']\n",
    "\n",
    "base_dir = 'model/'\n",
    "\n",
    "#Actual fraud data visualization\n",
    "model_steps = [ 0, 100, 200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000]\n",
    "rows = len(model_steps)\n",
    "columns = 5\n",
    "\n",
    "axarr = [[]]*len(model_steps)\n",
    "\n",
    "fig = plt.figure(figsize=(14,rows*3))\n",
    "\n",
    "for model_step_ix, model_step in enumerate(model_steps):        \n",
    "    axarr[model_step_ix] = plt.subplot(rows, columns, model_step_ix*columns + 1)\n",
    "    \n",
    "#     for group, color, marker in zip(real_samples.groupby(col_group_by), colors, markers):\n",
    "#         plt.scatter( group[1][[col1]], group[1][[col2]], marker=marker, edgecolors=color, facecolors='none' )\n",
    "    \n",
    "    plt.title('Actual Patients Data')\n",
    "    plt.ylabel(col2) # Only add y label to left plot\n",
    "    plt.xlabel(col1)\n",
    "    xlims, ylims = axarr[model_step_ix].get_xlim(), axarr[model_step_ix].get_ylim()\n",
    "    \n",
    "    if model_step_ix == 0: \n",
    "        legend = plt.legend()\n",
    "        legend.get_frame().set_facecolor('white')\n",
    "    \n",
    "    i=0\n",
    "    [model_name, with_class, generator_model] = models['GAN']\n",
    "\n",
    "    generator_model.load_weights( base_dir + '_generator_model_weights_step_'+str(model_step)+'.h5')\n",
    "\n",
    "    ax = plt.subplot(rows, columns, model_step_ix*columns + 1 + (i+1) )\n",
    "\n",
    "    g_z = generator_model.predict(z)\n",
    "\n",
    "    gen_samples = pd.DataFrame(g_z, columns=data_cols)\n",
    "    gen_samples.to_csv('Generated_sample.csv')\n",
    "    plt.scatter( gen_samples[[col1]], gen_samples[[col2]], marker=markers[0], edgecolors=colors[0], facecolors='none' )\n",
    "    plt.title(\"Generated Data\")   \n",
    "    plt.xlabel(data_cols[0])\n",
    "    ax.set_xlim(xlims), ax.set_ylim(ylims)\n",
    "\n",
    "plt.suptitle('Comparison of GAN outputs', size=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0.075,0,1,0.95])\n",
    "\n",
    "# Adding text labels for traning steps\n",
    "vpositions = np.array([ i._position.bounds[1] for i in axarr ])\n",
    "vpositions += ((vpositions[0] - vpositions[1]) * 0.35 )\n",
    "for model_step_ix, model_step in enumerate( model_steps ):\n",
    "    fig.text( 0.05, vpositions[model_step_ix], 'training\\nstep\\n'+str(model_step), ha='center', va='center', size=12)\n",
    "\n",
    "plt.savefig('Comparison_of_GAN_outputs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff02ffbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g_z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-55947da30d73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mg_z\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_z\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgen_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_z\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgen_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Generated_sample.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'g_z' is not defined"
     ]
    }
   ],
   "source": [
    "g_z=pw.inverse_transform(g_z)\n",
    "gen_samples = pd.DataFrame(g_z, columns=data_cols)\n",
    "gen_samples.to_csv('Generated_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install table_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ae209",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_samples.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "print(gen_df.columns)\n",
    "print(df.shape, gen_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18cc463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from table_evaluator import load_data, TableEvaluator\n",
    "\n",
    "print(len(df), len(gen_samples))\n",
    "table_evaluator =  TableEvaluator(df, gen_samples)\n",
    "\n",
    "table_evaluator.visual_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1552be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8ddf07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee3a2bcd",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "\n",
    "Some of the features in the syntehtic data match closely with actual data but there are some other features which were not learnt perfectly by the model. We can keep playing with the model and its hyperparameters to improve the model further.\n",
    "\n",
    "This post demonstrates that its fairly simply to use GANs to generate synthetic data where the actual data is sensitive in nature and can't be shared publicly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9324c654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6482676c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
